diff -Nuar zfs-kmod-9999.orig/cmd/zpool/zpool_main.c zfs-kmod-9999/cmd/zpool/zpool_main.c
--- zfs-kmod-9999.orig/cmd/zpool/zpool_main.c	2017-05-06 11:03:36.947219717 +0200
+++ zfs-kmod-9999/cmd/zpool/zpool_main.c	2017-05-06 11:04:34.207030776 +0200
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  * Copyright (c) 2011, 2015 by Delphix. All rights reserved.
  * Copyright (c) 2012 by Frederik Wessels. All rights reserved.
  * Copyright (c) 2012 by Cyril Plisko. All rights reserved.
@@ -88,6 +88,7 @@
 static int zpool_do_split(int, char **);
 
 static int zpool_do_scrub(int, char **);
+static int zpool_do_trim(int, char **);
 
 static int zpool_do_import(int, char **);
 static int zpool_do_export(int, char **);
@@ -137,6 +138,7 @@
 	HELP_REPLACE,
 	HELP_REMOVE,
 	HELP_SCRUB,
+	HELP_TRIM,
 	HELP_STATUS,
 	HELP_UPGRADE,
 	HELP_EVENTS,
@@ -175,7 +177,7 @@
  * of all the nvlists a flag requires.  Also specifies the order in
  * which data gets printed in zpool iostat.
  */
-static const char *vsx_type_to_nvlist[IOS_COUNT][11] = {
+static const char *vsx_type_to_nvlist[IOS_COUNT][13] = {
 	[IOS_L_HISTO] = {
 	    ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_TOT_W_LAT_HISTO,
@@ -186,12 +188,17 @@
 	    ZPOOL_CONFIG_VDEV_ASYNC_R_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_ASYNC_W_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO,
+	    ZPOOL_CONFIG_VDEV_AUTO_TRIM_LAT_HISTO,
+	    ZPOOL_CONFIG_VDEV_MAN_TRIM_LAT_HISTO,
 	    NULL},
 	[IOS_LATENCY] = {
 	    ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_TOT_W_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_DISK_R_LAT_HISTO,
 	    ZPOOL_CONFIG_VDEV_DISK_W_LAT_HISTO,
+	    ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO,
+	    ZPOOL_CONFIG_VDEV_AUTO_TRIM_LAT_HISTO,
+	    ZPOOL_CONFIG_VDEV_MAN_TRIM_LAT_HISTO,
 	    NULL},
 	[IOS_QUEUES] = {
 	    ZPOOL_CONFIG_VDEV_SYNC_R_ACTIVE_QUEUE,
@@ -199,6 +206,8 @@
 	    ZPOOL_CONFIG_VDEV_ASYNC_R_ACTIVE_QUEUE,
 	    ZPOOL_CONFIG_VDEV_ASYNC_W_ACTIVE_QUEUE,
 	    ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE,
+	    ZPOOL_CONFIG_VDEV_AUTO_TRIM_ACTIVE_QUEUE,
+	    ZPOOL_CONFIG_VDEV_MAN_TRIM_ACTIVE_QUEUE,
 	    NULL},
 	[IOS_RQ_HISTO] = {
 	    ZPOOL_CONFIG_VDEV_SYNC_IND_R_HISTO,
@@ -211,6 +220,8 @@
 	    ZPOOL_CONFIG_VDEV_ASYNC_AGG_W_HISTO,
 	    ZPOOL_CONFIG_VDEV_IND_SCRUB_HISTO,
 	    ZPOOL_CONFIG_VDEV_AGG_SCRUB_HISTO,
+	    ZPOOL_CONFIG_VDEV_IND_AUTO_TRIM_HISTO,
+	    ZPOOL_CONFIG_VDEV_IND_MAN_TRIM_HISTO,
 	    NULL},
 };
 
@@ -262,6 +273,8 @@
 	{ NULL },
 	{ "scrub",	zpool_do_scrub,		HELP_SCRUB		},
 	{ NULL },
+	{ "trim",	zpool_do_trim,		HELP_TRIM		},
+	{ NULL },
 	{ "import",	zpool_do_import,	HELP_IMPORT		},
 	{ "export",	zpool_do_export,	HELP_EXPORT		},
 	{ "upgrade",	zpool_do_upgrade,	HELP_UPGRADE		},
@@ -338,6 +351,8 @@
 		return (gettext("\treopen <pool>\n"));
 	case HELP_SCRUB:
 		return (gettext("\tscrub [-s] <pool> ...\n"));
+	case HELP_TRIM:
+		return (gettext("\ttrim [-s|-r <rate>] <pool> ...\n"));
 	case HELP_STATUS:
 		return (gettext("\tstatus [-c [script1,script2,...]] [-gLPvxD]"
 		    "[-T d|u] [pool] ... [interval [count]]\n"));
@@ -2713,21 +2728,22 @@
 	unsigned int columns;	/* Center name to this number of columns */
 } name_and_columns_t;
 
-#define	IOSTAT_MAX_LABELS	11	/* Max number of labels on one line */
+#define	IOSTAT_MAX_LABELS	15	/* Max number of labels on one line */
 
 static const name_and_columns_t iostat_top_labels[][IOSTAT_MAX_LABELS] =
 {
 	[IOS_DEFAULT] = {{"capacity", 2}, {"operations", 2}, {"bandwidth", 2},
 	    {NULL}},
 	[IOS_LATENCY] = {{"total_wait", 2}, {"disk_wait", 2}, {"syncq_wait", 2},
-	    {"asyncq_wait", 2}, {"scrub"}},
+	    {"asyncq_wait", 2}, {"scrub"}, {"atrim"}, {"mtrim"}},
 	[IOS_QUEUES] = {{"syncq_read", 2}, {"syncq_write", 2},
 	    {"asyncq_read", 2}, {"asyncq_write", 2}, {"scrubq_read", 2},
-	    {NULL}},
+	    {"auto_trimq", 2}, {"man_trimq", 2}, {NULL}},
 	[IOS_L_HISTO] = {{"total_wait", 2}, {"disk_wait", 2},
 	    {"sync_queue", 2}, {"async_queue", 2}, {NULL}},
 	[IOS_RQ_HISTO] = {{"sync_read", 2}, {"sync_write", 2},
-	    {"async_read", 2}, {"async_write", 2}, {"scrub", 2}, {NULL}},
+	    {"async_read", 2}, {"async_write", 2}, {"scrub", 2},
+	    {"trim", 2}, {NULL}},
 
 };
 
@@ -2737,13 +2753,16 @@
 	[IOS_DEFAULT] = {{"alloc"}, {"free"}, {"read"}, {"write"}, {"read"},
 	    {"write"}, {NULL}},
 	[IOS_LATENCY] = {{"read"}, {"write"}, {"read"}, {"write"}, {"read"},
-	    {"write"}, {"read"}, {"write"}, {"wait"}, {NULL}},
+	    {"write"}, {"read"}, {"write"}, {"wait"}, {"wait"},
+	    {"wait"}, {NULL}},
 	[IOS_QUEUES] = {{"pend"}, {"activ"}, {"pend"}, {"activ"}, {"pend"},
-	    {"activ"}, {"pend"}, {"activ"}, {"pend"}, {"activ"}, {NULL}},
+	    {"activ"}, {"pend"}, {"activ"}, {"pend"}, {"activ"},
+	    {"pend"}, {"activ"}, {"pend"}, {"activ"}, {NULL}},
 	[IOS_L_HISTO] = {{"read"}, {"write"}, {"read"}, {"write"}, {"read"},
-	    {"write"}, {"read"}, {"write"}, {"scrub"}, {NULL}},
+	    {"write"}, {"read"}, {"write"}, {"scrub"}, {"atrim"},
+	    {"mtrim"}, {NULL}},
 	[IOS_RQ_HISTO] = {{"ind"}, {"agg"}, {"ind"}, {"agg"}, {"ind"}, {"agg"},
-	    {"ind"}, {"agg"}, {"ind"}, {"agg"}, {NULL}},
+	    {"ind"}, {"agg"}, {"ind"}, {"agg"}, {"auto"}, {"man"}, {NULL}},
 };
 
 static const char *histo_to_title[] = {
@@ -3367,6 +3386,10 @@
 		ZPOOL_CONFIG_VDEV_ASYNC_W_ACTIVE_QUEUE,
 		ZPOOL_CONFIG_VDEV_SCRUB_PEND_QUEUE,
 		ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE,
+		ZPOOL_CONFIG_VDEV_AUTO_TRIM_PEND_QUEUE,
+		ZPOOL_CONFIG_VDEV_AUTO_TRIM_ACTIVE_QUEUE,
+		ZPOOL_CONFIG_VDEV_MAN_TRIM_PEND_QUEUE,
+		ZPOOL_CONFIG_VDEV_MAN_TRIM_ACTIVE_QUEUE,
 	};
 
 	struct stat_array *nva;
@@ -3405,6 +3428,8 @@
 		ZPOOL_CONFIG_VDEV_ASYNC_R_LAT_HISTO,
 		ZPOOL_CONFIG_VDEV_ASYNC_W_LAT_HISTO,
 		ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO,
+		ZPOOL_CONFIG_VDEV_AUTO_TRIM_LAT_HISTO,
+		ZPOOL_CONFIG_VDEV_MAN_TRIM_LAT_HISTO,
 	};
 	struct stat_array *nva;
 
@@ -5695,6 +5720,32 @@
 	return (err != 0);
 }
 
+typedef struct trim_cbdata {
+	boolean_t	cb_start;
+	uint64_t	cb_rate;
+	boolean_t	cb_fulltrim;
+} trim_cbdata_t;
+
+int
+trim_callback(zpool_handle_t *zhp, void *data)
+{
+	trim_cbdata_t *cb = data;
+	int err;
+
+	/*
+	 * Ignore faulted pools.
+	 */
+	if (zpool_get_state(zhp) == POOL_STATE_UNAVAIL) {
+		(void) fprintf(stderr, gettext("cannot trim '%s': pool is "
+		    "currently unavailable\n"), zpool_get_name(zhp));
+		return (1);
+	}
+
+	err = zpool_trim(zhp, cb->cb_start, cb->cb_rate, cb->cb_fulltrim);
+
+	return (err != 0);
+}
+
 /*
  * zpool scrub [-s] <pool> ...
  *
@@ -5735,6 +5786,57 @@
 }
 
 /*
+ * zpool trim [-s|-r <rate>] <pool> ...
+ *
+ *	-p		Partial trim.  Skips never-allocated space.
+ *	-s		Stop. Stops any in-progress trim.
+ *	-r <rate>	Sets the TRIM rate.
+ */
+int
+zpool_do_trim(int argc, char **argv)
+{
+	int c;
+	trim_cbdata_t cb;
+
+	cb.cb_start = B_TRUE;
+	cb.cb_rate = 0;
+	cb.cb_fulltrim = B_TRUE;
+
+	/* check options */
+	while ((c = getopt(argc, argv, "psr:")) != -1) {
+		switch (c) {
+		case 'p':
+			cb.cb_fulltrim = B_FALSE;
+			break;
+		case 's':
+			cb.cb_start = B_FALSE;
+			break;
+		case 'r':
+			if (zfs_nicestrtonum(NULL, optarg, &cb.cb_rate) == -1) {
+				(void) fprintf(stderr,
+				    gettext("invalid value for rate\n"));
+				usage(B_FALSE);
+			}
+			break;
+		case '?':
+			(void) fprintf(stderr, gettext("invalid option '%c'\n"),
+			    optopt);
+			usage(B_FALSE);
+		}
+	}
+
+	argc -= optind;
+	argv += optind;
+
+	if (argc < 1) {
+		(void) fprintf(stderr, gettext("missing pool name argument\n"));
+		usage(B_FALSE);
+	}
+
+	return (for_each_pool(argc, argv, B_TRUE, NULL, trim_callback, &cb));
+}
+
+/*
  * Print out detailed scrub status.
  */
 void
@@ -5847,6 +5949,59 @@
 }
 
 static void
+print_trim_status(uint64_t trim_prog, uint64_t total_size, uint64_t rate,
+    uint64_t start_time_u64, uint64_t end_time_u64)
+{
+	time_t start_time = start_time_u64, end_time = end_time_u64;
+	char *buf;
+
+	assert(trim_prog <= total_size);
+	if (trim_prog != 0 && trim_prog != total_size) {
+		buf = ctime(&start_time);
+		buf[strlen(buf) - 1] = '\0';	/* strip trailing newline */
+		if (rate != 0) {
+			char rate_str[32];
+			zfs_nicenum(rate, rate_str, sizeof (rate_str));
+			(void) printf("  trim: %.02f%%\tstarted: %s\t"
+			    "(rate: %s/s)\n", (((double)trim_prog) /
+			    total_size) * 100, buf, rate_str);
+		} else {
+			(void) printf("  trim: %.02f%%\tstarted: %s\t"
+			    "(rate: max)\n", (((double)trim_prog) /
+			    total_size) * 100, buf);
+		}
+	} else {
+		if (start_time != 0) {
+			/*
+			 * Non-zero start time means we were run at some point
+			 * in the past.
+			 */
+			if (end_time != 0) {
+				/* Non-zero end time means we completed */
+				time_t diff = end_time - start_time;
+				int hrs, mins;
+
+				buf = ctime(&end_time);
+				buf[strlen(buf) - 1] = '\0';
+				hrs = diff / 3600;
+				mins = (diff % 3600) / 60;
+				(void) printf(gettext("  trim: completed on %s "
+				    "(after %dh%dm)\n"), buf, hrs, mins);
+			} else {
+				buf = ctime(&start_time);
+				buf[strlen(buf) - 1] = '\0';
+				/* Zero end time means we were interrupted */
+				(void) printf(gettext("  trim: interrupted\t"
+				    "(started %s)\n"), buf);
+			}
+		} else {
+			/* trim was never run */
+			(void) printf(gettext("  trim: none requested\n"));
+		}
+	}
+}
+
+static void
 print_error_log(zpool_handle_t *zhp)
 {
 	nvlist_t *nverrlist = NULL;
@@ -5958,6 +6113,43 @@
 }
 
 /*
+ * Calculates the total space available on log devices on the pool.
+ * For whatever reason, this is not counted in the root vdev's space stats.
+ */
+static uint64_t
+zpool_slog_space(nvlist_t *nvroot)
+{
+	nvlist_t **newchild;
+	uint_t c, children;
+	uint64_t space = 0;
+
+	verify(nvlist_lookup_nvlist_array(nvroot, ZPOOL_CONFIG_CHILDREN,
+	    &newchild, &children) == 0);
+
+	for (c = 0; c < children; c++) {
+		uint64_t islog = B_FALSE;
+		vdev_stat_t *vs;
+		uint_t n;
+		uint_t n_subchildren = 1;
+		nvlist_t **subchild;
+
+		(void) nvlist_lookup_uint64(newchild[c], ZPOOL_CONFIG_IS_LOG,
+		    &islog);
+		if (!islog)
+			continue;
+		verify(nvlist_lookup_uint64_array(newchild[c],
+		    ZPOOL_CONFIG_VDEV_STATS, (uint64_t **)&vs, &n) == 0);
+
+		/* vdev can be non-leaf, so multiply by number of children */
+		(void) nvlist_lookup_nvlist_array(newchild[c],
+		    ZPOOL_CONFIG_CHILDREN, &subchild, &n_subchildren);
+		space += n_subchildren * vs->vs_space;
+	}
+
+	return (space);
+}
+
+/*
  * Display a summary of pool status.  Displays a summary such as:
  *
  *        pool: tank
@@ -6252,6 +6444,7 @@
 		nvlist_t **spares, **l2cache;
 		uint_t nspares, nl2cache;
 		pool_scan_stat_t *ps = NULL;
+		uint64_t trim_prog, trim_rate, trim_start_time, trim_stop_time;
 
 		(void) nvlist_lookup_uint64_array(nvroot,
 		    ZPOOL_CONFIG_SCAN_STATS, (uint64_t **)&ps, &c);
@@ -6262,6 +6455,24 @@
 		if (cbp->cb_namewidth < 10)
 			cbp->cb_namewidth = 10;
 
+		/* Grab trim stats if the pool supports it */
+		if (nvlist_lookup_uint64(config, ZPOOL_CONFIG_TRIM_PROG,
+		    &trim_prog) == 0 &&
+		    nvlist_lookup_uint64(config, ZPOOL_CONFIG_TRIM_RATE,
+		    &trim_rate) == 0 &&
+		    nvlist_lookup_uint64(config, ZPOOL_CONFIG_TRIM_START_TIME,
+		    &trim_start_time) == 0 &&
+		    nvlist_lookup_uint64(config, ZPOOL_CONFIG_TRIM_STOP_TIME,
+		    &trim_stop_time) == 0) {
+			/*
+			 * For whatever reason, root vdev_stats_t don't
+			 * include log devices.
+			 */
+			print_trim_status(trim_prog, vs->vs_space +
+			    zpool_slog_space(nvroot), trim_rate,
+			    trim_start_time, trim_stop_time);
+		}
+
 		(void) printf(gettext("config:\n\n"));
 		(void) printf(gettext("\t%-*s  %-8s %5s %5s %5s"),
 		    cbp->cb_namewidth, "NAME", "STATE", "READ", "WRITE",
diff -Nuar zfs-kmod-9999.orig/configure.ac zfs-kmod-9999/configure.ac
--- zfs-kmod-9999.orig/configure.ac	2017-05-06 11:03:36.975219625 +0200
+++ zfs-kmod-9999/configure.ac	2017-05-06 11:04:34.180030865 +0200
@@ -275,6 +275,7 @@
 	tests/zfs-tests/tests/functional/sparse/Makefile
 	tests/zfs-tests/tests/functional/threadsappend/Makefile
 	tests/zfs-tests/tests/functional/tmpfile/Makefile
+	tests/zfs-tests/tests/functional/trim/Makefile
 	tests/zfs-tests/tests/functional/truncate/Makefile
 	tests/zfs-tests/tests/functional/userquota/Makefile
 	tests/zfs-tests/tests/functional/upgrade/Makefile
diff -Nuar zfs-kmod-9999.orig/include/libzfs.h zfs-kmod-9999/include/libzfs.h
--- zfs-kmod-9999.orig/include/libzfs.h	2017-05-06 11:03:36.993219566 +0200
+++ zfs-kmod-9999/include/libzfs.h	2017-05-06 11:04:34.196030812 +0200
@@ -260,6 +260,8 @@
  * Functions to manipulate pool and vdev state
  */
 extern int zpool_scan(zpool_handle_t *, pool_scan_func_t);
+extern int zpool_trim(zpool_handle_t *, boolean_t start, uint64_t rate,
+    boolean_t fulltrim);
 extern int zpool_clear(zpool_handle_t *, const char *, nvlist_t *);
 extern int zpool_reguid(zpool_handle_t *);
 extern int zpool_reopen(zpool_handle_t *);
diff -Nuar zfs-kmod-9999.orig/include/linux/blkdev_compat.h zfs-kmod-9999/include/linux/blkdev_compat.h
--- zfs-kmod-9999.orig/include/linux/blkdev_compat.h	2017-05-06 11:03:36.994219562 +0200
+++ zfs-kmod-9999/include/linux/blkdev_compat.h	2017-05-06 11:04:34.207030776 +0200
@@ -423,6 +423,40 @@
 }
 
 /*
+ * bio_set_discard - Set the appropriate flags in a bio to indicate
+ * that the specific random of sectors should be discarded.
+ *
+ * 4.8 - 4.x API,
+ *   REQ_OP_DISCARD
+ *
+ * 2.6.36 - 4.7 API,
+ *   REQ_DISCARD
+ *
+ * 2.6.28 - 2.6.35 API,
+ *   BIO_RW_DISCARD
+ *
+ * In all cases the normal I/O path is used for discards.  The only
+ * difference is how the kernel tags individual I/Os as discards.
+ *
+ * Note that 2.6.32 era kernels provide both BIO_RW_DISCARD and REQ_DISCARD,
+ * where BIO_RW_DISCARD is the correct interface.  Therefore, it is important
+ * that the HAVE_BIO_RW_DISCARD check occur before the REQ_DISCARD check.
+ */
+static inline void
+bio_set_discard(struct bio *bio)
+{
+#if defined(HAVE_REQ_OP_DISCARD)
+	bio_set_op_attrs(bio, REQ_OP_DISCARD, 0);
+#elif defined(HAVE_BIO_RW_DISCARD)
+	bio_set_op_attrs(bio, (1 << BIO_RW_DISCARD), 0);
+#elif defined(REQ_DISCARD)
+	bio_set_op_attrs(bio, REQ_WRITE | REQ_DISCARD, 0);
+#else
+#error	"Allowing the build will cause discard requests to become writes."
+#endif
+}
+
+/*
  * 4.8 - 4.x API,
  *   REQ_OP_DISCARD
  *
diff -Nuar zfs-kmod-9999.orig/include/sys/dmu.h zfs-kmod-9999/include/sys/dmu.h
--- zfs-kmod-9999.orig/include/sys/dmu.h	2017-05-06 11:03:36.999219546 +0200
+++ zfs-kmod-9999/include/sys/dmu.h	2017-05-06 11:04:34.181030862 +0200
@@ -21,7 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2016 by Delphix. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  * Copyright (c) 2012, Joyent, Inc. All rights reserved.
  * Copyright 2014 HybridCluster. All rights reserved.
  * Copyright (c) 2014 Spectra Logic Corporation, All rights reserved.
@@ -326,6 +326,8 @@
 #define	DMU_POOL_EMPTY_BPOBJ		"empty_bpobj"
 #define	DMU_POOL_CHECKSUM_SALT		"org.illumos:checksum_salt"
 #define	DMU_POOL_VDEV_ZAP_MAP		"com.delphix:vdev_zap_map"
+#define	DMU_POOL_TRIM_START_TIME	"trim_start_time"
+#define	DMU_POOL_TRIM_STOP_TIME		"trim_stop_time"
 
 /*
  * Allocate an object from this objset.  The range of object numbers
diff -Nuar zfs-kmod-9999.orig/include/sys/fs/zfs.h zfs-kmod-9999/include/sys/fs/zfs.h
--- zfs-kmod-9999.orig/include/sys/fs/zfs.h	2017-05-06 11:03:37.005219526 +0200
+++ zfs-kmod-9999/include/sys/fs/zfs.h	2017-05-06 11:04:34.207030776 +0200
@@ -22,7 +22,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2014 by Delphix. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2013, Joyent, Inc. All rights reserved.
  */
 
@@ -220,6 +220,8 @@
 	ZPOOL_PROP_MAXBLOCKSIZE,
 	ZPOOL_PROP_TNAME,
 	ZPOOL_PROP_MAXDNODESIZE,
+	ZPOOL_PROP_FORCETRIM,
+	ZPOOL_PROP_AUTOTRIM,
 	ZPOOL_NUM_PROPS
 } zpool_prop_t;
 
@@ -566,6 +568,10 @@
 #define	ZPOOL_CONFIG_VDEV_ASYNC_R_ACTIVE_QUEUE	"vdev_async_r_active_queue"
 #define	ZPOOL_CONFIG_VDEV_ASYNC_W_ACTIVE_QUEUE	"vdev_async_w_active_queue"
 #define	ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE	"vdev_async_scrub_active_queue"
+#define	ZPOOL_CONFIG_VDEV_AUTO_TRIM_ACTIVE_QUEUE \
+	"vdev_async_auto_trim_active_queue"
+#define	ZPOOL_CONFIG_VDEV_MAN_TRIM_ACTIVE_QUEUE \
+	"vdev_async_man_trim_active_queue"
 
 /* Queue sizes */
 #define	ZPOOL_CONFIG_VDEV_SYNC_R_PEND_QUEUE	"vdev_sync_r_pend_queue"
@@ -573,6 +579,10 @@
 #define	ZPOOL_CONFIG_VDEV_ASYNC_R_PEND_QUEUE	"vdev_async_r_pend_queue"
 #define	ZPOOL_CONFIG_VDEV_ASYNC_W_PEND_QUEUE	"vdev_async_w_pend_queue"
 #define	ZPOOL_CONFIG_VDEV_SCRUB_PEND_QUEUE	"vdev_async_scrub_pend_queue"
+#define	ZPOOL_CONFIG_VDEV_AUTO_TRIM_PEND_QUEUE \
+	"vdev_async_auto_trim_pend_queue"
+#define	ZPOOL_CONFIG_VDEV_MAN_TRIM_PEND_QUEUE \
+	"vdev_async_man_trim_pend_queue"
 
 /* Latency read/write histogram stats */
 #define	ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO	"vdev_tot_r_lat_histo"
@@ -584,6 +594,8 @@
 #define	ZPOOL_CONFIG_VDEV_ASYNC_R_LAT_HISTO	"vdev_async_r_lat_histo"
 #define	ZPOOL_CONFIG_VDEV_ASYNC_W_LAT_HISTO	"vdev_async_w_lat_histo"
 #define	ZPOOL_CONFIG_VDEV_SCRUB_LAT_HISTO	"vdev_scrub_histo"
+#define	ZPOOL_CONFIG_VDEV_AUTO_TRIM_LAT_HISTO	"vdev_auto_trim_histo"
+#define	ZPOOL_CONFIG_VDEV_MAN_TRIM_LAT_HISTO	"vdev_man_trim_histo"
 
 /* Request size histograms */
 #define	ZPOOL_CONFIG_VDEV_SYNC_IND_R_HISTO	"vdev_sync_ind_r_histo"
@@ -596,6 +608,8 @@
 #define	ZPOOL_CONFIG_VDEV_ASYNC_AGG_R_HISTO	"vdev_async_agg_r_histo"
 #define	ZPOOL_CONFIG_VDEV_ASYNC_AGG_W_HISTO	"vdev_async_agg_w_histo"
 #define	ZPOOL_CONFIG_VDEV_AGG_SCRUB_HISTO	"vdev_agg_scrub_histo"
+#define	ZPOOL_CONFIG_VDEV_IND_AUTO_TRIM_HISTO	"vdev_ind_auto_trim_histo"
+#define	ZPOOL_CONFIG_VDEV_IND_MAN_TRIM_HISTO	"vdev_ind_man_trim_histo"
 
 /* vdev enclosure sysfs path */
 #define	ZPOOL_CONFIG_VDEV_ENC_SYSFS_PATH	"vdev_enc_sysfs_path"
@@ -652,6 +666,10 @@
 #define	ZPOOL_CONFIG_REMOVED		"removed"
 #define	ZPOOL_CONFIG_FRU		"fru"
 #define	ZPOOL_CONFIG_AUX_STATE		"aux_state"
+#define	ZPOOL_CONFIG_TRIM_PROG		"trim_prog"
+#define	ZPOOL_CONFIG_TRIM_RATE		"trim_rate"
+#define	ZPOOL_CONFIG_TRIM_START_TIME	"trim_start_time"
+#define	ZPOOL_CONFIG_TRIM_STOP_TIME	"trim_stop_time"
 
 /* Rewind policy parameters */
 #define	ZPOOL_REWIND_POLICY		"rewind-policy"
@@ -764,6 +782,15 @@
 } pool_scan_func_t;
 
 /*
+ * TRIM command configuration info.
+ */
+typedef struct trim_cmd_info_s {
+	uint64_t	tci_start;	/* B_TRUE = start; B_FALSE = stop */
+	uint64_t	tci_rate;	/* requested TRIM rate in bytes/sec */
+	uint64_t	tci_fulltrim;	/* B_TRUE=trim never allocated space */
+} trim_cmd_info_t;
+
+/*
  * ZIO types.  Needed to interpret vdev statistics below.
  */
 typedef enum zio_type {
@@ -896,6 +923,21 @@
 } vdev_stat_ex_t;
 
 /*
+ * Discard stats
+ *
+ * Aggregate statistics for all discards issued as part of a zio TRIM.
+ * They are merged with standard and extended stats when the zio is done.
+ */
+typedef struct vdev_stat_trim {
+	uint64_t	vsd_ops;
+	uint64_t	vsd_bytes;
+	uint64_t	vsd_ind_histo[VDEV_RQ_HISTO_BUCKETS];
+	uint64_t	vsd_queue_histo[VDEV_L_HISTO_BUCKETS];
+	uint64_t	vsd_disk_histo[VDEV_L_HISTO_BUCKETS];
+	uint64_t	vsd_total_histo[VDEV_L_HISTO_BUCKETS];
+} vdev_stat_trim_t;
+
+/*
  * DDT statistics.  Note: all fields should be 64-bit because this
  * is passed between kernel and userland as an nvlist uint64 array.
  */
@@ -1027,6 +1069,7 @@
 	ZFS_IOC_EVENTS_NEXT,
 	ZFS_IOC_EVENTS_CLEAR,
 	ZFS_IOC_EVENTS_SEEK,
+	ZFS_IOC_POOL_TRIM,
 
 	/*
 	 * FreeBSD - 1/64 numbers reserved.
diff -Nuar zfs-kmod-9999.orig/include/sys/Makefile.am zfs-kmod-9999/include/sys/Makefile.am
--- zfs-kmod-9999.orig/include/sys/Makefile.am	2017-05-06 11:03:36.996219556 +0200
+++ zfs-kmod-9999/include/sys/Makefile.am	2017-05-06 11:04:34.181030862 +0200
@@ -67,6 +67,7 @@
 	$(top_srcdir)/include/sys/trace_dnode.h \
 	$(top_srcdir)/include/sys/trace_multilist.h \
 	$(top_srcdir)/include/sys/trace_txg.h \
+	$(top_srcdir)/include/sys/trace_vdev.h \
 	$(top_srcdir)/include/sys/trace_zil.h \
 	$(top_srcdir)/include/sys/trace_zio.h \
 	$(top_srcdir)/include/sys/trace_zrlock.h \
diff -Nuar zfs-kmod-9999.orig/include/sys/metaslab.h zfs-kmod-9999/include/sys/metaslab.h
--- zfs-kmod-9999.orig/include/sys/metaslab.h	2017-05-06 11:03:37.005219526 +0200
+++ zfs-kmod-9999/include/sys/metaslab.h	2017-05-06 11:04:34.181030862 +0200
@@ -21,6 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2016 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _SYS_METASLAB_H
@@ -56,6 +57,8 @@
 void metaslab_sync_done(metaslab_t *, uint64_t);
 void metaslab_sync_reassess(metaslab_group_t *);
 uint64_t metaslab_block_maxsize(metaslab_t *);
+void metaslab_auto_trim(metaslab_t *, uint64_t, boolean_t);
+uint64_t metaslab_trim_mem_used(metaslab_t *);
 
 #define	METASLAB_HINTBP_FAVOR		0x0
 #define	METASLAB_HINTBP_AVOID		0x1
@@ -70,6 +73,7 @@
 void metaslab_free(spa_t *, const blkptr_t *, uint64_t, boolean_t);
 int metaslab_claim(spa_t *, const blkptr_t *, uint64_t);
 void metaslab_check_free(spa_t *, const blkptr_t *);
+zio_t *metaslab_trim_all(metaslab_t *, uint64_t *, uint64_t *, boolean_t *);
 void metaslab_fastwrite_mark(spa_t *, const blkptr_t *);
 void metaslab_fastwrite_unmark(spa_t *, const blkptr_t *);
 
@@ -107,6 +111,9 @@
 void metaslab_group_alloc_decrement(spa_t *, uint64_t, void *, int);
 void metaslab_group_alloc_verify(spa_t *, const blkptr_t *, void *);
 
+void metaslab_trimstats_create(spa_t *spa);
+void metaslab_trimstats_destroy(spa_t *spa);
+
 #ifdef	__cplusplus
 }
 #endif
diff -Nuar zfs-kmod-9999.orig/include/sys/metaslab_impl.h zfs-kmod-9999/include/sys/metaslab_impl.h
--- zfs-kmod-9999.orig/include/sys/metaslab_impl.h	2017-05-06 11:03:37.006219523 +0200
+++ zfs-kmod-9999/include/sys/metaslab_impl.h	2017-05-06 11:04:34.181030862 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2011, 2016 by Delphix. All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _SYS_METASLAB_IMPL_H
@@ -246,6 +247,11 @@
 	uint64_t		mg_histogram[RANGE_TREE_HISTOGRAM_SIZE];
 };
 
+typedef struct {
+	uint64_t	ts_birth;	/* TXG at which this trimset starts */
+	range_tree_t	*ts_tree;	/* tree of extents in the trimset */
+} metaslab_trimset_t;
+
 /*
  * This value defines the number of elements in the ms_lbas array. The value
  * of 64 was chosen as it covers all power of 2 buckets up to UINT64_MAX.
@@ -320,6 +326,11 @@
 	range_tree_t	*ms_alloctree[TXG_SIZE];
 	range_tree_t	*ms_tree;
 
+	metaslab_trimset_t *ms_cur_ts;	/* currently prepared trims */
+	metaslab_trimset_t *ms_prev_ts;	/* previous (aging) trims */
+	kcondvar_t	ms_trim_cv;
+	metaslab_trimset_t *ms_trimming_ts;
+
 	/*
 	 * The following range trees are accessed only from syncing context.
 	 * ms_free*tree only have entries while syncing, and are empty
@@ -330,6 +341,7 @@
 	range_tree_t	*ms_defertree[TXG_DEFER_SIZE];
 
 	boolean_t	ms_condensing;	/* condensing? */
+	kcondvar_t	ms_condensing_cv;
 	boolean_t	ms_condense_wanted;
 
 	/*
diff -Nuar zfs-kmod-9999.orig/include/sys/range_tree.h zfs-kmod-9999/include/sys/range_tree.h
--- zfs-kmod-9999.orig/include/sys/range_tree.h	2017-05-06 11:03:37.007219519 +0200
+++ zfs-kmod-9999/include/sys/range_tree.h	2017-05-06 11:04:34.181030862 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2013, 2014 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _SYS_RANGE_TREE_H
@@ -78,6 +79,9 @@
 range_tree_t *range_tree_create(range_tree_ops_t *ops, void *arg, kmutex_t *lp);
 void range_tree_destroy(range_tree_t *rt);
 boolean_t range_tree_contains(range_tree_t *rt, uint64_t start, uint64_t size);
+boolean_t range_tree_contains_part(range_tree_t *rt, uint64_t start,
+    uint64_t size);
+uint64_t range_tree_find_gap(range_tree_t *rt, uint64_t start, uint64_t size);
 uint64_t range_tree_space(range_tree_t *rt);
 void range_tree_verify(range_tree_t *rt, uint64_t start, uint64_t size);
 void range_tree_swap(range_tree_t **rtsrc, range_tree_t **rtdst);
diff -Nuar zfs-kmod-9999.orig/include/sys/spa.h zfs-kmod-9999/include/sys/spa.h
--- zfs-kmod-9999.orig/include/sys/spa.h	2017-05-06 11:03:37.008219516 +0200
+++ zfs-kmod-9999/include/sys/spa.h	2017-05-06 11:04:34.197030809 +0200
@@ -21,7 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2014 by Delphix. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2014 Spectra Logic Corporation, All rights reserved.
  * Copyright 2013 Saso Kiselkov. All rights reserved.
  */
@@ -589,6 +589,28 @@
 	SPA_IMPORT_ASSEMBLE
 } spa_import_type_t;
 
+/*
+ * Should we force sending TRIM commands even to devices which evidently
+ * don't support it?
+ *	OFF: no, only send to devices which indicated support
+ *	ON: yes, force send to everybody
+ */
+typedef enum {
+	SPA_FORCE_TRIM_OFF = 0,	/* default */
+	SPA_FORCE_TRIM_ON
+} spa_force_trim_t;
+
+/*
+ * Should we send TRIM commands in-line during normal pool operation while
+ * deleting stuff?
+ *	OFF: no
+ *	ON: yes
+ */
+typedef enum {
+	SPA_AUTO_TRIM_OFF = 0,	/* default */
+	SPA_AUTO_TRIM_ON
+} spa_auto_trim_t;
+
 /* state manipulation functions */
 extern int spa_open(const char *pool, spa_t **, void *tag);
 extern int spa_open_rewind(const char *pool, spa_t **, void *tag,
@@ -613,14 +635,15 @@
 extern void spa_scan_stat_init(spa_t *spa);
 extern int spa_scan_get_stats(spa_t *spa, pool_scan_stat_t *ps);
 
-#define	SPA_ASYNC_CONFIG_UPDATE	0x01
-#define	SPA_ASYNC_REMOVE	0x02
-#define	SPA_ASYNC_PROBE		0x04
-#define	SPA_ASYNC_RESILVER_DONE	0x08
-#define	SPA_ASYNC_RESILVER	0x10
-#define	SPA_ASYNC_AUTOEXPAND	0x20
-#define	SPA_ASYNC_REMOVE_DONE	0x40
-#define	SPA_ASYNC_REMOVE_STOP	0x80
+#define	SPA_ASYNC_CONFIG_UPDATE			0x01
+#define	SPA_ASYNC_REMOVE			0x02
+#define	SPA_ASYNC_PROBE				0x04
+#define	SPA_ASYNC_RESILVER_DONE			0x08
+#define	SPA_ASYNC_RESILVER			0x10
+#define	SPA_ASYNC_AUTOEXPAND			0x20
+#define	SPA_ASYNC_REMOVE_DONE			0x40
+#define	SPA_ASYNC_REMOVE_STOP			0x80
+#define	SPA_ASYNC_MAN_TRIM_TASKQ_DESTROY	0x100
 
 /*
  * Controls the behavior of spa_vdev_remove().
@@ -658,6 +681,13 @@
 extern int spa_scan(spa_t *spa, pool_scan_func_t func);
 extern int spa_scan_stop(spa_t *spa);
 
+/* trimming */
+extern void spa_man_trim(spa_t *spa, uint64_t rate, boolean_t fulltrim);
+extern void spa_man_trim_stop(spa_t *spa);
+extern void spa_get_trim_prog(spa_t *spa, uint64_t *prog, uint64_t *rate,
+    uint64_t *start_time, uint64_t *stop_time);
+extern void spa_trim_stop_wait(spa_t *spa);
+
 /* spa syncing */
 extern void spa_sync(spa_t *spa, uint64_t txg); /* only for DMU use */
 extern void spa_sync_allpools(void);
@@ -826,6 +856,8 @@
 extern uint64_t spa_delegation(spa_t *spa);
 extern objset_t *spa_meta_objset(spa_t *spa);
 extern uint64_t spa_deadman_synctime(spa_t *spa);
+extern spa_force_trim_t spa_get_force_trim(spa_t *spa);
+extern spa_auto_trim_t spa_get_auto_trim(spa_t *spa);
 
 /* Miscellaneous support routines */
 extern void spa_activate_mos_feature(spa_t *spa, const char *feature,
@@ -909,6 +941,11 @@
 /* asynchronous event notification */
 extern void spa_event_notify(spa_t *spa, vdev_t *vdev, const char *name);
 
+/* TRIM/UNMAP kstat update */
+extern void spa_trimstats_update(spa_t *spa, uint64_t extents, uint64_t bytes,
+    uint64_t extents_skipped, uint64_t bytes_skipped);
+extern void spa_trimstats_auto_slow_incr(spa_t *spa);
+
 #ifdef ZFS_DEBUG
 #define	dprintf_bp(bp, fmt, ...) do {				\
 	if (zfs_flags & ZFS_DEBUG_DPRINTF) {			\
diff -Nuar zfs-kmod-9999.orig/include/sys/spa_impl.h zfs-kmod-9999/include/sys/spa_impl.h
--- zfs-kmod-9999.orig/include/sys/spa_impl.h	2017-05-06 11:03:37.009219513 +0200
+++ zfs-kmod-9999/include/sys/spa_impl.h	2017-05-06 11:04:34.182030858 +0200
@@ -21,7 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2015 by Delphix. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2014 Spectra Logic Corporation, All rights reserved.
  * Copyright 2013 Saso Kiselkov. All rights reserved.
  * Copyright (c) 2016 Actifio, Inc. All rights reserved.
@@ -124,6 +124,8 @@
 	AVZ_ACTION_INITIALIZE
 } spa_avz_action_t;
 
+typedef struct spa_trimstats spa_trimstats_t;
+
 struct spa {
 	/*
 	 * Fields protected by spa_namespace_lock.
@@ -268,6 +270,31 @@
 	uint64_t	spa_deadman_synctime;	/* deadman expiration timer */
 	uint64_t	spa_all_vdev_zaps;	/* ZAP of per-vd ZAP obj #s */
 	spa_avz_action_t	spa_avz_action;	/* destroy/rebuild AVZ? */
+
+	/* TRIM */
+	uint64_t	spa_force_trim;		/* force sending trim? */
+	uint64_t	spa_auto_trim;		/* see spa_auto_trim_t */
+
+	kmutex_t	spa_auto_trim_lock;
+	kcondvar_t	spa_auto_trim_done_cv;	/* all autotrim thrd's exited */
+	uint64_t	spa_num_auto_trimming;	/* # of autotrim threads */
+	taskq_t		*spa_auto_trim_taskq;
+
+	kmutex_t	spa_man_trim_lock;
+	uint64_t	spa_man_trim_rate;	/* rate of trim in bytes/sec */
+	uint64_t	spa_num_man_trimming;	/* # of manual trim threads */
+	boolean_t	spa_man_trim_stop;	/* requested manual trim stop */
+	kcondvar_t	spa_man_trim_update_cv;	/* updates to TRIM settings */
+	kcondvar_t	spa_man_trim_done_cv;	/* manual trim has completed */
+	/* For details on trim start/stop times see spa_get_trim_prog. */
+	uint64_t	spa_man_trim_start_time;
+	uint64_t	spa_man_trim_stop_time;
+	taskq_t		*spa_man_trim_taskq;
+
+	/* TRIM/UNMAP kstats */
+	spa_trimstats_t	*spa_trimstats;		/* alloc'd by kstat_create */
+	kstat_t		*spa_trimstats_ks;
+
 	uint64_t	spa_errata;		/* errata issues detected */
 	spa_stats_t	spa_stats;		/* assorted spa statistics */
 	hrtime_t	spa_ccw_fail_time;	/* Conf cache write fail time */
@@ -292,6 +319,10 @@
 extern void spa_taskq_dispatch_sync(spa_t *, zio_type_t t, zio_taskq_type_t q,
     task_func_t *func, void *arg, uint_t flags);
 
+extern void spa_auto_trim_taskq_create(spa_t *spa);
+extern void spa_man_trim_taskq_create(spa_t *spa);
+extern void spa_auto_trim_taskq_destroy(spa_t *spa);
+extern void spa_man_trim_taskq_destroy(spa_t *spa);
 
 #ifdef	__cplusplus
 }
diff -Nuar zfs-kmod-9999.orig/include/sys/sysevent/eventdefs.h zfs-kmod-9999/include/sys/sysevent/eventdefs.h
--- zfs-kmod-9999.orig/include/sys/sysevent/eventdefs.h	2017-05-06 11:03:37.010219510 +0200
+++ zfs-kmod-9999/include/sys/sysevent/eventdefs.h	2017-05-06 11:04:34.182030858 +0200
@@ -112,6 +112,8 @@
 #define	ESC_ZFS_VDEV_AUTOEXPAND		"vdev_autoexpand"
 #define	ESC_ZFS_BOOTFS_VDEV_ATTACH	"bootfs_vdev_attach"
 #define	ESC_ZFS_POOL_REGUID		"pool_reguid"
+#define	ESC_ZFS_TRIM_START		"trim_start"
+#define	ESC_ZFS_TRIM_FINISH		"trim_finish"
 
 /*
  * datalink subclass definitions.
diff -Nuar zfs-kmod-9999.orig/include/sys/trace_vdev.h zfs-kmod-9999/include/sys/trace_vdev.h
--- zfs-kmod-9999.orig/include/sys/trace_vdev.h	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/include/sys/trace_vdev.h	2017-05-06 11:04:34.182030858 +0200
@@ -0,0 +1,121 @@
+/*
+ * CDDL HEADER START
+ *
+ * The contents of this file are subject to the terms of the
+ * Common Development and Distribution License (the "License").
+ * You may not use this file except in compliance with the License.
+ *
+ * You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+ * or http://www.opensolaris.org/os/licensing.
+ * See the License for the specific language governing permissions
+ * and limitations under the License.
+ *
+ * When distributing Covered Code, include this CDDL HEADER in each
+ * file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+ * If applicable, add the following below this CDDL HEADER, with the
+ * fields enclosed by brackets "[]" replaced with your own identifying
+ * information: Portions Copyright [yyyy] [name of copyright owner]
+ *
+ * CDDL HEADER END
+ */
+
+#if defined(_KERNEL) && defined(HAVE_DECLARE_EVENT_CLASS)
+
+#undef TRACE_SYSTEM
+#define	TRACE_SYSTEM zfs
+
+#undef TRACE_SYSTEM_VAR
+#define	TRACE_SYSTEM_VAR zfs_vdev
+
+#if !defined(_TRACE_VDEV_H) || defined(TRACE_HEADER_MULTI_READ)
+#define	_TRACE_VDEV_H
+
+#include <linux/tracepoint.h>
+#include <sys/types.h>
+
+/*
+ * Support for tracepoints of the form:
+ *
+ * DTRACE_PROBE3(...,
+ *      vdev_t *vd, ...,
+ *	uint64_t mused, ...,
+ *	uint64_t mlim, ...,
+ */
+/* BEGIN CSTYLED */
+DECLARE_EVENT_CLASS(zfs_vdev_mused_mlim_class,
+	TP_PROTO(vdev_t *vd, uint64_t mused, uint64_t mlim),
+	TP_ARGS(vd, mused, mlim),
+	TP_STRUCT__entry(
+	    __field(uint64_t,	vdev_id)
+	    __field(uint64_t,	vdev_guid)
+	    __field(uint64_t,	mused)
+	    __field(uint64_t,	mlim)
+	),
+	TP_fast_assign(
+	    __entry->vdev_id		= vd->vdev_id;
+	    __entry->vdev_guid		= vd->vdev_guid;
+	    __entry->mused		= mused;
+	    __entry->mlim		= mlim;
+	),
+	TP_printk("vd { vdev_id %llu vdev_guid %llu }"
+	    " mused = %llu mlim = %llu",
+	    __entry->vdev_id, __entry->vdev_guid,
+	    __entry->mused, __entry->mlim)
+);
+/* END CSTYLED */
+
+/* BEGIN CSTYLED */
+#define DEFINE_VDEV_MUSED_MLIM_EVENT(name) \
+DEFINE_EVENT(zfs_vdev_mused_mlim_class, name, \
+	TP_PROTO(vdev_t *vd, uint64_t mused, uint64_t mlim), \
+	TP_ARGS(vd, mused, mlim))
+/* END CSTYLED */
+DEFINE_VDEV_MUSED_MLIM_EVENT(zfs_autotrim__mem__lim);
+
+/*
+ * Generic support for tracepoints of the form:
+ *
+ * DTRACE_PROBE1(...,
+ *      metaslab_t *, ...,
+ */
+/* BEGIN CSTYLED */
+DECLARE_EVENT_CLASS(zfs_msp_class,
+	TP_PROTO(metaslab_t *msp),
+	TP_ARGS(msp),
+	TP_STRUCT__entry(
+	    __field(uint64_t,	ms_id)
+	    __field(uint64_t,	ms_start)
+	    __field(uint64_t,	ms_size)
+	    __field(uint64_t,	ms_fragmentation)
+	),
+	TP_fast_assign(
+	    __entry->ms_id		= msp->ms_id;
+	    __entry->ms_start		= msp->ms_start;
+	    __entry->ms_size		= msp->ms_size;
+	    __entry->ms_fragmentation	= msp->ms_fragmentation;
+	),
+	TP_printk("msp { ms_id %llu ms_start %llu ms_size %llu "
+	    "ms_fragmentation %llu }",
+	    __entry->ms_id, __entry->ms_start,
+	    __entry->ms_size, __entry->ms_fragmentation)
+);
+/* END CSTYLED */
+
+/* BEGIN CSTYLED */
+#define	DEFINE_MSP_EVENT(name) \
+DEFINE_EVENT(zfs_msp_class, name, \
+	TP_PROTO(metaslab_t *msp), \
+	TP_ARGS(msp))
+/* END CSTYLED */
+DEFINE_MSP_EVENT(zfs_preserve__spilled);
+DEFINE_MSP_EVENT(zfs_drop__spilled);
+
+#endif /* _TRACE_VDEV_H */
+
+#undef TRACE_INCLUDE_PATH
+#undef TRACE_INCLUDE_FILE
+#define	TRACE_INCLUDE_PATH sys
+#define	TRACE_INCLUDE_FILE trace_vdev
+#include <trace/define_trace.h>
+
+#endif /* _KERNEL && HAVE_DECLARE_EVENT_CLASS */
diff -Nuar zfs-kmod-9999.orig/include/sys/vdev.h zfs-kmod-9999/include/sys/vdev.h
--- zfs-kmod-9999.orig/include/sys/vdev.h	2017-05-06 11:03:37.024219463 +0200
+++ zfs-kmod-9999/include/sys/vdev.h	2017-05-06 11:04:34.208030773 +0200
@@ -22,6 +22,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2013 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _SYS_VDEV_H
@@ -45,6 +46,22 @@
 	DTL_TYPES
 } vdev_dtl_type_t;
 
+typedef struct vdev_trim_info {
+	vdev_t *vti_vdev;
+	uint64_t vti_txg;	/* ignored for manual trim */
+	void (*vti_done_cb)(void *);
+	void *vti_done_arg;
+} vdev_trim_info_t;
+
+typedef enum vdev_trim_stat_flags
+{
+	TRIM_STAT_OP		= 1 << 0,
+	TRIM_STAT_RQ_HISTO	= 1 << 1,
+	TRIM_STAT_L_HISTO	= 1 << 2,
+} vdev_trim_stat_flags_t;
+
+#define	TRIM_STAT_ALL	(TRIM_STAT_OP | TRIM_STAT_RQ_HISTO | TRIM_STAT_L_HISTO)
+
 extern int zfs_nocacheflush;
 
 extern int vdev_open(vdev_t *);
@@ -89,6 +106,8 @@
 extern void vdev_get_stats(vdev_t *vd, vdev_stat_t *vs);
 extern void vdev_clear_stats(vdev_t *vd);
 extern void vdev_stat_update(zio_t *zio, uint64_t psize);
+extern void vdev_trim_stat_update(zio_t *zio, uint64_t psize,
+    vdev_trim_stat_flags_t flags);
 extern void vdev_scan_stat_init(vdev_t *vd);
 extern void vdev_propagate_state(vdev_t *vd);
 extern void vdev_set_state(vdev_t *vd, boolean_t isopen, vdev_state_t state,
@@ -145,6 +164,11 @@
 extern nvlist_t *vdev_config_generate(spa_t *spa, vdev_t *vd,
     boolean_t getstats, vdev_config_flag_t flags);
 
+extern void vdev_man_trim(vdev_trim_info_t *vti);
+extern void vdev_man_trim_full(vdev_trim_info_t *vti);
+extern void vdev_auto_trim(vdev_trim_info_t *vti);
+extern void vdev_trim_stop_wait(vdev_t *vd);
+
 /*
  * Label routines
  */
diff -Nuar zfs-kmod-9999.orig/include/sys/vdev_impl.h zfs-kmod-9999/include/sys/vdev_impl.h
--- zfs-kmod-9999.orig/include/sys/vdev_impl.h	2017-05-06 11:03:37.025219460 +0200
+++ zfs-kmod-9999/include/sys/vdev_impl.h	2017-05-06 11:04:34.182030858 +0200
@@ -21,6 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2015 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _SYS_VDEV_IMPL_H
@@ -70,6 +71,8 @@
 typedef void	vdev_state_change_func_t(vdev_t *vd, int, int);
 typedef void	vdev_hold_func_t(vdev_t *vd);
 typedef void	vdev_rele_func_t(vdev_t *vd);
+typedef void	vdev_trim_func_t(vdev_t *vd, zio_t *pio,
+    dkioc_free_list_t *trim_exts, boolean_t auto_trim);
 
 typedef const struct vdev_ops {
 	vdev_open_func_t		*vdev_op_open;
@@ -80,6 +83,7 @@
 	vdev_state_change_func_t	*vdev_op_state_change;
 	vdev_hold_func_t		*vdev_op_hold;
 	vdev_rele_func_t		*vdev_op_rele;
+	vdev_trim_func_t		*vdev_op_trim;
 	char				vdev_op_type[16];
 	boolean_t			vdev_op_leaf;
 } vdev_ops_t;
@@ -186,6 +190,20 @@
 	kmutex_t	vdev_queue_lock; /* protects vdev_queue_depth	*/
 	uint64_t	vdev_top_zap;
 
+	boolean_t	vdev_man_trimming; /* manual trim is ongoing	*/
+	uint64_t	vdev_trim_prog;	/* trim progress in bytes	*/
+	/*
+	 * Because trim zios happen outside of the DMU transactional engine,
+	 * we cannot rely on the DMU quiescing async trim zios to the vdev
+	 * before doing pool reconfiguration tasks. Therefore we count them
+	 * separately and quiesce them using vdev_trim_stop_wait before
+	 * removing or changing vdevs.
+	 */
+	kmutex_t	vdev_trim_zios_lock;
+	kcondvar_t	vdev_trim_zios_cv;
+	uint64_t	vdev_trim_zios;	/* # of in-flight async trim zios */
+	boolean_t	vdev_trim_zios_stop;	/* see zio_trim_should_bypass */
+
 	/*
 	 * The queue depth parameters determine how many async writes are
 	 * still pending (i.e. allocated by net yet issued to disk) per
@@ -219,6 +237,7 @@
 	uint64_t	vdev_not_present; /* not present during import	*/
 	uint64_t	vdev_unspare;	/* unspare when resilvering done */
 	boolean_t	vdev_nowritecache; /* true if flushwritecache failed */
+	boolean_t	vdev_notrim;	/* true if Unmap/TRIM is unsupported */
 	boolean_t	vdev_checkremove; /* temporary online test	*/
 	boolean_t	vdev_forcefault; /* force online fault		*/
 	boolean_t	vdev_splitting;	/* split or repair in progress  */
@@ -347,6 +366,7 @@
 extern void vdev_sync(vdev_t *vd, uint64_t txg);
 extern void vdev_sync_done(vdev_t *vd, uint64_t txg);
 extern void vdev_dirty(vdev_t *vd, int flags, void *arg, uint64_t txg);
+extern boolean_t vdev_is_dirty(vdev_t *vd, int flags, void *arg);
 extern void vdev_dirty_leaves(vdev_t *vd, int flags, uint64_t txg);
 
 /*
diff -Nuar zfs-kmod-9999.orig/include/sys/zfs_context.h zfs-kmod-9999/include/sys/zfs_context.h
--- zfs-kmod-9999.orig/include/sys/zfs_context.h	2017-05-06 11:03:37.028219450 +0200
+++ zfs-kmod-9999/include/sys/zfs_context.h	2017-05-06 11:04:34.183030855 +0200
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2012, 2016 by Delphix. All rights reserved.
  * Copyright (c) 2012, Joyent, Inc. All rights reserved.
  */
@@ -599,6 +599,8 @@
 
 #define	CRCREAT		0
 
+#define	F_FREESP	11
+
 extern int fop_getattr(vnode_t *vp, vattr_t *vap);
 
 #define	VOP_CLOSE(vp, f, c, o, cr, ct)	vn_close(vp)
@@ -607,6 +609,16 @@
 
 #define	VOP_FSYNC(vp, f, cr, ct)	fsync((vp)->v_fd)
 
+#if defined(HAVE_FILE_FALLOCATE) && \
+	defined(FALLOC_FL_PUNCH_HOLE) && \
+	defined(FALLOC_FL_KEEP_SIZE)
+#define	VOP_SPACE(vp, cmd, flck, fl, off, cr, ct) \
+	fallocate((vp)->v_fd, FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE, \
+	    (flck)->l_start, (flck)->l_len)
+#else
+#define	VOP_SPACE(vp, cmd, flck, fl, off, cr, ct) (0)
+#endif
+
 #define	VN_RELE(vp)	vn_close(vp)
 
 extern int vn_open(char *path, int x1, int oflags, int mode, vnode_t **vpp,
diff -Nuar zfs-kmod-9999.orig/include/sys/zio.h zfs-kmod-9999/include/sys/zio.h
--- zfs-kmod-9999.orig/include/sys/zio.h	2017-05-06 11:03:37.034219430 +0200
+++ zfs-kmod-9999/include/sys/zio.h	2017-05-06 11:04:34.208030773 +0200
@@ -21,11 +21,11 @@
 
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc. All rights reserved.
  * Copyright (c) 2012, 2017 by Delphix. All rights reserved.
  * Copyright (c) 2013 by Saso Kiselkov. All rights reserved.
  * Copyright (c) 2013, Joyent, Inc. All rights reserved.
  * Copyright 2016 Toomas Soome <tsoome@me.com>
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  */
 
 #ifndef _ZIO_H
@@ -38,6 +38,8 @@
 #include <sys/avl.h>
 #include <sys/fs/zfs.h>
 #include <sys/zio_impl.h>
+#include <sys/dkio.h>
+#include <sys/dkioc_free_util.h>
 
 #ifdef	__cplusplus
 extern "C" {
@@ -240,6 +242,10 @@
 
 extern int zio_dva_throttle_enabled;
 extern const char *zio_type_name[ZIO_TYPES];
+extern int zfs_trim;
+extern int zfs_trim_sync;
+
+struct range_tree;
 
 /*
  * A bookmark is a four-tuple <objset, object, level, blkid> that uniquely
@@ -294,6 +300,9 @@
 	(zb)->zb_level == ZB_ROOT_LEVEL &&	\
 	(zb)->zb_blkid == ZB_ROOT_BLKID)
 
+#define	ZIO_IS_TRIM(zio)	\
+	((zio)->io_type == ZIO_TYPE_IOCTL && (zio)->io_cmd == DKIOCFREE)
+
 typedef struct zio_prop {
 	enum zio_checksum	zp_checksum;
 	enum zio_compress	zp_compress;
@@ -419,6 +428,11 @@
 	uint64_t	io_size;
 	uint64_t	io_orig_size;
 
+	/* Used by trim zios */
+	dkioc_free_list_t	*io_dfl;
+	vdev_stat_trim_t	*io_dfl_stats;
+	boolean_t		io_dfl_free_on_destroy;
+
 	/* Stuff for the vdev stack */
 	vdev_t		*io_vd;
 	void		*io_vsd;
@@ -501,6 +515,14 @@
 extern zio_t *zio_ioctl(zio_t *pio, spa_t *spa, vdev_t *vd, int cmd,
     zio_done_func_t *done, void *private, enum zio_flag flags);
 
+extern zio_t *zio_trim_dfl(zio_t *pio, spa_t *spa, vdev_t *vd,
+    dkioc_free_list_t *dfl, boolean_t dfl_free_on_destroy, boolean_t auto_trim,
+    zio_done_func_t *done, void *private);
+
+extern zio_t *zio_trim_tree(zio_t *pio, spa_t *spa, vdev_t *vd,
+    struct range_tree *tree, boolean_t auto_trim, zio_done_func_t *done,
+    void *private, int dkiocfree_flags, metaslab_t *msp);
+
 extern zio_t *zio_read_phys(zio_t *pio, vdev_t *vd, uint64_t offset,
     uint64_t size, struct abd *data, int checksum,
     zio_done_func_t *done, void *private, zio_priority_t priority,
diff -Nuar zfs-kmod-9999.orig/include/sys/zio_impl.h zfs-kmod-9999/include/sys/zio_impl.h
--- zfs-kmod-9999.orig/include/sys/zio_impl.h	2017-05-06 11:03:37.035219427 +0200
+++ zfs-kmod-9999/include/sys/zio_impl.h	2017-05-06 11:04:34.183030855 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2012, 2015 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #ifndef _ZIO_IMPL_H
@@ -234,6 +235,11 @@
 	ZIO_STAGE_VDEV_IO_START |		\
 	ZIO_STAGE_VDEV_IO_ASSESS)
 
+#define	ZIO_TRIM_PIPELINE			\
+	(ZIO_INTERLOCK_STAGES |			\
+	ZIO_STAGE_ISSUE_ASYNC |			\
+	ZIO_VDEV_IO_STAGES)
+
 #define	ZIO_BLOCKING_STAGES			\
 	(ZIO_STAGE_DVA_ALLOCATE |		\
 	ZIO_STAGE_DVA_CLAIM |			\
diff -Nuar zfs-kmod-9999.orig/include/sys/zio_priority.h zfs-kmod-9999/include/sys/zio_priority.h
--- zfs-kmod-9999.orig/include/sys/zio_priority.h	2017-05-06 11:03:37.035219427 +0200
+++ zfs-kmod-9999/include/sys/zio_priority.h	2017-05-06 11:04:34.183030855 +0200
@@ -14,6 +14,7 @@
  */
 /*
  * Copyright (c) 2014 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 #ifndef	_ZIO_PRIORITY_H
 #define	_ZIO_PRIORITY_H
@@ -28,6 +29,14 @@
 	ZIO_PRIORITY_ASYNC_READ,	/* prefetch */
 	ZIO_PRIORITY_ASYNC_WRITE,	/* spa_sync() */
 	ZIO_PRIORITY_SCRUB,		/* asynchronous scrub/resilver reads */
+	/*
+	 * Trims are separated into auto & manual trims. If a manual trim is
+	 * initiated, auto trims are discarded late in the zio pipeline just
+	 * prior to being issued. This lets manual trim start up much faster
+	 * if a lot of auto trims have already been queued up.
+	 */
+	ZIO_PRIORITY_AUTO_TRIM,		/* async auto trim operation */
+	ZIO_PRIORITY_MAN_TRIM,		/* manual trim operation */
 	ZIO_PRIORITY_NUM_QUEUEABLE,
 	ZIO_PRIORITY_NOW,		/* non-queued i/os (e.g. free) */
 } zio_priority_t;
diff -Nuar zfs-kmod-9999.orig/lib/libspl/include/sys/dkioc_free_util.h zfs-kmod-9999/lib/libspl/include/sys/dkioc_free_util.h
--- zfs-kmod-9999.orig/lib/libspl/include/sys/dkioc_free_util.h	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/lib/libspl/include/sys/dkioc_free_util.h	2017-05-06 11:04:34.184030852 +0200
@@ -0,0 +1,38 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2016 Nexenta Inc.  All rights reserved.
+ */
+
+#ifndef _SYS_DKIOC_FREE_UTIL_H
+#define	_SYS_DKIOC_FREE_UTIL_H
+
+#include <sys/dkio.h>
+
+#ifdef	__cplusplus
+extern "C" {
+#endif
+
+static inline void dfl_free(dkioc_free_list_t *dfl) {
+	vmem_free(dfl, DFL_SZ(dfl->dfl_num_exts));
+}
+
+static inline dkioc_free_list_t *dfl_alloc(uint64_t dfl_num_exts, int flags) {
+	return (vmem_zalloc(DFL_SZ(dfl_num_exts), flags));
+}
+
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* _SYS_DKIOC_FREE_UTIL_H */
diff -Nuar zfs-kmod-9999.orig/lib/libspl/include/sys/dkio.h zfs-kmod-9999/lib/libspl/include/sys/dkio.h
--- zfs-kmod-9999.orig/lib/libspl/include/sys/dkio.h	2017-05-06 11:03:37.056219358 +0200
+++ zfs-kmod-9999/lib/libspl/include/sys/dkio.h	2017-05-06 11:04:34.211030763 +0200
@@ -18,17 +18,19 @@
  *
  * CDDL HEADER END
  */
+
 /*
- * Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
- * Use is subject to license terms.
+ * Copyright (c) 1982, 2010, Oracle and/or its affiliates. All rights reserved.
+ *
+ * Copyright 2016 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2012 DEY Storage Systems, Inc.  All rights reserved.
  */
 
 #ifndef _SYS_DKIO_H
 #define	_SYS_DKIO_H
 
-
-
 #include <sys/dklabel.h>	/* Needed for NDKMAP define */
+#include <sys/int_limits.h>	/* Needed for UINT16_MAX */
 
 #ifdef	__cplusplus
 extern "C" {
@@ -83,9 +85,10 @@
 #define	DKC_MD		16	/* meta-disk (virtual-disk) driver */
 #define	DKC_INTEL82077	19	/* 82077 floppy disk controller */
 #define	DKC_DIRECT	20	/* Intel direct attached device i.e. IDE */
-#define	DKC_PCMCIA_MEM	21	/* PCMCIA memory disk-like type */
+#define	DKC_PCMCIA_MEM	21	/* PCMCIA memory disk-like type (Obsolete) */
 #define	DKC_PCMCIA_ATA	22	/* PCMCIA AT Attached type */
 #define	DKC_VBD		23	/* virtual block device */
+#define	DKC_BLKDEV	24	/* generic block device (see blkdev(7d)) */
 
 /*
  * Sun reserves up through 1023
@@ -166,6 +169,9 @@
 #define	DKIOCGVTOC	(DKIOC|11)		/* Get VTOC */
 #define	DKIOCSVTOC	(DKIOC|12)		/* Set VTOC & Write to Disk */
 
+#define	DKIOCGEXTVTOC	(DKIOC|23)	/* Get extended VTOC */
+#define	DKIOCSEXTVTOC	(DKIOC|24)	/* Set extended VTOC, Write to Disk */
+
 /*
  * Disk Cache Controls.  These ioctls should be supported by
  * all disk drivers.
@@ -228,6 +234,14 @@
  */
 #define	DKIOCHOTPLUGGABLE	(DKIOC|35)	/* is hotpluggable */
 
+#if defined(__i386) || defined(__amd64)
+/* ioctl to write extended partition structure into the disk */
+#define	DKIOCSETEXTPART	(DKIOC|46)
+#endif
+
+/* ioctl to report whether the disk is solid state or not - used for ZFS */
+#define	DKIOCSOLIDSTATE		(DKIOC|38)
+
 /*
  * Ioctl to force driver to re-read the alternate partition and rebuild
  * the internal defect map.
@@ -252,6 +266,9 @@
 };
 
 #define	DKIOCPARTINFO	(DKIOC|22)	/* Get partition or slice parameters */
+#define	DKIOCEXTPARTINFO (DKIOC|19)	/* Get extended partition or slice */
+					/* parameters */
+
 
 /*
  * Used by applications to get partition or slice information
@@ -268,6 +285,11 @@
 	int		p_length;
 };
 
+struct extpart_info {
+	diskaddr_t	p_start;
+	diskaddr_t	p_length;
+};
+
 /* The following ioctls are for Optical Memory Device */
 #define	DKIOC_EBP_ENABLE  (DKIOC|40)	/* enable by pass erase on write */
 #define	DKIOC_EBP_DISABLE (DKIOC|41)	/* disable by pass erase on write */
@@ -291,6 +313,16 @@
 #define	DKIOCGTEMPERATURE	(DKIOC|45)	/* get temperature */
 
 /*
+ * ioctl to get the media info including physical block size
+ */
+#define	DKIOCGMEDIAINFOEXT	(DKIOC|48)
+
+/*
+ * ioctl to determine whether media is write-protected
+ */
+#define	DKIOCREADONLY	(DKIOC|49)
+
+/*
  * Used for providing the temperature.
  */
 
@@ -314,6 +346,17 @@
 };
 
 /*
+ * Used for Media info or the current profile info
+ * including physical block size if supported.
+ */
+struct dk_minfo_ext {
+	uint_t		dki_media_type;	/* Media type or profile info */
+	uint_t		dki_lbsize;	/* Logical blocksize of media */
+	diskaddr_t	dki_capacity;	/* Capacity as # of dki_lbsize blks */
+	uint_t		dki_pbsize;	/* Physical blocksize of media */
+};
+
+/*
  * Media types or profiles known
  */
 #define	DK_UNKNOWN		0x00	/* Media inserted - type unknown */
@@ -358,6 +401,9 @@
 #define	DKIOCSETVOLCAP	(DKIOC | 26)	/* Set volume capabilities */
 #define	DKIOCDMR	(DKIOC | 27)	/* Issue a directed read */
 
+#define	DKIOCDUMPINIT	(DKIOC | 28)	/* Dumpify a zvol */
+#define	DKIOCDUMPFINI	(DKIOC | 29)	/* Un-Dumpify a zvol */
+
 typedef uint_t volcapinfo_t;
 
 typedef uint_t volcapset_t;
@@ -476,6 +522,38 @@
 #define	FW_TYPE_TEMP	0x0		/* temporary use */
 #define	FW_TYPE_PERM	0x1		/* permanent use */
 
+/*
+ * ioctl to free space (e.g. SCSI UNMAP) off a disk.
+ * Pass a dkioc_free_list_t containing a list of extents to be freed.
+ */
+#define	DKIOCFREE	(DKIOC|50)
+
+#define	DF_WAIT_SYNC	0x00000001	/* Wait for full write-out of free. */
+typedef struct dkioc_free_list_ext_s {
+	uint64_t		dfle_start;
+	uint64_t		dfle_length;
+} dkioc_free_list_ext_t;
+
+typedef struct dkioc_free_list_s {
+	uint64_t		dfl_flags;
+	uint64_t		dfl_num_exts;
+	int64_t			dfl_offset;
+
+	/*
+	 * N.B. this is only an internal debugging API! This is only called
+	 * from debug builds of sd for integrity self-checking. The reason it
+	 * isn't #ifdef DEBUG is because that breaks ABI compatibility when
+	 * mixing DEBUG and non-DEBUG kernel modules and the cost of having
+	 * a couple unused pointers is too low to justify that risk.
+	 */
+	void			(*dfl_ck_func)(uint64_t, uint64_t, void *);
+	void			*dfl_ck_arg;
+
+	dkioc_free_list_ext_t	dfl_exts[1];
+} dkioc_free_list_t;
+#define	DFL_SZ(num_exts) \
+	(sizeof (dkioc_free_list_t) +\
+	(num_exts - 1) * sizeof (dkioc_free_list_ext_t))
 
 #ifdef	__cplusplus
 }
diff -Nuar zfs-kmod-9999.orig/lib/libspl/include/sys/Makefile.am zfs-kmod-9999/lib/libspl/include/sys/Makefile.am
--- zfs-kmod-9999.orig/lib/libspl/include/sys/Makefile.am	2017-05-06 11:03:37.055219361 +0200
+++ zfs-kmod-9999/lib/libspl/include/sys/Makefile.am	2017-05-06 11:04:34.178030872 +0200
@@ -12,6 +12,7 @@
 	$(top_srcdir)/lib/libspl/include/sys/cred.h \
 	$(top_srcdir)/lib/libspl/include/sys/debug.h \
 	$(top_srcdir)/lib/libspl/include/sys/dkio.h \
+	$(top_srcdir)/lib/libspl/include/sys/dkioc_free_util.h \
 	$(top_srcdir)/lib/libspl/include/sys/dklabel.h \
 	$(top_srcdir)/lib/libspl/include/sys/errno.h \
 	$(top_srcdir)/lib/libspl/include/sys/feature_tests.h \
diff -Nuar zfs-kmod-9999.orig/lib/libzfs/libzfs_pool.c zfs-kmod-9999/lib/libzfs/libzfs_pool.c
--- zfs-kmod-9999.orig/lib/libzfs/libzfs_pool.c	2017-05-06 11:03:37.075219295 +0200
+++ zfs-kmod-9999/lib/libzfs/libzfs_pool.c	2017-05-06 11:04:34.199030802 +0200
@@ -1942,6 +1942,30 @@
 }
 
 /*
+ * Trim the pool.
+ */
+int
+zpool_trim(zpool_handle_t *zhp, boolean_t start, uint64_t rate,
+    boolean_t fulltrim)
+{
+	zfs_cmd_t zc = {"\0"};
+	char msg[1024];
+	libzfs_handle_t *hdl = zhp->zpool_hdl;
+	trim_cmd_info_t tci = { .tci_start = start, .tci_rate = rate,
+	    .tci_fulltrim = fulltrim };
+
+	(void) strlcpy(zc.zc_name, zhp->zpool_name, sizeof (zc.zc_name));
+	zc.zc_cookie = (uintptr_t)&tci;
+
+	if (zfs_ioctl(hdl, ZFS_IOC_POOL_TRIM, &zc) == 0)
+		return (0);
+
+	(void) snprintf(msg, sizeof (msg),
+	    dgettext(TEXT_DOMAIN, "cannot trim %s"), zc.zc_name);
+	return (zpool_standard_error(hdl, errno, msg));
+}
+
+/*
  * Find a vdev that matches the search criteria specified. We use the
  * the nvpair name to determine how we should look for the device.
  * 'avail_spare' is set to TRUE if the provided guid refers to an AVAIL
diff -Nuar zfs-kmod-9999.orig/lib/libzfs/libzfs_util.c zfs-kmod-9999/lib/libzfs/libzfs_util.c
--- zfs-kmod-9999.orig/lib/libzfs/libzfs_util.c	2017-05-06 11:03:37.076219292 +0200
+++ zfs-kmod-9999/lib/libzfs/libzfs_util.c	2017-05-06 11:04:34.185030848 +0200
@@ -24,6 +24,7 @@
  * Copyright (c) 2013, Joyent, Inc. All rights reserved.
  * Copyright (c) 2011, 2014 by Delphix. All rights reserved.
  * Copyright 2016 Igor Kozhukhov <ikozhukhov@gmail.com>
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 /*
diff -Nuar zfs-kmod-9999.orig/man/man5/zfs-module-parameters.5 zfs-kmod-9999/man/man5/zfs-module-parameters.5
--- zfs-kmod-9999.orig/man/man5/zfs-module-parameters.5	2017-05-06 11:03:37.081219275 +0200
+++ zfs-kmod-9999/man/man5/zfs-module-parameters.5	2017-05-06 11:04:34.208030773 +0200
@@ -1681,6 +1681,49 @@
 .sp
 .ne 2
 .na
+\fBzfs_trim\fR (int)
+.ad
+.RS 12n
+Controls whether the underlying vdevs of the pool are notified when
+space is freed using the device-type-specific command set (TRIM here
+being a general placeholder term rather than referring to just the SATA
+TRIM command). This is frequently used on backing storage devices which
+support thin provisioning or pre-erasure of blocks on flash media.
+.sp
+Default value: \fB1\fR.
+.RE
+
+.sp
+.ne 2
+.na
+\fBzfs_trim_min_ext_sz\fR (int)
+.ad
+.RS 12n
+Minimum size region in bytes over which a device-specific TRIM command
+will be sent to the underlying vdevs when \fBzfs_trim\fR is set.
+.sp
+Default value: \fB131072\fR.
+.RE
+
+.sp
+.ne 2
+.na
+\fBzfs_trim_sync\fR (int)
+.ad
+.RS 12n
+Controls whether the underlying vdevs should issue TRIM commands synchronously
+or asynchronously.  When set for synchronous operation extents to TRIM are
+processed sequentially with each extent waiting for the last to complete.
+In asynchronous mode TRIM commands for all provided extents are submitted
+concurrently to the underlying vdev.  The optimal strategy depends on how
+the physical device handles TRIM commands.
+.sp
+Default value: \fB1\fR.
+.RE
+
+.sp
+.ne 2
+.na
 \fBzfs_txg_history\fR (int)
 .ad
 .RS 12n
@@ -1702,6 +1745,18 @@
 .RE
 
 .sp
+.ne 2
+.na
+\fBzfs_txgs_per_trim\fR (int)
+.ad
+.RS 12n
+Number of transaction groups over which device-specific TRIM commands
+are batched when \fBzfs_trim\fR is set.
+.sp
+Default value: \fB32\fR.
+.RE
+
+.sp
 .ne 2
 .na
 \fBzfs_vdev_aggregation_limit\fR (int)
diff -Nuar zfs-kmod-9999.orig/man/man8/zpool.8 zfs-kmod-9999/man/man8/zpool.8
--- zfs-kmod-9999.orig/man/man8/zpool.8	2017-05-06 11:03:37.087219255 +0200
+++ zfs-kmod-9999/man/man8/zpool.8	2017-05-06 11:04:34.209030769 +0200
@@ -149,6 +149,11 @@
 
 .LP
 .nf
+\fBzpool trim\fR [\fB-p\fR] [\fB-r \fIrate\fR|\fB-s\fR] \fIpool\fR ...
+.fi
+
+.LP
+.nf
 \fBzpool set\fR \fIproperty\fR=\fIvalue\fR \fIpool\fR
 .fi
 
@@ -711,7 +716,6 @@
 .RS 12n
 Prints out a message to the console and generates a system crash dump.
 .RE
-
 .RE
 
 .sp
@@ -729,6 +733,49 @@
 .sp
 .ne 2
 .na
+\fB\fBautotrim\fR=\fBon\fR | \fBoff\fR\fR
+.ad
+.sp .6
+.RS 4n
+When set to \fBon\fR, while deleting data, ZFS will inform the underlying
+vdevs of any blocks that have been marked as freed. This allows thinly
+provisioned vdevs to reclaim unused blocks. This feature is supported on
+file vdevs via hole punching if it is supported by their underlying file system
+and on block device vdevs if their underlying driver supports BLKDISCARD.
+The default setting for this property is \fBoff\fR.
+.sp
+Please note that automatic trimming of data blocks can put significant
+stress on the underlying storage devices if they do not handle these
+commands in a background, low-priority manner. In that case, it may be
+possible to achieve most of the benefits of trimming free space on the
+pool by running an on-demand (manual) trim every once in a while during
+a maintenance window using the \fBzpool trim\fR command.
+.sp
+Automatic trim does not reclaim blocks after a delete immediately.
+Instead, it waits approximately 2-4 minutes to allow for more efficient
+aggregation of smaller portions of free space into fewer larger regions,
+as well as to allow for longer pool corruption recovery via \fBzpool
+import -F\fR.
+.RE
+
+.sp
+.ne 2
+.na
+\fB\fBforcetrim\fR=\fBon\fR | \fBoff\fR\fR
+.ad
+.sp .6
+.RS 4n
+Controls whether device support is taken into consideration when issuing
+TRIM commands to the underlying vdevs of the pool. Normally, both
+automatic trim and on-demand (manual) trim only issue TRIM commands if a
+vdev indicates support for it. Setting the \fBforcetrim\fR property to
+\fBon\fR will force ZFS to issue TRIMs even if it thinks a device does
+not support it. The default is \fBoff\fR.
+.RE
+
+.sp
+.ne 2
+.na
 \fB\fBlistsnapshots\fR=on | off\fR
 .ad
 .sp .6
@@ -1636,11 +1683,13 @@
 \fB\fB-r\fR\fR
 .ad
 .RS 12n
-Print request size histograms for the leaf ZIOs.  This includes histograms of
-individual ZIOs ("ind") and aggregate ZIOs ("agg").  These stats can be useful
-for seeing how well the ZFS IO aggregator is working.  Do not confuse these
-request size stats with the block layer requests; it's possible ZIOs can
-be broken up before being sent to the block device.
+Print request size histograms for the leaf vdev's IO.  This includes histograms
+of individual IOs ("ind") and aggregate IOs ("agg").  TRIM IOs will not be
+aggregated and are split in to automatic ("auto") and manual ("man").  TRIM
+requests which exceed 16M in size are counted as 16M requests.  These stats
+can be useful for seeing how well the ZFS IO aggregator is working.  Do not
+confuse these request size stats with the block layer requests; it's possible
+these IOs will be broken up or merged before being sent to the block device.
 .RE
 
 .sp
@@ -1705,8 +1754,13 @@
 .ad
 .RS 20n
 Amount of time IO spent in scrub queue. Does not include disk time.
-
-
+.RE
+.ne 2
+.na
+trim:
+.ad
+.RS 20n
+Amount of time IO spent in trim queue. Does not include disk time.
 .RE
 
 All histogram buckets are power-of-two sized.  The time labels are the end
@@ -1760,6 +1814,13 @@
 .RS 20n
 Average queuing time in scrub queue.  Does not include disk time.
 .RE
+.ne 2
+.na
+trim:
+.ad
+.RS 20n
+Average queuing time in trim queue.  Does not include disk time.
+.RE
 
 .RE
 .sp
@@ -1794,6 +1855,13 @@
 .RS 20n
 Current number of entries in scrub queue.
 .RE
+.ne 2
+.na
+auto/man_trimq:
+.ad
+.RS 20n
+Current number of entries in automatic or manual trim queues.
+.RE
 
 All queue statistics are instantaneous measurements of the number of entries
 in the queues.  If you specify an interval, the measurements will be sampled
@@ -2033,6 +2101,75 @@
 .RE
 
 .RE
+
+.sp
+.ne 2
+.na
+\fB\fBzpool trim\fR [\fB-p\fR] [\fB-r \fIrate\fR|\fB-s\fR] \fIpool\fR ...\fR
+.ad
+.sp .6
+.RS 4n
+Initiates an immediate on-demand TRIM operation on all of the free space
+of a pool without delaying 2-4 minutes as it done for automatic trim.
+This informs the underlying storage devices of all of the blocks that
+the pool no longer considers allocated, thus allowing thinly provisioned
+storage devices to reclaim them.
+.sp
+Also note that an on-demand TRIM operation can be initiated irrespective of
+the \fBautotrim\fR zpool property setting. It does, however, respect the
+\fBforcetrim\fR zpool property.
+.sp
+An on-demand TRIM operation does not conflict with an ongoing scrub, but
+it can put significant I/O stress on the underlying vdevs. A resilver,
+however, automatically stops an on-demand TRIM operation. You can
+manually reinitiate the TRIM operation after the resilver has started,
+by simply reissuing the \fBzpool trim\fR command.
+.sp
+Adding a vdev during TRIM is supported, although the progression display
+in \fBzpool status\fR might not be entirely accurate in that case (TRIM
+will complete before reaching 100%). Removing or detaching a vdev will
+prematurely terminate an on-demand TRIM operation.
+.sp
+See the documentation for the \fBautotrim\fR property above for a description
+of the vdevs on which \fBzpool trim\fR is supported.
+.sp
+.ne 2
+.na
+\fB\fB-p\fR
+.ad
+.RS 6n
+Causes a "partial" trim to be initiated in which space never
+been allocated by ZFS is not trimmed.  This option is useful for certain
+storage backends such as large thinly-provisioned SANS on which large
+trim operations are slow.
+.RE
+.sp
+.ne 2
+.na
+\fB\fB-r\fR \fIrate\fR
+.ad
+.RS 6n
+Controls the speed at which the TRIM operation progresses.  Without this
+option, TRIM is executed as quickly as possible. The rate, expressed in
+bytes per second, is applied on a per-vdev basis; every top-level vdev
+in the pool tries to match this speed.  The requested rate is achieved
+by inserting delays between each TRIMmed region.
+.sp
+When an on-demand TRIM operation is already in progress, this option
+changes its rate. To change a rate-limited TRIM to an unlimited one,
+simply execute the \fBzpool trim\fR command without a \fB-r\fR option.
+.RE
+.sp
+.ne 2
+.na
+\fB\fB-s\fR\fR
+.ad
+.RS 6n
+Stop trimming. If an on-demand TRIM operation is not ongoing at the
+moment, this does nothing and the command returns success.
+.RE
+
+.RE
 
 .sp
 .ne 2
diff -Nuar zfs-kmod-9999.orig/module/zcommon/zpool_prop.c zfs-kmod-9999/module/zcommon/zpool_prop.c
--- zfs-kmod-9999.orig/module/zcommon/zpool_prop.c	2017-05-06 11:03:37.128219120 +0200
+++ zfs-kmod-9999/module/zcommon/zpool_prop.c	2017-05-06 11:04:34.186030845 +0200
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2007, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  * Copyright (c) 2012, 2014 by Delphix. All rights reserved.
  */
 
@@ -125,6 +125,12 @@
 	zprop_register_index(ZPOOL_PROP_FAILUREMODE, "failmode",
 	    ZIO_FAILURE_MODE_WAIT, PROP_DEFAULT, ZFS_TYPE_POOL,
 	    "wait | continue | panic", "FAILMODE", failuremode_table);
+	zprop_register_index(ZPOOL_PROP_FORCETRIM, "forcetrim",
+	    SPA_FORCE_TRIM_OFF, PROP_DEFAULT, ZFS_TYPE_POOL,
+	    "on | off", "FORCETRIM", boolean_table);
+	zprop_register_index(ZPOOL_PROP_AUTOTRIM, "autotrim",
+	    SPA_AUTO_TRIM_OFF, PROP_DEFAULT, ZFS_TYPE_POOL,
+	    "on | off", "AUTOTRIM", boolean_table);
 
 	/* hidden properties */
 	zprop_register_hidden(ZPOOL_PROP_NAME, "name", PROP_TYPE_STRING,
diff -Nuar zfs-kmod-9999.orig/module/zfs/dsl_scan.c zfs-kmod-9999/module/zfs/dsl_scan.c
--- zfs-kmod-9999.orig/module/zfs/dsl_scan.c	2017-05-06 11:03:37.144219067 +0200
+++ zfs-kmod-9999/module/zfs/dsl_scan.c	2017-05-06 11:04:34.186030845 +0200
@@ -22,6 +22,7 @@
  * Copyright (c) 2008, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2016 by Delphix. All rights reserved.
  * Copyright 2016 Gary Mills
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/dsl_scan.h>
@@ -1743,6 +1744,9 @@
 void
 dsl_resilver_restart(dsl_pool_t *dp, uint64_t txg)
 {
+	/* Stop any ongoing TRIMs */
+	spa_man_trim_stop(dp->dp_spa);
+
 	if (txg == 0) {
 		dmu_tx_t *tx;
 		tx = dmu_tx_create_dd(dp->dp_mos_dir);
diff -Nuar zfs-kmod-9999.orig/module/zfs/dsl_synctask.c zfs-kmod-9999/module/zfs/dsl_synctask.c
--- zfs-kmod-9999.orig/module/zfs/dsl_synctask.c	2017-05-06 11:03:37.145219064 +0200
+++ zfs-kmod-9999/module/zfs/dsl_synctask.c	2017-05-06 11:04:34.186030845 +0200
@@ -112,7 +112,6 @@
 		txg_wait_synced(dp, dst.dst_txg + TXG_DEFER_SIZE);
 		goto top;
 	}
-
 	spa_close(spa, FTAG);
 	return (dst.dst_error);
 }
diff -Nuar zfs-kmod-9999.orig/module/zfs/metaslab.c zfs-kmod-9999/module/zfs/metaslab.c
--- zfs-kmod-9999.orig/module/zfs/metaslab.c	2017-05-06 11:03:37.148219054 +0200
+++ zfs-kmod-9999/module/zfs/metaslab.c	2017-05-06 11:04:34.205030782 +0200
@@ -22,6 +22,7 @@
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2016 by Delphix. All rights reserved.
  * Copyright (c) 2013 by Saso Kiselkov. All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -33,6 +34,7 @@
 #include <sys/zio.h>
 #include <sys/spa_impl.h>
 #include <sys/zfeature.h>
+#include <sys/trace_vdev.h>
 
 #define	WITH_DF_BLOCK_ALLOCATOR
 
@@ -208,6 +210,51 @@
 #endif
 
 /*
+ * How many TXG's worth of updates should be aggregated per TRIM/UNMAP
+ * issued to the underlying vdev. We keep two range trees of extents
+ * (called "trim sets") to be trimmed per metaslab, the `current' and
+ * the `previous' TS. New free's are added to the current TS. Then,
+ * once `zfs_txgs_per_trim' transactions have elapsed, the `current'
+ * TS becomes the `previous' TS and a new, blank TS is created to be
+ * the new `current', which will then start accumulating any new frees.
+ * Once another zfs_txgs_per_trim TXGs have passed, the previous TS's
+ * extents are trimmed, the TS is destroyed and the current TS again
+ * becomes the previous TS.
+ * This serves to fulfill two functions: aggregate many small frees
+ * into fewer larger trim operations (which should help with devices
+ * which do not take so kindly to them) and to allow for disaster
+ * recovery (extents won't get trimmed immediately, but instead only
+ * after passing this rather long timeout, thus preserving
+ * 'zfs import -F' functionality).
+ * The exact default value of this tunable is a tradeoff between:
+ * 1) Keeping the trim commands reasonably small.
+ * 2) Keeping the ability to rollback back for as many txgs as possible.
+ * 3) Waiting around too long that the user starts to get uneasy about not
+ *	seeing any space being freed after they remove some files.
+ * The default value of 32 is the maximum number of uberblocks in a vdev
+ * label, assuming a 4k physical sector size (which seems to be the almost
+ * universal smallest sector size used in SSDs).
+ */
+unsigned int zfs_txgs_per_trim = 32;
+/*
+ * Maximum number of bytes we'll put into a single zio_trim. This is for
+ * vdev queue processing purposes and also because some devices advertise
+ * they can handle a lot more LBAs per command than they can handle
+ * efficiently.
+ */
+uint64_t zfs_max_bytes_per_trim = 128 << 20;
+
+static void metaslab_trim_remove(void *arg, uint64_t offset, uint64_t size);
+static void metaslab_trim_add(void *arg, uint64_t offset, uint64_t size);
+
+static zio_t *metaslab_exec_trim(metaslab_t *msp, boolean_t auto_trim);
+
+static metaslab_trimset_t *metaslab_new_trimset(uint64_t txg, kmutex_t *lock);
+static void metaslab_free_trimset(metaslab_trimset_t *ts);
+static boolean_t metaslab_check_trim_conflict(metaslab_t *msp,
+    uint64_t *offset, uint64_t size, uint64_t align, uint64_t limit);
+
+/*
  * ==========================================================================
  * Metaslab classes
  * ==========================================================================
@@ -1103,19 +1150,20 @@
  * tree looking for a block that matches the specified criteria.
  */
 static uint64_t
-metaslab_block_picker(avl_tree_t *t, uint64_t *cursor, uint64_t size,
-    uint64_t align)
+metaslab_block_picker(metaslab_t *msp, avl_tree_t *t, uint64_t *cursor,
+    uint64_t size, uint64_t align)
 {
 	range_seg_t *rs = metaslab_block_find(t, *cursor, size);
 
-	while (rs != NULL) {
+	for (; rs != NULL; rs = AVL_NEXT(t, rs)) {
 		uint64_t offset = P2ROUNDUP(rs->rs_start, align);
 
-		if (offset + size <= rs->rs_end) {
+		if (offset + size <= rs->rs_end &&
+		    !metaslab_check_trim_conflict(msp, &offset, size, align,
+		    rs->rs_end)) {
 			*cursor = offset + size;
 			return (offset);
 		}
-		rs = AVL_NEXT(t, rs);
 	}
 
 	/*
@@ -1126,7 +1174,7 @@
 		return (-1ULL);
 
 	*cursor = 0;
-	return (metaslab_block_picker(t, cursor, size, align));
+	return (metaslab_block_picker(msp, t, cursor, size, align));
 }
 #endif /* WITH_FF/DF/CF_BLOCK_ALLOCATOR */
 
@@ -1150,7 +1198,7 @@
 	uint64_t *cursor = &msp->ms_lbas[highbit64(align) - 1];
 	avl_tree_t *t = &msp->ms_tree->rt_root;
 
-	return (metaslab_block_picker(t, cursor, size, align));
+	return (metaslab_block_picker(msp, t, cursor, size, align));
 }
 
 static metaslab_ops_t metaslab_ff_ops = {
@@ -1202,7 +1250,7 @@
 		*cursor = 0;
 	}
 
-	return (metaslab_block_picker(t, cursor, size, 1ULL));
+	return (metaslab_block_picker(msp, t, cursor, size, 1ULL));
 }
 
 static metaslab_ops_t metaslab_df_ops = {
@@ -1238,13 +1286,19 @@
 
 	if ((*cursor + size) > *cursor_end) {
 		range_seg_t *rs;
-
-		rs = avl_last(&msp->ms_size_tree);
-		if (rs == NULL || (rs->rs_end - rs->rs_start) < size)
+		for (rs = avl_last(&msp->ms_size_tree);
+		    rs != NULL && rs->rs_end - rs->rs_start >= size;
+		    rs = AVL_PREV(&msp->ms_size_tree, rs)) {
+			*cursor = rs->rs_start;
+			*cursor_end = rs->rs_end;
+			if (!metaslab_check_trim_conflict(msp, cursor, size,
+			    1, *cursor_end)) {
+				/* segment appears to be acceptable */
+				break;
+			}
+		}
+		if (rs == NULL || rs->rs_end - rs->rs_start < size)
 			return (-1ULL);
-
-		*cursor = rs->rs_start;
-		*cursor_end = rs->rs_end;
 	}
 
 	offset = *cursor;
@@ -1285,6 +1339,8 @@
 	uint64_t hbit = highbit64(size);
 	uint64_t *cursor = &msp->ms_lbas[hbit - 1];
 	uint64_t max_size = metaslab_block_maxsize(msp);
+	/* mutable copy for adjustment by metaslab_check_trim_conflict */
+	uint64_t adjustable_start;
 
 	ASSERT(MUTEX_HELD(&msp->ms_lock));
 	ASSERT3U(avl_numnodes(t), ==, avl_numnodes(&msp->ms_size_tree));
@@ -1296,7 +1352,12 @@
 	rsearch.rs_end = *cursor + size;
 
 	rs = avl_find(t, &rsearch, &where);
-	if (rs == NULL || (rs->rs_end - rs->rs_start) < size) {
+	if (rs != NULL)
+		adjustable_start = rs->rs_start;
+	if (rs == NULL || rs->rs_end - adjustable_start < size ||
+	    metaslab_check_trim_conflict(msp, &adjustable_start, size, 1,
+	    rs->rs_end)) {
+		/* segment not usable, try the largest remaining one */
 		t = &msp->ms_size_tree;
 
 		rsearch.rs_start = 0;
@@ -1306,13 +1367,17 @@
 		if (rs == NULL)
 			rs = avl_nearest(t, where, AVL_AFTER);
 		ASSERT(rs != NULL);
+		adjustable_start = rs->rs_start;
+		if (rs->rs_end - adjustable_start < size ||
+		    metaslab_check_trim_conflict(msp, &adjustable_start,
+		    size, 1, rs->rs_end)) {
+			/* even largest remaining segment not usable */
+			return (-1ULL);
+		}
 	}
 
-	if ((rs->rs_end - rs->rs_start) >= size) {
-		*cursor = rs->rs_start + size;
-		return (rs->rs_start);
-	}
-	return (-1ULL);
+	*cursor = adjustable_start + size;
+	return (*cursor);
 }
 
 static metaslab_ops_t metaslab_ndf_ops = {
@@ -1376,6 +1441,8 @@
 		for (t = 0; t < TXG_DEFER_SIZE; t++) {
 			range_tree_walk(msp->ms_defertree[t],
 			    range_tree_remove, msp->ms_tree);
+			range_tree_walk(msp->ms_defertree[t],
+			    metaslab_trim_remove, msp);
 		}
 		msp->ms_max_size = metaslab_block_maxsize(msp);
 	}
@@ -1405,6 +1472,7 @@
 	ms = kmem_zalloc(sizeof (metaslab_t), KM_SLEEP);
 	mutex_init(&ms->ms_lock, NULL, MUTEX_DEFAULT, NULL);
 	cv_init(&ms->ms_load_cv, NULL, CV_DEFAULT, NULL);
+	cv_init(&ms->ms_trim_cv, NULL, CV_DEFAULT, NULL);
 	ms->ms_id = id;
 	ms->ms_start = id << vd->vdev_ms_shift;
 	ms->ms_size = 1ULL << vd->vdev_ms_shift;
@@ -1425,6 +1493,8 @@
 		ASSERT(ms->ms_sm != NULL);
 	}
 
+	ms->ms_cur_ts = metaslab_new_trimset(0, &ms->ms_lock);
+
 	/*
 	 * We create the main range tree here, but we don't create the
 	 * other range trees until metaslab_sync_done().  This serves
@@ -1477,6 +1547,12 @@
 
 	metaslab_group_t *mg = msp->ms_group;
 
+	/* Wait for trimming to finish */
+	mutex_enter(&msp->ms_lock);
+	while (msp->ms_trimming_ts != NULL)
+		cv_wait(&msp->ms_trim_cv, &msp->ms_lock);
+	mutex_exit(&msp->ms_lock);
+
 	metaslab_group_remove(mg, msp);
 
 	mutex_enter(&msp->ms_lock);
@@ -1498,10 +1574,16 @@
 		range_tree_destroy(msp->ms_defertree[t]);
 	}
 
+	metaslab_free_trimset(msp->ms_cur_ts);
+	if (msp->ms_prev_ts)
+		metaslab_free_trimset(msp->ms_prev_ts);
+	ASSERT3P(msp->ms_trimming_ts, ==, NULL);
+
 	ASSERT0(msp->ms_deferspace);
 
 	mutex_exit(&msp->ms_lock);
 	cv_destroy(&msp->ms_load_cv);
+	cv_destroy(&msp->ms_trim_cv);
 	mutex_destroy(&msp->ms_lock);
 
 	kmem_free(msp, sizeof (metaslab_t));
@@ -2434,6 +2516,14 @@
 	 * defer_tree -- this is safe to do because we've just emptied out
 	 * the defer_tree.
 	 */
+	if (spa_get_auto_trim(spa) == SPA_AUTO_TRIM_ON &&
+	    !vd->vdev_man_trimming) {
+		range_tree_walk(*defer_tree, metaslab_trim_add, msp);
+		if (!defer_allowed) {
+			range_tree_walk(msp->ms_freedtree, metaslab_trim_add,
+			    msp);
+		}
+	}
 	range_tree_vacate(*defer_tree,
 	    msp->ms_loaded ? range_tree_add : NULL, msp->ms_tree);
 	if (defer_allowed) {
@@ -2716,6 +2806,7 @@
 		VERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));
 		VERIFY3U(range_tree_space(rt) - size, <=, msp->ms_size);
 		range_tree_remove(rt, start, size);
+		metaslab_trim_remove(msp, start, size);
 
 		if (range_tree_space(msp->ms_alloctree[txg & TXG_MASK]) == 0)
 			vdev_dirty(mg->mg_vd, VDD_METASLAB, msp, txg);
@@ -2922,7 +3013,8 @@
 		 * we may end up in an infinite loop retrying the same
 		 * metaslab.
 		 */
-		ASSERT(!metaslab_should_allocate(msp, asize));
+		ASSERT(!metaslab_should_allocate(msp, asize) ||
+		    msp->ms_trimming_ts != NULL);
 		mutex_exit(&msp->ms_lock);
 	}
 	mutex_exit(&msp->ms_lock);
@@ -3263,6 +3355,9 @@
 		VERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));
 		range_tree_add(msp->ms_tree, offset, size);
 		msp->ms_max_size = metaslab_block_maxsize(msp);
+		if (spa_get_auto_trim(spa) == SPA_AUTO_TRIM_ON &&
+		    !vd->vdev_man_trimming)
+			metaslab_trim_add(msp, offset, size);
 	} else {
 		VERIFY3U(txg, ==, spa->spa_syncing_txg);
 		if (range_tree_space(msp->ms_freeingtree) == 0)
@@ -3318,6 +3413,7 @@
 	VERIFY0(P2PHASE(size, 1ULL << vd->vdev_ashift));
 	VERIFY3U(range_tree_space(msp->ms_tree) - size, <=, msp->ms_size);
 	range_tree_remove(msp->ms_tree, offset, size);
+	metaslab_trim_remove(msp, offset, size);
 
 	if (spa_writeable(spa)) {	/* don't dirty if we're zdb(1M) */
 		if (range_tree_space(msp->ms_alloctree[txg & TXG_MASK]) == 0)
@@ -3552,17 +3648,480 @@
 		uint64_t size = DVA_GET_ASIZE(&bp->blk_dva[i]);
 		metaslab_t *msp = vd->vdev_ms[offset >> vd->vdev_ms_shift];
 
-		if (msp->ms_loaded)
+		mutex_enter(&msp->ms_lock);
+		if (msp->ms_loaded) {
+			VERIFY(&msp->ms_lock == msp->ms_tree->rt_lock);
 			range_tree_verify(msp->ms_tree, offset, size);
+#ifdef	DEBUG
+			VERIFY(&msp->ms_lock ==
+			    msp->ms_cur_ts->ts_tree->rt_lock);
+			range_tree_verify(msp->ms_cur_ts->ts_tree,
+			    offset, size);
+			if (msp->ms_prev_ts != NULL) {
+				VERIFY(&msp->ms_lock ==
+				    msp->ms_prev_ts->ts_tree->rt_lock);
+				range_tree_verify(msp->ms_prev_ts->ts_tree,
+				    offset, size);
+			}
+#endif
+		}
 
 		range_tree_verify(msp->ms_freeingtree, offset, size);
 		range_tree_verify(msp->ms_freedtree, offset, size);
 		for (j = 0; j < TXG_DEFER_SIZE; j++)
 			range_tree_verify(msp->ms_defertree[j], offset, size);
+		mutex_exit(&msp->ms_lock);
 	}
 	spa_config_exit(spa, SCL_VDEV, FTAG);
 }
 
+/*
+ * Trims all free space in the metaslab. Returns the root TRIM zio (that the
+ * caller should zio_wait() for) and the amount of space in the metaslab that
+ * has been scheduled for trimming in the `delta' return argument.
+ */
+zio_t *
+metaslab_trim_all(metaslab_t *msp, uint64_t *cursor, uint64_t *delta,
+    boolean_t *was_loaded)
+{
+	uint64_t cur = *cursor, trimmed_space = 0;
+	zio_t *trim_io = NULL;
+	range_seg_t rsearch, *rs;
+	avl_index_t where;
+	const uint64_t max_bytes = zfs_max_bytes_per_trim;
+
+	ASSERT(!MUTEX_HELD(&msp->ms_group->mg_lock));
+	ASSERT3U(cur, >=, msp->ms_start);
+	ASSERT3U(cur, <=, msp->ms_start + msp->ms_size);
+
+	mutex_enter(&msp->ms_lock);
+
+	while (msp->ms_condensing)
+		cv_wait(&msp->ms_condensing_cv, &msp->ms_lock);
+
+	while (msp->ms_loading)
+		metaslab_load_wait(msp);
+	/*
+	 * On the initial call we memorize if we had to load the metaslab
+	 * for ourselves, so we can unload it when we're done.
+	 */
+	if (cur == msp->ms_start)
+		*was_loaded = msp->ms_loaded;
+	if (!msp->ms_loaded) {
+		if (metaslab_load(msp) != 0) {
+			/* Load failed, stop trimming this metaslab */
+			*cursor = msp->ms_start + msp->ms_size;
+			mutex_exit(&msp->ms_lock);
+			return (NULL);
+		}
+	}
+
+	/*
+	 * Flush out any scheduled extents and add everything in ms_tree
+	 * from the last cursor position, but not more than the trim run
+	 * limit.
+	 */
+	range_tree_vacate(msp->ms_cur_ts->ts_tree, NULL, NULL);
+
+	rsearch.rs_start = cur;
+	rsearch.rs_end = cur + SPA_MINBLOCKSIZE;
+	rs = avl_find(&msp->ms_tree->rt_root, &rsearch, &where);
+	if (rs == NULL) {
+		rs = avl_nearest(&msp->ms_tree->rt_root, where, AVL_AFTER);
+		if (rs != NULL)
+			cur = rs->rs_start;
+	}
+
+	/* Clear out ms_prev_ts, since we'll be trimming everything. */
+	if (msp->ms_prev_ts != NULL) {
+		metaslab_free_trimset(msp->ms_prev_ts);
+		msp->ms_prev_ts = NULL;
+	}
+
+	while (rs != NULL && trimmed_space < max_bytes) {
+		uint64_t end;
+		if (cur < rs->rs_start)
+			cur = rs->rs_start;
+		end = MIN(cur + (max_bytes - trimmed_space), rs->rs_end);
+		metaslab_trim_add(msp, cur, end - cur);
+		trimmed_space += (end - cur);
+		cur = end;
+		if (cur == rs->rs_end)
+			rs = AVL_NEXT(&msp->ms_tree->rt_root, rs);
+	}
+
+	if (trimmed_space != 0) {
+		/* Force this trim to take place ASAP. */
+		msp->ms_prev_ts = msp->ms_cur_ts;
+		msp->ms_cur_ts = metaslab_new_trimset(0, &msp->ms_lock);
+		trim_io = metaslab_exec_trim(msp, B_FALSE);
+		ASSERT(trim_io != NULL);
+
+		/*
+		 * Not at the end of this metaslab yet, have vdev_man_trim
+		 * come back around for another run.
+		 */
+		*cursor = cur;
+	} else {
+		*cursor = msp->ms_start + msp->ms_size;
+		if (!(*was_loaded) && !vdev_is_dirty(msp->ms_group->mg_vd,
+		    VDD_METASLAB, msp) && msp->ms_activation_weight == 0)
+			metaslab_unload(msp);
+	}
+
+	mutex_exit(&msp->ms_lock);
+	*delta = trimmed_space;
+
+	return (trim_io);
+}
+
+/*
+ * Notifies the trimsets in a metaslab that an extent has been allocated.
+ * This removes the segment from the queues of extents awaiting to be trimmed.
+ */
+static void
+metaslab_trim_remove(void *arg, uint64_t offset, uint64_t size)
+{
+	metaslab_t *msp = arg;
+
+	range_tree_clear(msp->ms_cur_ts->ts_tree, offset, size);
+	if (msp->ms_prev_ts != NULL)
+		range_tree_clear(msp->ms_prev_ts->ts_tree, offset, size);
+	ASSERT(msp->ms_trimming_ts == NULL ||
+	    !range_tree_contains(msp->ms_trimming_ts->ts_tree, offset, size));
+}
+
+/*
+ * Notifies the trimsets in a metaslab that an extent has been freed.
+ * This adds the segment to the currently open queue of extents awaiting
+ * to be trimmed.
+ */
+static void
+metaslab_trim_add(void *arg, uint64_t offset, uint64_t size)
+{
+	metaslab_t *msp = arg;
+	ASSERT(msp->ms_cur_ts != NULL);
+	range_tree_add(msp->ms_cur_ts->ts_tree, offset, size);
+	if (msp->ms_prev_ts != NULL) {
+		ASSERT(!range_tree_contains_part(msp->ms_prev_ts->ts_tree,
+		    offset, size));
+	}
+}
+
+/*
+ * Does a metaslab's automatic trim operation processing. This function
+ * issues trims in intervals as dictated by the zfs_txgs_per_trim tunable.
+ * If the previous trimset has not yet finished trimming, this function
+ * decides what to do based on `preserve_spilled'. If preserve_spilled is
+ * false, the next trimset which would have been issued is simply dropped to
+ * limit memory usage. Otherwise it is preserved by adding it to the cur_ts
+ * trimset.
+ */
+void
+metaslab_auto_trim(metaslab_t *msp, uint64_t txg, boolean_t preserve_spilled)
+{
+	/* for atomicity */
+	uint64_t txgs_per_trim = zfs_txgs_per_trim;
+
+	ASSERT(!MUTEX_HELD(&msp->ms_lock));
+	mutex_enter(&msp->ms_lock);
+
+	/*
+	 * Since we typically have hundreds of metaslabs per vdev, but we only
+	 * trim them once every zfs_txgs_per_trim txgs, it'd be best if we
+	 * could sequence the TRIM commands from all metaslabs so that they
+	 * don't all always pound the device in the same txg. We do so by
+	 * artificially inflating the birth txg of the first trim set by a
+	 * sequence number derived from the metaslab's starting offset
+	 * (modulo zfs_txgs_per_trim). Thus, for the default 200 metaslabs and
+	 * 32 txgs per trim, we'll only be trimming ~6.25 metaslabs per txg.
+	 *
+	 * If we detect that the txg has advanced too far ahead of ts_birth,
+	 * it means our birth txg is out of lockstep. Recompute it by
+	 * rounding down to the nearest zfs_txgs_per_trim multiple and adding
+	 * our metaslab id modulo zfs_txgs_per_trim.
+	 */
+	if (txg > msp->ms_cur_ts->ts_birth + txgs_per_trim) {
+		msp->ms_cur_ts->ts_birth = (txg / txgs_per_trim) *
+		    txgs_per_trim + (msp->ms_id % txgs_per_trim);
+	}
+
+	/* Time to swap out the current and previous trimsets */
+	if (txg == msp->ms_cur_ts->ts_birth + txgs_per_trim) {
+		if (msp->ms_prev_ts != NULL) {
+			if (msp->ms_trimming_ts != NULL) {
+				spa_t *spa = msp->ms_group->mg_class->mc_spa;
+				/*
+				 * The previous trim run is still ongoing, so
+				 * the device is reacting slowly to our trim
+				 * requests. Drop this trimset, so as not to
+				 * back the device up with trim requests.
+				 */
+				if (preserve_spilled) {
+					DTRACE_PROBE1(preserve__spilled,
+					    metaslab_t *, msp);
+					range_tree_vacate(
+					    msp->ms_prev_ts->ts_tree,
+					    range_tree_add,
+					    msp->ms_cur_ts->ts_tree);
+				} else {
+					DTRACE_PROBE1(drop__spilled,
+					    metaslab_t *, msp);
+					spa_trimstats_auto_slow_incr(spa);
+				}
+				metaslab_free_trimset(msp->ms_prev_ts);
+			} else if (msp->ms_group->mg_vd->vdev_man_trimming) {
+				/*
+				 * If a manual trim is ongoing, we want to
+				 * inhibit autotrim temporarily so it doesn't
+				 * slow down the manual trim.
+				 */
+				metaslab_free_trimset(msp->ms_prev_ts);
+			} else {
+				/*
+				 * Trim out aged extents on the vdevs - these
+				 * are safe to be destroyed now. We'll keep
+				 * the trimset around to deny allocations from
+				 * these regions while the trims are ongoing.
+				 */
+				zio_nowait(metaslab_exec_trim(msp, B_TRUE));
+			}
+		}
+		msp->ms_prev_ts = msp->ms_cur_ts;
+		msp->ms_cur_ts = metaslab_new_trimset(txg, &msp->ms_lock);
+	}
+	mutex_exit(&msp->ms_lock);
+}
+
+/*
+ * Computes the amount of memory a trimset is expected to use if issued out
+ * to be trimmed. The calculation isn't 100% accurate, because we don't
+ * know how the trimset's extents might subdivide into smaller extents
+ * (dkioc_free_list_ext_t) that actually get passed to the zio, but luckily
+ * the extent structure is fairly small compared to the size of a zio_t, so
+ * it's less important that we get that absolutely correct. We just want to
+ * get it "close enough".
+ */
+static uint64_t
+metaslab_trimset_mem_used(metaslab_trimset_t *ts)
+{
+	uint64_t result = 0;
+
+	result += avl_numnodes(&ts->ts_tree->rt_root) * (sizeof (range_seg_t) +
+	    sizeof (dkioc_free_list_ext_t));
+	result += ((range_tree_space(ts->ts_tree) / zfs_max_bytes_per_trim) +
+	    1) * sizeof (zio_t);
+	result += sizeof (range_tree_t) + sizeof (metaslab_trimset_t);
+
+	return (result);
+}
+
+/*
+ * Computes the amount of memory used by the trimsets and queued trim zios of
+ * a metaslab.
+ */
+uint64_t
+metaslab_trim_mem_used(metaslab_t *msp)
+{
+	uint64_t result = 0;
+
+	ASSERT(!MUTEX_HELD(&msp->ms_lock));
+	mutex_enter(&msp->ms_lock);
+	result += metaslab_trimset_mem_used(msp->ms_cur_ts);
+	if (msp->ms_prev_ts != NULL)
+		result += metaslab_trimset_mem_used(msp->ms_prev_ts);
+	mutex_exit(&msp->ms_lock);
+
+	return (result);
+}
+
+static void
+metaslab_trim_done(zio_t *zio)
+{
+	metaslab_t *msp = zio->io_private;
+	boolean_t held;
+
+	ASSERT(msp != NULL);
+	ASSERT(msp->ms_trimming_ts != NULL);
+	held = MUTEX_HELD(&msp->ms_lock);
+	if (!held)
+		mutex_enter(&msp->ms_lock);
+	metaslab_free_trimset(msp->ms_trimming_ts);
+	msp->ms_trimming_ts = NULL;
+	cv_broadcast(&msp->ms_trim_cv);
+	if (!held)
+		mutex_exit(&msp->ms_lock);
+}
+
+/*
+ * Executes a zio_trim on a range tree holding freed extents in the metaslab.
+ * The set of extents is taken from the metaslab's ms_prev_ts. If there is
+ * another trim currently executing on that metaslab, this function blocks
+ * until that trim completes.
+ * The `auto_trim' argument signals whether the trim is being invoked on
+ * behalf of auto or manual trim. The differences are:
+ * 1) For auto trim the trimset is split up into subtrees, each containing no
+ *	more than zfs_max_bytes_per_trim total bytes. Each subtree is then
+ *	trimmed in one zio. This is done to limit the number of LBAs per
+ *	trim command, as many devices perform suboptimally with large trim
+ *	commands, even if they indicate support for them. Manual trim already
+ *	applies this limit earlier by limiting the trimset size, so the
+ *	whole trimset can be issued in a single zio.
+ * 2) The zio(s) generated are tagged with either ZIO_PRIORITY_AUTO_TRIM or
+ *	ZIO_PRIORITY_MAN_TRIM to allow differentiating them further down
+ *	the pipeline (see zio_priority_t in sys/zio_priority.h).
+ * The function always returns a zio that the caller should zio_(no)wait.
+ */
+static zio_t *
+metaslab_exec_trim(metaslab_t *msp, boolean_t auto_trim)
+{
+	metaslab_group_t *mg = msp->ms_group;
+	spa_t *spa = mg->mg_class->mc_spa;
+	vdev_t *vd = mg->mg_vd;
+	range_tree_t *trim_tree;
+	const uint64_t max_bytes = zfs_max_bytes_per_trim;
+	const enum zio_flag trim_flags = ZIO_FLAG_CANFAIL |
+	    ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_RETRY |
+	    ZIO_FLAG_CONFIG_WRITER;
+
+	ASSERT(MUTEX_HELD(&msp->ms_lock));
+
+	/* wait for a preceding trim to finish */
+	while (msp->ms_trimming_ts != NULL)
+		cv_wait(&msp->ms_trim_cv, &msp->ms_lock);
+	msp->ms_trimming_ts = msp->ms_prev_ts;
+	msp->ms_prev_ts = NULL;
+	trim_tree = msp->ms_trimming_ts->ts_tree;
+#ifdef	DEBUG
+	if (msp->ms_loaded) {
+		for (range_seg_t *rs = avl_first(&trim_tree->rt_root);
+		    rs != NULL; rs = AVL_NEXT(&trim_tree->rt_root, rs)) {
+			if (!range_tree_contains_part(msp->ms_tree,
+			    rs->rs_start, rs->rs_end - rs->rs_start)) {
+				panic("trimming allocated region; rs=%p",
+				    (void*)rs);
+			}
+		}
+	}
+#endif
+
+	/* Nothing to trim */
+	if (range_tree_space(trim_tree) == 0) {
+		metaslab_free_trimset(msp->ms_trimming_ts);
+		msp->ms_trimming_ts = 0;
+		return (zio_null(NULL, spa, NULL, NULL, NULL, 0));
+	}
+
+	if (auto_trim) {
+		uint64_t start = 0;
+		range_seg_t *rs;
+		range_tree_t *sub_trim_tree = range_tree_create(NULL, NULL,
+		    &msp->ms_lock);
+		zio_t *pio = zio_null(NULL, spa, vd, metaslab_trim_done, msp,
+		    0);
+
+		rs = avl_first(&trim_tree->rt_root);
+		if (rs != NULL)
+			start = rs->rs_start;
+		while (rs != NULL) {
+			uint64_t end = MIN(rs->rs_end, start + (max_bytes -
+			    range_tree_space(sub_trim_tree)));
+
+			ASSERT3U(start, <=, end);
+			if (start == end) {
+				rs = AVL_NEXT(&trim_tree->rt_root, rs);
+				if (rs != NULL)
+					start = rs->rs_start;
+				continue;
+			}
+			range_tree_add(sub_trim_tree, start, end - start);
+			ASSERT3U(range_tree_space(sub_trim_tree), <=,
+			    max_bytes);
+			if (range_tree_space(sub_trim_tree) == max_bytes) {
+				zio_nowait(zio_trim_tree(pio, spa, vd,
+				    sub_trim_tree, auto_trim, NULL, NULL,
+				    trim_flags, msp));
+				range_tree_vacate(sub_trim_tree, NULL, NULL);
+			}
+			start = end;
+		}
+		if (range_tree_space(sub_trim_tree) != 0) {
+			zio_nowait(zio_trim_tree(pio, spa, vd, sub_trim_tree,
+			    auto_trim, NULL, NULL, trim_flags, msp));
+			range_tree_vacate(sub_trim_tree, NULL, NULL);
+		}
+		range_tree_destroy(sub_trim_tree);
+
+		return (pio);
+	} else {
+		return (zio_trim_tree(NULL, spa, vd, trim_tree, auto_trim,
+		    metaslab_trim_done, msp, trim_flags, msp));
+	}
+}
+
+/*
+ * Allocates and initializes a new trimset structure. The `txg' argument
+ * indicates when this trimset was born and `lock' indicates the lock to
+ * link to the range tree.
+ */
+static metaslab_trimset_t *
+metaslab_new_trimset(uint64_t txg, kmutex_t *lock)
+{
+	metaslab_trimset_t *ts;
+
+	ts = kmem_zalloc(sizeof (*ts), KM_SLEEP);
+	ts->ts_birth = txg;
+	ts->ts_tree = range_tree_create(NULL, NULL, lock);
+
+	return (ts);
+}
+
+/*
+ * Destroys and frees a trim set previously allocated by metaslab_new_trimset.
+ */
+static void
+metaslab_free_trimset(metaslab_trimset_t *ts)
+{
+	range_tree_vacate(ts->ts_tree, NULL, NULL);
+	range_tree_destroy(ts->ts_tree);
+	kmem_free(ts, sizeof (*ts));
+}
+
+/*
+ * Checks whether an allocation conflicts with an ongoing trim operation in
+ * the given metaslab. This function takes a segment starting at `*offset'
+ * of `size' and checks whether it hits any region in the metaslab currently
+ * being trimmed. If yes, it tries to adjust the allocation to the end of
+ * the region being trimmed (P2ROUNDUP aligned by `align'), but only up to
+ * `limit' (no part of the allocation is allowed to go past this point).
+ *
+ * Returns B_FALSE if either the original allocation wasn't in conflict, or
+ * the conflict could be resolved by adjusting the value stored in `offset'
+ * such that the whole allocation still fits below `limit'. Returns B_TRUE
+ * if the allocation conflict couldn't be resolved.
+ */
+static boolean_t metaslab_check_trim_conflict(metaslab_t *msp,
+    uint64_t *offset, uint64_t size, uint64_t align, uint64_t limit)
+{
+	uint64_t new_offset;
+
+	ASSERT3U(*offset + size, <=, limit);
+
+	if (msp->ms_trimming_ts == NULL)
+		/* no trim conflict, original offset is OK */
+		return (B_FALSE);
+
+	new_offset = P2ROUNDUP(range_tree_find_gap(msp->ms_trimming_ts->ts_tree,
+	    *offset, size), align);
+	if (new_offset + size > limit)
+		/* trim conflict and adjustment not possible */
+		return (B_TRUE);
+
+	/* trim conflict, but adjusted offset still within limit */
+	*offset = new_offset;
+	return (B_FALSE);
+}
+
 #if defined(_KERNEL) && defined(HAVE_SPL)
 /* CSTYLED */
 module_param(metaslab_aliquot, ulong, 0644);
@@ -3612,4 +4171,9 @@
 module_param(zfs_metaslab_switch_threshold, int, 0644);
 MODULE_PARM_DESC(zfs_metaslab_switch_threshold,
 	"segment-based metaslab selection maximum buckets before switching");
+
+module_param(zfs_txgs_per_trim, int, 0644);
+MODULE_PARM_DESC(zfs_txgs_per_trim,
+	"txgs per trim");
+
 #endif /* _KERNEL && HAVE_SPL */
diff -Nuar zfs-kmod-9999.orig/module/zfs/range_tree.c zfs-kmod-9999/module/zfs/range_tree.c
--- zfs-kmod-9999.orig/module/zfs/range_tree.c	2017-05-06 11:03:37.150219048 +0200
+++ zfs-kmod-9999/module/zfs/range_tree.c	2017-05-06 11:04:34.187030842 +0200
@@ -24,6 +24,7 @@
  */
 /*
  * Copyright (c) 2013, 2014 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -317,16 +318,29 @@
 	return (NULL);
 }
 
+/*
+ * Given an extent start offset and size, will look through the provided
+ * range tree and find a suitable start offset (starting at `start') such
+ * that the requested extent _doesn't_ overlap with any range segment in
+ * the range tree.
+ */
+uint64_t
+range_tree_find_gap(range_tree_t *rt, uint64_t start, uint64_t size)
+{
+	range_seg_t *rs;
+	while ((rs = range_tree_find_impl(rt, start, size)) != NULL)
+		start = rs->rs_end;
+	return (start);
+}
+
 void
 range_tree_verify(range_tree_t *rt, uint64_t off, uint64_t size)
 {
 	range_seg_t *rs;
 
-	mutex_enter(rt->rt_lock);
 	rs = range_tree_find(rt, off, size);
 	if (rs != NULL)
 		panic("freeing free block; rs=%p", (void *)rs);
-	mutex_exit(rt->rt_lock);
 }
 
 boolean_t
@@ -336,6 +350,15 @@
 }
 
 /*
+ * Same as range_tree_contains, but locates even just a partial overlap.
+ */
+boolean_t
+range_tree_contains_part(range_tree_t *rt, uint64_t start, uint64_t size)
+{
+	return (range_tree_find_impl(rt, start, size) != NULL);
+}
+
+/*
  * Ensure that this range is not in the tree, regardless of whether
  * it is currently in the tree.
  */
diff -Nuar zfs-kmod-9999.orig/module/zfs/spa.c zfs-kmod-9999/module/zfs/spa.c
--- zfs-kmod-9999.orig/module/zfs/spa.c	2017-05-06 11:03:37.155219031 +0200
+++ zfs-kmod-9999/module/zfs/spa.c	2017-05-06 11:04:34.200030799 +0200
@@ -22,8 +22,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2013 by Delphix. All rights reserved.
- * Copyright (c) 2015, Nexenta Systems, Inc.  All rights reserved.
- * Copyright (c) 2013, 2014, Nexenta Systems, Inc.  All rights reserved.
+ * Copyright (c) 2016, Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2014 Spectra Logic Corporation, All rights reserved.
  * Copyright 2013 Saso Kiselkov. All rights reserved.
  * Copyright (c) 2014 Integros [integros.com]
@@ -152,6 +151,10 @@
     spa_load_state_t state, spa_import_type_t type, boolean_t mosconfig,
     char **ereport);
 static void spa_vdev_resilver_done(spa_t *spa);
+static void spa_auto_trim(spa_t *spa, uint64_t txg);
+static void spa_vdev_man_trim_done(spa_t *spa);
+static void spa_vdev_auto_trim_done(spa_t *spa);
+static uint64_t spa_min_trim_rate(spa_t *spa);
 
 uint_t		zio_taskq_batch_pct = 75;	/* 1 thread per cpu in pset */
 id_t		zio_taskq_psrset_bind = PS_NONE;
@@ -479,6 +482,8 @@
 		case ZPOOL_PROP_AUTOREPLACE:
 		case ZPOOL_PROP_LISTSNAPS:
 		case ZPOOL_PROP_AUTOEXPAND:
+		case ZPOOL_PROP_FORCETRIM:
+		case ZPOOL_PROP_AUTOTRIM:
 			error = nvpair_value_uint64(elem, &intval);
 			if (!error && intval > 1)
 				error = SET_ERROR(EINVAL);
@@ -1314,6 +1319,16 @@
 	ASSERT(MUTEX_HELD(&spa_namespace_lock));
 
 	/*
+	 * Stop manual trim before stopping spa sync, because manual trim
+	 * needs to execute a synctask (trim timestamp sync) at the end.
+	 */
+	mutex_enter(&spa->spa_auto_trim_lock);
+	mutex_enter(&spa->spa_man_trim_lock);
+	spa_trim_stop_wait(spa);
+	mutex_exit(&spa->spa_man_trim_lock);
+	mutex_exit(&spa->spa_auto_trim_lock);
+
+	/*
 	 * Stop async tasks.
 	 */
 	spa_async_suspend(spa);
@@ -1327,6 +1342,14 @@
 	}
 
 	/*
+	 * Stop autotrim tasks.
+	 */
+	mutex_enter(&spa->spa_auto_trim_lock);
+	if (spa->spa_auto_trim_taskq)
+		spa_auto_trim_taskq_destroy(spa);
+	mutex_exit(&spa->spa_auto_trim_lock);
+
+	/*
 	 * Even though vdev_free() also calls vdev_metaslab_fini, we need
 	 * to call it earlier, before we wait for async i/o to complete.
 	 * This ensures that there is no async metaslab prefetching, by
@@ -2845,10 +2868,22 @@
 		spa_prop_find(spa, ZPOOL_PROP_AUTOEXPAND, &spa->spa_autoexpand);
 		spa_prop_find(spa, ZPOOL_PROP_DEDUPDITTO,
 		    &spa->spa_dedup_ditto);
+		spa_prop_find(spa, ZPOOL_PROP_FORCETRIM, &spa->spa_force_trim);
+
+		mutex_enter(&spa->spa_auto_trim_lock);
+		spa_prop_find(spa, ZPOOL_PROP_AUTOTRIM, &spa->spa_auto_trim);
+		if (spa->spa_auto_trim == SPA_AUTO_TRIM_ON)
+			spa_auto_trim_taskq_create(spa);
+		mutex_exit(&spa->spa_auto_trim_lock);
 
 		spa->spa_autoreplace = (autoreplace != 0);
 	}
 
+	(void) spa_dir_prop(spa, DMU_POOL_TRIM_START_TIME,
+	    &spa->spa_man_trim_start_time);
+	(void) spa_dir_prop(spa, DMU_POOL_TRIM_STOP_TIME,
+	    &spa->spa_man_trim_stop_time);
+
 	/*
 	 * If the 'autoreplace' property is set, then post a resource notifying
 	 * the ZFS DE that it should not issue any faults for unopenable
@@ -3979,6 +4014,13 @@
 	spa->spa_delegation = zpool_prop_default_numeric(ZPOOL_PROP_DELEGATION);
 	spa->spa_failmode = zpool_prop_default_numeric(ZPOOL_PROP_FAILUREMODE);
 	spa->spa_autoexpand = zpool_prop_default_numeric(ZPOOL_PROP_AUTOEXPAND);
+	spa->spa_force_trim = zpool_prop_default_numeric(ZPOOL_PROP_FORCETRIM);
+
+	mutex_enter(&spa->spa_auto_trim_lock);
+	spa->spa_auto_trim = zpool_prop_default_numeric(ZPOOL_PROP_AUTOTRIM);
+	if (spa->spa_auto_trim == SPA_AUTO_TRIM_ON)
+		spa_auto_trim_taskq_create(spa);
+	mutex_exit(&spa->spa_auto_trim_lock);
 
 	if (props != NULL) {
 		spa_configfile_set(spa, props, B_FALSE);
@@ -4671,6 +4713,8 @@
 	if (newvd->vdev_ashift > oldvd->vdev_top->vdev_ashift)
 		return (spa_vdev_exit(spa, newrootvd, txg, EDOM));
 
+	vdev_trim_stop_wait(oldvd->vdev_top);
+
 	/*
 	 * If this is an in-place replacement, update oldvd's path and devid
 	 * to make it distinguishable from newvd, and unopenable from now on.
@@ -4845,6 +4889,8 @@
 	if (vdev_dtl_required(vd))
 		return (spa_vdev_exit(spa, NULL, txg, EBUSY));
 
+	vdev_trim_stop_wait(vd->vdev_top);
+
 	ASSERT(pvd->vdev_children >= 2);
 
 	/*
@@ -5081,6 +5127,8 @@
 	    nvlist_lookup_nvlist(nvl, ZPOOL_CONFIG_L2CACHE, &tmp) == 0)
 		return (spa_vdev_exit(spa, NULL, txg, EINVAL));
 
+	vdev_trim_stop_wait(rvd);
+
 	vml = kmem_zalloc(children * sizeof (vdev_t *), KM_SLEEP);
 	glist = kmem_zalloc(children * sizeof (uint64_t), KM_SLEEP);
 
@@ -5511,6 +5559,8 @@
 		 */
 		metaslab_group_passivate(mg);
 
+		vdev_trim_stop_wait(vd);
+
 		/*
 		 * Wait for the youngest allocations and frees to sync,
 		 * and then wait for the deferral of those frees to finish.
@@ -5904,6 +5954,12 @@
 	if (tasks & SPA_ASYNC_RESILVER)
 		dsl_resilver_restart(spa->spa_dsl_pool, 0);
 
+	if (tasks & SPA_ASYNC_MAN_TRIM_TASKQ_DESTROY) {
+		mutex_enter(&spa->spa_man_trim_lock);
+		spa_man_trim_taskq_destroy(spa);
+		mutex_exit(&spa->spa_man_trim_lock);
+	}
+
 	/*
 	 * Let the world know that we're done.
 	 */
@@ -5975,6 +6031,15 @@
 	mutex_exit(&spa->spa_async_lock);
 }
 
+void
+spa_async_unrequest(spa_t *spa, int task)
+{
+	zfs_dbgmsg("spa=%s async unrequest task=%u", spa->spa_name, task);
+	mutex_enter(&spa->spa_async_lock);
+	spa->spa_async_tasks &= ~task;
+	mutex_exit(&spa->spa_async_lock);
+}
+
 /*
  * ==========================================================================
  * SPA syncing routines
@@ -6383,6 +6448,21 @@
 			case ZPOOL_PROP_FAILUREMODE:
 				spa->spa_failmode = intval;
 				break;
+			case ZPOOL_PROP_FORCETRIM:
+				spa->spa_force_trim = intval;
+				break;
+			case ZPOOL_PROP_AUTOTRIM:
+				mutex_enter(&spa->spa_auto_trim_lock);
+				if (intval != spa->spa_auto_trim) {
+					spa->spa_auto_trim = intval;
+					if (intval != 0)
+						spa_auto_trim_taskq_create(spa);
+					else
+						spa_auto_trim_taskq_destroy(
+						    spa);
+				}
+				mutex_exit(&spa->spa_auto_trim_lock);
+				break;
 			case ZPOOL_PROP_AUTOEXPAND:
 				spa->spa_autoexpand = intval;
 				if (tx->tx_txg != TXG_INITIAL)
@@ -6510,6 +6590,9 @@
 	VERIFY0(avl_numnodes(&spa->spa_alloc_tree));
 	mutex_exit(&spa->spa_alloc_lock);
 
+	if (spa->spa_auto_trim == SPA_AUTO_TRIM_ON)
+		spa_auto_trim(spa, txg);
+
 	/*
 	 * If there are any pending vdev state changes, convert them
 	 * into config changes that go out with this transaction group.
@@ -6952,6 +7035,281 @@
 	zfs_post_sysevent(spa, vd, name);
 }
 
+
+/*
+ * Dispatches all auto-trim processing to all top-level vdevs. This is
+ * called from spa_sync once every txg.
+ */
+static void
+spa_auto_trim(spa_t *spa, uint64_t txg)
+{
+	ASSERT(spa_config_held(spa, SCL_CONFIG, RW_READER) == SCL_CONFIG);
+	ASSERT(!MUTEX_HELD(&spa->spa_auto_trim_lock));
+	ASSERT(spa->spa_auto_trim_taskq != NULL);
+
+	/*
+	 * Another pool management task might be currently prevented from
+	 * starting and the current txg sync was invoked on its behalf,
+	 * so be prepared to postpone autotrim processing.
+	 */
+	if (!mutex_tryenter(&spa->spa_auto_trim_lock))
+		return;
+	spa->spa_num_auto_trimming += spa->spa_root_vdev->vdev_children;
+	mutex_exit(&spa->spa_auto_trim_lock);
+
+	for (uint64_t i = 0; i < spa->spa_root_vdev->vdev_children; i++) {
+		vdev_trim_info_t *vti = kmem_zalloc(sizeof (*vti), KM_SLEEP);
+		vti->vti_vdev = spa->spa_root_vdev->vdev_child[i];
+		vti->vti_txg = txg;
+		vti->vti_done_cb = (void (*)(void *))spa_vdev_auto_trim_done;
+		vti->vti_done_arg = spa;
+		(void) taskq_dispatch(spa->spa_auto_trim_taskq,
+		    (void (*)(void *))vdev_auto_trim, vti, TQ_SLEEP);
+	}
+}
+
+/*
+ * Performs the sync update of the MOS pool directory's trim start/stop values.
+ */
+static void
+spa_trim_update_time_sync(void *arg, dmu_tx_t *tx)
+{
+	spa_t *spa = arg;
+	VERIFY0(zap_update(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,
+	    DMU_POOL_TRIM_START_TIME, sizeof (uint64_t), 1,
+	    &spa->spa_man_trim_start_time, tx));
+	VERIFY0(zap_update(spa->spa_meta_objset, DMU_POOL_DIRECTORY_OBJECT,
+	    DMU_POOL_TRIM_STOP_TIME, sizeof (uint64_t), 1,
+	    &spa->spa_man_trim_stop_time, tx));
+}
+
+/*
+ * Updates the in-core and on-disk manual TRIM operation start/stop time.
+ * Passing UINT64_MAX for either start_time or stop_time means that no
+ * update to that value should be recorded.
+ */
+static dmu_tx_t *
+spa_trim_update_time(spa_t *spa, uint64_t start_time, uint64_t stop_time)
+{
+	int err;
+	dmu_tx_t *tx;
+
+	ASSERT(MUTEX_HELD(&spa->spa_man_trim_lock));
+	if (start_time != UINT64_MAX)
+		spa->spa_man_trim_start_time = start_time;
+	if (stop_time != UINT64_MAX)
+		spa->spa_man_trim_stop_time = stop_time;
+	tx = dmu_tx_create_dd(spa_get_dsl(spa)->dp_mos_dir);
+	err = dmu_tx_assign(tx, TXG_WAIT);
+	if (err) {
+		dmu_tx_abort(tx);
+		return (NULL);
+	}
+	dsl_sync_task_nowait(spa_get_dsl(spa), spa_trim_update_time_sync,
+	    spa, 1, ZFS_SPACE_CHECK_RESERVED, tx);
+
+	return (tx);
+}
+
+/*
+ * Initiates an manual TRIM of the whole pool. This kicks off individual
+ * TRIM tasks for each top-level vdev, which then pass over all of the free
+ * space in all of the vdev's metaslabs and issues TRIM commands for that
+ * space to the underlying vdevs.
+ */
+extern void
+spa_man_trim(spa_t *spa, uint64_t rate, boolean_t fulltrim)
+{
+	dmu_tx_t *time_update_tx;
+	void (*trimfunc)(void *);
+
+	mutex_enter(&spa->spa_man_trim_lock);
+	if (fulltrim)
+		trimfunc = (void (*)(void *))vdev_man_trim_full;
+	else
+		trimfunc = (void (*)(void *))vdev_man_trim;
+
+	if (rate != 0)
+		spa->spa_man_trim_rate = MAX(rate, spa_min_trim_rate(spa));
+	else
+		spa->spa_man_trim_rate = 0;
+
+	if (spa->spa_num_man_trimming) {
+		/*
+		 * TRIM is already ongoing. Wake up all sleeping vdev trim
+		 * threads because the trim rate might have changed above.
+		 */
+		cv_broadcast(&spa->spa_man_trim_update_cv);
+		mutex_exit(&spa->spa_man_trim_lock);
+		return;
+	}
+	spa_man_trim_taskq_create(spa);
+	spa->spa_man_trim_stop = B_FALSE;
+
+	spa_event_notify(spa, NULL, ESC_ZFS_TRIM_START);
+	spa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);
+	for (uint64_t i = 0; i < spa->spa_root_vdev->vdev_children; i++) {
+		vdev_t *vd = spa->spa_root_vdev->vdev_child[i];
+		vdev_trim_info_t *vti = kmem_zalloc(sizeof (*vti), KM_SLEEP);
+		vti->vti_vdev = vd;
+		vti->vti_done_cb = (void (*)(void *))spa_vdev_man_trim_done;
+		vti->vti_done_arg = spa;
+		spa->spa_num_man_trimming++;
+
+		vd->vdev_trim_prog = 0;
+		(void) taskq_dispatch(spa->spa_man_trim_taskq,
+		    trimfunc, vti, TQ_SLEEP);
+	}
+	spa_config_exit(spa, SCL_CONFIG, FTAG);
+	time_update_tx = spa_trim_update_time(spa, gethrestime_sec(), 0);
+	mutex_exit(&spa->spa_man_trim_lock);
+	/* mustn't hold spa_man_trim_lock to prevent deadlock /w syncing ctx */
+	if (time_update_tx != NULL)
+		dmu_tx_commit(time_update_tx);
+}
+
+/*
+ * Orders a manual TRIM operation to stop and returns immediately.
+ */
+extern void
+spa_man_trim_stop(spa_t *spa)
+{
+	boolean_t held = MUTEX_HELD(&spa->spa_man_trim_lock);
+	if (!held)
+		mutex_enter(&spa->spa_man_trim_lock);
+	spa->spa_man_trim_stop = B_TRUE;
+	cv_broadcast(&spa->spa_man_trim_update_cv);
+	if (!held)
+		mutex_exit(&spa->spa_man_trim_lock);
+}
+
+/*
+ * Orders a manual TRIM operation to stop and waits for both manual and
+ * automatic TRIM to complete. By holding both the spa_man_trim_lock and
+ * the spa_auto_trim_lock, the caller can guarantee that after this
+ * function returns, no new TRIM operations can be initiated in parallel.
+ */
+void
+spa_trim_stop_wait(spa_t *spa)
+{
+	ASSERT(MUTEX_HELD(&spa->spa_man_trim_lock));
+	ASSERT(MUTEX_HELD(&spa->spa_auto_trim_lock));
+	spa->spa_man_trim_stop = B_TRUE;
+	cv_broadcast(&spa->spa_man_trim_update_cv);
+	while (spa->spa_num_man_trimming > 0)
+		cv_wait(&spa->spa_man_trim_done_cv, &spa->spa_man_trim_lock);
+	while (spa->spa_num_auto_trimming > 0)
+		cv_wait(&spa->spa_auto_trim_done_cv, &spa->spa_auto_trim_lock);
+}
+
+/*
+ * Returns manual TRIM progress. Progress is indicated by four return values:
+ * 1) prog: the number of bytes of space on the pool in total that manual
+ *	TRIM has already passed (regardless if the space is allocated or not).
+ *	Completion of the operation is indicated when either the returned value
+ *	is zero, or when the returned value is equal to the sum of the sizes of
+ *	all top-level vdevs.
+ * 2) rate: the trim rate in bytes per second. A value of zero indicates that
+ *	trim progresses as fast as possible.
+ * 3) start_time: the UNIXTIME of when the last manual TRIM operation was
+ *	started. If no manual trim was ever initiated on the pool, this is
+ *	zero.
+ * 4) stop_time: the UNIXTIME of when the last manual TRIM operation has
+ *	stopped on the pool. If a trim was started (start_time != 0), but has
+ *	not yet completed, stop_time will be zero. If a trim is NOT currently
+ *	ongoing and start_time is non-zero, this indicates that the previously
+ *	initiated TRIM operation was interrupted.
+ */
+extern void
+spa_get_trim_prog(spa_t *spa, uint64_t *prog, uint64_t *rate,
+    uint64_t *start_time, uint64_t *stop_time)
+{
+	uint64_t total = 0;
+	vdev_t *root_vd = spa->spa_root_vdev;
+
+	ASSERT(spa_config_held(spa, SCL_CONFIG, RW_READER));
+	mutex_enter(&spa->spa_man_trim_lock);
+	if (spa->spa_num_man_trimming > 0) {
+		for (uint64_t i = 0; i < root_vd->vdev_children; i++) {
+			total += root_vd->vdev_child[i]->vdev_trim_prog;
+		}
+	}
+	*prog = total;
+	*rate = spa->spa_man_trim_rate;
+	*start_time = spa->spa_man_trim_start_time;
+	*stop_time = spa->spa_man_trim_stop_time;
+	mutex_exit(&spa->spa_man_trim_lock);
+}
+
+/*
+ * Callback when a vdev_man_trim has finished on a single top-level vdev.
+ */
+static void
+spa_vdev_man_trim_done(spa_t *spa)
+{
+	dmu_tx_t *time_update_tx = NULL;
+
+	mutex_enter(&spa->spa_man_trim_lock);
+	ASSERT(spa->spa_num_man_trimming > 0);
+	spa->spa_num_man_trimming--;
+	if (spa->spa_num_man_trimming == 0) {
+		/* if we were interrupted, leave stop_time at zero */
+		if (!spa->spa_man_trim_stop)
+			time_update_tx = spa_trim_update_time(spa, UINT64_MAX,
+			    gethrestime_sec());
+		spa_event_notify(spa, NULL, ESC_ZFS_TRIM_FINISH);
+		spa_async_request(spa, SPA_ASYNC_MAN_TRIM_TASKQ_DESTROY);
+		cv_broadcast(&spa->spa_man_trim_done_cv);
+	}
+	mutex_exit(&spa->spa_man_trim_lock);
+
+	if (time_update_tx != NULL)
+		dmu_tx_commit(time_update_tx);
+}
+
+/*
+ * Called from vdev_auto_trim when a vdev has completed its auto-trim
+ * processing.
+ */
+static void
+spa_vdev_auto_trim_done(spa_t *spa)
+{
+	mutex_enter(&spa->spa_auto_trim_lock);
+	ASSERT(spa->spa_num_auto_trimming > 0);
+	spa->spa_num_auto_trimming--;
+	if (spa->spa_num_auto_trimming == 0)
+		cv_broadcast(&spa->spa_auto_trim_done_cv);
+	mutex_exit(&spa->spa_auto_trim_lock);
+}
+
+/*
+ * Determines the minimum sensible rate at which a manual TRIM can be
+ * performed on a given spa and returns it. Since we perform TRIM in
+ * metaslab-sized increments, we'll just let the longest step between
+ * metaslab TRIMs be 100s (random number, really). Thus, on a typical
+ * 200-metaslab vdev, the longest TRIM should take is about 5.5 hours.
+ * It *can* take longer if the device is really slow respond to
+ * zio_trim() commands or it contains more than 200 metaslabs, or
+ * metaslab sizes vary widely between top-level vdevs.
+ */
+static uint64_t
+spa_min_trim_rate(spa_t *spa)
+{
+	uint64_t i, smallest_ms_sz = UINT64_MAX;
+
+	/* find the smallest metaslab */
+	spa_config_enter(spa, SCL_CONFIG, FTAG, RW_READER);
+	for (i = 0; i < spa->spa_root_vdev->vdev_children; i++) {
+		smallest_ms_sz = MIN(smallest_ms_sz,
+		    spa->spa_root_vdev->vdev_child[i]->vdev_ms[0]->ms_size);
+	}
+	spa_config_exit(spa, SCL_CONFIG, FTAG);
+	VERIFY(smallest_ms_sz != 0);
+
+	/* minimum TRIM rate is 1/100th of the smallest metaslab size */
+	return (smallest_ms_sz / 100);
+}
+
 #if defined(_KERNEL) && defined(HAVE_SPL)
 /* state manipulation functions */
 EXPORT_SYMBOL(spa_open);
diff -Nuar zfs-kmod-9999.orig/module/zfs/spa_config.c zfs-kmod-9999/module/zfs/spa_config.c
--- zfs-kmod-9999.orig/module/zfs/spa_config.c	2017-05-06 11:03:37.156219028 +0200
+++ zfs-kmod-9999/module/zfs/spa_config.c	2017-05-06 11:04:34.189030835 +0200
@@ -21,8 +21,8 @@
 
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2011 Nexenta Systems, Inc. All rights reserved.
  * Copyright (c) 2011, 2015 by Delphix. All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/spa.h>
@@ -493,6 +493,19 @@
 	fnvlist_add_nvlist(config, ZPOOL_CONFIG_VDEV_TREE, nvroot);
 	nvlist_free(nvroot);
 
+	/* If we're getting stats, calculate trim progress from leaf vdevs. */
+	if (getstats) {
+		uint64_t prog, rate, start_time, stop_time;
+
+		spa_get_trim_prog(spa, &prog, &rate, &start_time, &stop_time);
+		fnvlist_add_uint64(config, ZPOOL_CONFIG_TRIM_PROG, prog);
+		fnvlist_add_uint64(config, ZPOOL_CONFIG_TRIM_RATE, rate);
+		fnvlist_add_uint64(config, ZPOOL_CONFIG_TRIM_START_TIME,
+		    start_time);
+		fnvlist_add_uint64(config, ZPOOL_CONFIG_TRIM_STOP_TIME,
+		    stop_time);
+	}
+
 	/*
 	 * Store what's necessary for reading the MOS in the label.
 	 */
diff -Nuar zfs-kmod-9999.orig/module/zfs/spa_misc.c zfs-kmod-9999/module/zfs/spa_misc.c
--- zfs-kmod-9999.orig/module/zfs/spa_misc.c	2017-05-06 11:03:37.158219021 +0200
+++ zfs-kmod-9999/module/zfs/spa_misc.c	2017-05-06 11:04:34.189030835 +0200
@@ -21,7 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2017 by Delphix. All rights reserved.
- * Copyright 2015 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2014 Spectra Logic Corporation, All rights reserved.
  * Copyright 2013 Saso Kiselkov. All rights reserved.
  */
@@ -228,6 +228,22 @@
  * manipulation of the namespace.
  */
 
+struct spa_trimstats {
+	kstat_named_t	st_extents;		/* # of extents issued to zio */
+	kstat_named_t	st_bytes;		/* # of bytes issued to zio */
+	kstat_named_t	st_extents_skipped;	/* # of extents too small */
+	kstat_named_t	st_bytes_skipped;	/* bytes in extents_skipped */
+	kstat_named_t	st_auto_slow;		/* trim slow, exts dropped */
+};
+
+static spa_trimstats_t spa_trimstats_template = {
+	{ "extents",		KSTAT_DATA_UINT64 },
+	{ "bytes",		KSTAT_DATA_UINT64 },
+	{ "extents_skipped",	KSTAT_DATA_UINT64 },
+	{ "bytes_skipped",	KSTAT_DATA_UINT64 },
+	{ "auto_slow",		KSTAT_DATA_UINT64 },
+};
+
 static avl_tree_t spa_namespace_avl;
 kmutex_t spa_namespace_lock;
 static kcondvar_t spa_namespace_cv;
@@ -352,6 +368,14 @@
 uint64_t spa_min_slop = 128 * 1024 * 1024;
 
 /*
+ * Percentage of the number of CPUs to use as the autotrim taskq thread count.
+ */
+int zfs_auto_trim_taskq_batch_pct = 75;
+
+static void spa_trimstats_create(spa_t *spa);
+static void spa_trimstats_destroy(spa_t *spa);
+
+/*
  * ==========================================================================
  * SPA config locking
  * ==========================================================================
@@ -581,12 +605,17 @@
 	mutex_init(&spa->spa_vdev_top_lock, NULL, MUTEX_DEFAULT, NULL);
 	mutex_init(&spa->spa_feat_stats_lock, NULL, MUTEX_DEFAULT, NULL);
 	mutex_init(&spa->spa_alloc_lock, NULL, MUTEX_DEFAULT, NULL);
+	mutex_init(&spa->spa_auto_trim_lock, NULL, MUTEX_DEFAULT, NULL);
+	mutex_init(&spa->spa_man_trim_lock, NULL, MUTEX_DEFAULT, NULL);
 
 	cv_init(&spa->spa_async_cv, NULL, CV_DEFAULT, NULL);
 	cv_init(&spa->spa_evicting_os_cv, NULL, CV_DEFAULT, NULL);
 	cv_init(&spa->spa_proc_cv, NULL, CV_DEFAULT, NULL);
 	cv_init(&spa->spa_scrub_io_cv, NULL, CV_DEFAULT, NULL);
 	cv_init(&spa->spa_suspend_cv, NULL, CV_DEFAULT, NULL);
+	cv_init(&spa->spa_auto_trim_done_cv, NULL, CV_DEFAULT, NULL);
+	cv_init(&spa->spa_man_trim_update_cv, NULL, CV_DEFAULT, NULL);
+	cv_init(&spa->spa_man_trim_done_cv, NULL, CV_DEFAULT, NULL);
 
 	for (t = 0; t < TXG_SIZE; t++)
 		bplist_create(&spa->spa_free_bplist[t]);
@@ -646,6 +675,8 @@
 		    KM_SLEEP) == 0);
 	}
 
+	spa_trimstats_create(spa);
+
 	spa->spa_debug = ((zfs_flags & ZFS_DEBUG_SPA) != 0);
 
 	spa->spa_min_ashift = INT_MAX;
@@ -709,6 +740,8 @@
 	spa_stats_destroy(spa);
 	spa_config_lock_destroy(spa);
 
+	spa_trimstats_destroy(spa);
+
 	for (t = 0; t < TXG_SIZE; t++)
 		bplist_destroy(&spa->spa_free_bplist[t]);
 
@@ -719,6 +752,9 @@
 	cv_destroy(&spa->spa_proc_cv);
 	cv_destroy(&spa->spa_scrub_io_cv);
 	cv_destroy(&spa->spa_suspend_cv);
+	cv_destroy(&spa->spa_auto_trim_done_cv);
+	cv_destroy(&spa->spa_man_trim_update_cv);
+	cv_destroy(&spa->spa_man_trim_done_cv);
 
 	mutex_destroy(&spa->spa_alloc_lock);
 	mutex_destroy(&spa->spa_async_lock);
@@ -733,6 +769,8 @@
 	mutex_destroy(&spa->spa_suspend_lock);
 	mutex_destroy(&spa->spa_vdev_top_lock);
 	mutex_destroy(&spa->spa_feat_stats_lock);
+	mutex_destroy(&spa->spa_auto_trim_lock);
+	mutex_destroy(&spa->spa_man_trim_lock);
 
 	kmem_free(spa, sizeof (spa_t));
 }
@@ -1051,6 +1089,9 @@
 {
 	mutex_enter(&spa->spa_vdev_top_lock);
 	mutex_enter(&spa_namespace_lock);
+	mutex_enter(&spa->spa_auto_trim_lock);
+	mutex_enter(&spa->spa_man_trim_lock);
+	spa_trim_stop_wait(spa);
 	return (spa_vdev_config_enter(spa));
 }
 
@@ -1141,6 +1182,8 @@
 spa_vdev_exit(spa_t *spa, vdev_t *vd, uint64_t txg, int error)
 {
 	spa_vdev_config_exit(spa, vd, txg, error, FTAG);
+	mutex_exit(&spa->spa_man_trim_lock);
+	mutex_exit(&spa->spa_auto_trim_lock);
 	mutex_exit(&spa_namespace_lock);
 	mutex_exit(&spa->spa_vdev_top_lock);
 
@@ -1759,6 +1802,18 @@
 	return (spa->spa_deadman_synctime);
 }
 
+spa_force_trim_t
+spa_get_force_trim(spa_t *spa)
+{
+	return (spa->spa_force_trim);
+}
+
+spa_auto_trim_t
+spa_get_auto_trim(spa_t *spa)
+{
+	return (spa->spa_auto_trim);
+}
+
 uint64_t
 dva_get_dsize_sync(spa_t *spa, const dva_t *dva)
 {
@@ -2057,6 +2112,185 @@
 		return (DNODE_MIN_SIZE);
 }
 
+int
+spa_trimstats_kstat_update(kstat_t *ksp, int rw)
+{
+	spa_t *spa;
+	spa_trimstats_t *trimstats;
+	int i;
+
+	ASSERT(ksp != NULL);
+
+	if (rw == KSTAT_WRITE) {
+		spa = ksp->ks_private;
+		trimstats = spa->spa_trimstats;
+		for (i = 0; i < sizeof (spa_trimstats_t) /
+		    sizeof (kstat_named_t); ++i)
+			((kstat_named_t *)trimstats)[i].value.ui64 = 0;
+	}
+	return (0);
+}
+
+/*
+ * Creates the trim kstats structure for a spa.
+ */
+static void
+spa_trimstats_create(spa_t *spa)
+{
+	char name[KSTAT_STRLEN];
+	kstat_t *ksp;
+
+	if (spa->spa_name[0] == '$')
+		return;
+
+	ASSERT3P(spa->spa_trimstats, ==, NULL);
+	ASSERT3P(spa->spa_trimstats_ks, ==, NULL);
+
+	(void) snprintf(name, KSTAT_STRLEN, "zfs/%s", spa_name(spa));
+	ksp = kstat_create(name, 0, "trimstats", "misc",
+	    KSTAT_TYPE_NAMED, sizeof (spa_trimstats_template) /
+	    sizeof (kstat_named_t), KSTAT_FLAG_VIRTUAL);
+	if (ksp != NULL) {
+		ksp->ks_private = spa;
+		ksp->ks_update = spa_trimstats_kstat_update;
+		spa->spa_trimstats_ks = ksp;
+		spa->spa_trimstats =
+		    kmem_alloc(sizeof (spa_trimstats_t), KM_SLEEP);
+		*spa->spa_trimstats = spa_trimstats_template;
+		spa->spa_trimstats_ks->ks_data = spa->spa_trimstats;
+		kstat_install(spa->spa_trimstats_ks);
+	} else {
+		cmn_err(CE_NOTE, "!Cannot create trim kstats for pool %s",
+		    spa->spa_name);
+	}
+}
+
+/*
+ * Destroys the trim kstats for a spa.
+ */
+static void
+spa_trimstats_destroy(spa_t *spa)
+{
+	if (spa->spa_trimstats_ks) {
+		kstat_delete(spa->spa_trimstats_ks);
+		kmem_free(spa->spa_trimstats, sizeof (spa_trimstats_t));
+		spa->spa_trimstats_ks = NULL;
+	}
+}
+
+/*
+ * Updates the numerical trim kstats for a spa.
+ */
+void
+spa_trimstats_update(spa_t *spa, uint64_t extents, uint64_t bytes,
+    uint64_t extents_skipped, uint64_t bytes_skipped)
+{
+	spa_trimstats_t *st = spa->spa_trimstats;
+	if (st) {
+		atomic_add_64(&st->st_extents.value.ui64, extents);
+		atomic_add_64(&st->st_bytes.value.ui64, bytes);
+		atomic_add_64(&st->st_extents_skipped.value.ui64,
+		    extents_skipped);
+		atomic_add_64(&st->st_bytes_skipped.value.ui64,
+		    bytes_skipped);
+	}
+}
+
+/*
+ * Increments the slow-trim kstat for a spa.
+ */
+void
+spa_trimstats_auto_slow_incr(spa_t *spa)
+{
+	spa_trimstats_t *st = spa->spa_trimstats;
+	if (st)
+		atomic_inc_64(&st->st_auto_slow.value.ui64);
+}
+
+/*
+ * Creates the taskq used for dispatching auto-trim. This is called only when
+ * the property is set to `on' or when the pool is loaded (and the autotrim
+ * property is `on').
+ */
+void
+spa_auto_trim_taskq_create(spa_t *spa)
+{
+	char *name = kmem_alloc(MAXPATHLEN, KM_SLEEP);
+
+	ASSERT(MUTEX_HELD(&spa->spa_auto_trim_lock));
+	ASSERT(spa->spa_auto_trim_taskq == NULL);
+	(void) snprintf(name, MAXPATHLEN, "%s_auto_trim", spa->spa_name);
+	spa->spa_auto_trim_taskq = taskq_create(name,
+	    zfs_auto_trim_taskq_batch_pct, minclsyspri, 1, INT_MAX,
+	    TASKQ_THREADS_CPU_PCT);
+	VERIFY(spa->spa_auto_trim_taskq != NULL);
+	kmem_free(name, MAXPATHLEN);
+}
+
+/*
+ * Creates the taskq for dispatching manual trim. This taskq is recreated
+ * each time `zpool trim <poolname>' is issued and destroyed after the run
+ * completes in an async spa request.
+ */
+void
+spa_man_trim_taskq_create(spa_t *spa)
+{
+	char *name = kmem_alloc(MAXPATHLEN, KM_SLEEP);
+
+	ASSERT(MUTEX_HELD(&spa->spa_man_trim_lock));
+	spa_async_unrequest(spa, SPA_ASYNC_MAN_TRIM_TASKQ_DESTROY);
+	if (spa->spa_man_trim_taskq != NULL) {
+		/*
+		 * The async taskq destroy has been pre-empted, so just
+		 * return, the taskq is still good to use.
+		 */
+		return;
+	}
+	(void) snprintf(name, MAXPATHLEN, "%s_man_trim", spa->spa_name);
+	spa->spa_man_trim_taskq = taskq_create(name,
+	    spa->spa_root_vdev->vdev_children, minclsyspri,
+	    spa->spa_root_vdev->vdev_children,
+	    spa->spa_root_vdev->vdev_children, TASKQ_PREPOPULATE);
+	VERIFY(spa->spa_man_trim_taskq != NULL);
+	kmem_free(name, MAXPATHLEN);
+}
+
+/*
+ * Destroys the taskq created in spa_auto_trim_taskq_create. The taskq
+ * is only destroyed when the autotrim property is set to `off'.
+ */
+void
+spa_auto_trim_taskq_destroy(spa_t *spa)
+{
+	ASSERT(MUTEX_HELD(&spa->spa_auto_trim_lock));
+	ASSERT(spa->spa_auto_trim_taskq != NULL);
+	while (spa->spa_num_auto_trimming != 0)
+		cv_wait(&spa->spa_auto_trim_done_cv, &spa->spa_auto_trim_lock);
+	taskq_destroy(spa->spa_auto_trim_taskq);
+	spa->spa_auto_trim_taskq = NULL;
+}
+
+/*
+ * Destroys the taskq created in spa_man_trim_taskq_create. The taskq is
+ * destroyed after a manual trim run completes from an async spa request.
+ * There is a bit of lag between an async request being issued at the
+ * completion of a trim run and it finally being acted on, hence why this
+ * function checks if new manual trimming threads haven't been re-spawned.
+ * If they have, we assume the async spa request been preempted by another
+ * manual trim request and we back off.
+ */
+void
+spa_man_trim_taskq_destroy(spa_t *spa)
+{
+	ASSERT(MUTEX_HELD(&spa->spa_man_trim_lock));
+	ASSERT(spa->spa_man_trim_taskq != NULL);
+	if (spa->spa_num_man_trimming != 0)
+		/* another trim got started before we got here, back off */
+		return;
+	taskq_destroy(spa->spa_man_trim_taskq);
+	spa->spa_man_trim_taskq = NULL;
+}
+
 #if defined(_KERNEL) && defined(HAVE_SPL)
 /* Namespace manipulation */
 EXPORT_SYMBOL(spa_lookup);
@@ -2163,5 +2397,10 @@
 
 module_param(spa_slop_shift, int, 0644);
 MODULE_PARM_DESC(spa_slop_shift, "Reserved free space in pool");
+
+module_param(zfs_auto_trim_taskq_batch_pct, int, 0644);
+MODULE_PARM_DESC(zfs_auto_trim_taskq_batch_pct,
+	"Percentage of the number of CPUs to use as the autotrim taskq"
+	" thread count");
 /* END CSTYLED */
 #endif
diff -Nuar zfs-kmod-9999.orig/module/zfs/trace.c zfs-kmod-9999/module/zfs/trace.c
--- zfs-kmod-9999.orig/module/zfs/trace.c	2017-05-06 11:03:37.159219018 +0200
+++ zfs-kmod-9999/module/zfs/trace.c	2017-05-06 11:04:34.189030835 +0200
@@ -26,6 +26,7 @@
 #include <sys/multilist.h>
 #include <sys/arc_impl.h>
 #include <sys/vdev_impl.h>
+#include <sys/metaslab_impl.h>
 #include <sys/zio.h>
 #include <sys/dbuf.h>
 #include <sys/dmu_objset.h>
@@ -46,6 +47,7 @@
 #include <sys/trace_dnode.h>
 #include <sys/trace_multilist.h>
 #include <sys/trace_txg.h>
+#include <sys/trace_vdev.h>
 #include <sys/trace_zil.h>
 #include <sys/trace_zio.h>
 #include <sys/trace_zrlock.h>
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev.c zfs-kmod-9999/module/zfs/vdev.c
--- zfs-kmod-9999.orig/module/zfs/vdev.c	2017-05-06 11:03:37.161219011 +0200
+++ zfs-kmod-9999/module/zfs/vdev.c	2017-05-06 11:04:34.210030766 +0200
@@ -48,6 +48,7 @@
 #include <sys/abd.h>
 #include <sys/zvol.h>
 #include <sys/zfs_ratelimit.h>
+#include <sys/trace_vdev.h>
 
 /*
  * When a vdev is added, it will be divided into approximately (but no
@@ -73,6 +74,15 @@
 };
 
 /*
+ * If we accumulate a lot of trim extents due to trim running slow, this
+ * is the memory pressure valve. We limit the amount of memory consumed
+ * by the extents in memory to physmem/zfs_trim_mem_lim_fact (by default
+ * 2%). If we exceed this limit, we start throwing out new extents
+ * without queueing them.
+ */
+int zfs_trim_mem_lim_fact = 50;
+
+/*
  * Given a vdev type, return the appropriate ops vector.
  */
 static vdev_ops_t *
@@ -378,6 +388,9 @@
 	vdev_queue_init(vd);
 	vdev_cache_init(vd);
 
+	mutex_init(&vd->vdev_trim_zios_lock, NULL, MUTEX_DEFAULT, NULL);
+	cv_init(&vd->vdev_trim_zios_cv, NULL, CV_DEFAULT, NULL);
+
 	return (vd);
 }
 
@@ -713,6 +726,10 @@
 	zfs_ratelimit_fini(&vd->vdev_delay_rl);
 	zfs_ratelimit_fini(&vd->vdev_checksum_rl);
 
+	ASSERT0(vd->vdev_trim_zios);
+	mutex_destroy(&vd->vdev_trim_zios_lock);
+	cv_destroy(&vd->vdev_trim_zios_cv);
+
 	if (vd == spa->spa_root_vdev)
 		spa->spa_root_vdev = NULL;
 
@@ -1724,6 +1741,23 @@
 	(void) txg_list_add(&vd->vdev_spa->spa_vdev_txg_list, vd, txg);
 }
 
+boolean_t
+vdev_is_dirty(vdev_t *vd, int flags, void *arg)
+{
+	ASSERT(vd == vd->vdev_top);
+	ASSERT(!vd->vdev_ishole);
+	ASSERT(ISP2(flags));
+	ASSERT(spa_writeable(vd->vdev_spa));
+	ASSERT3U(flags, ==, VDD_METASLAB);
+
+	for (uint64_t txg = 0; txg < TXG_SIZE; txg++) {
+		if (txg_list_member(&vd->vdev_ms_list, arg, txg))
+			return (B_TRUE);
+	}
+
+	return (B_FALSE);
+}
+
 void
 vdev_dirty_leaves(vdev_t *vd, int flags, uint64_t txg)
 {
@@ -3054,13 +3088,19 @@
 				vs->vs_self_healed += psize;
 		}
 
+		if ((!vd->vdev_ops->vdev_op_leaf) ||
+		    (zio->io_priority >= ZIO_PRIORITY_NUM_QUEUEABLE)) {
+			mutex_exit(&vd->vdev_stat_lock);
+			return;
+		}
+
 		/*
 		 * The bytes/ops/histograms are recorded at the leaf level and
 		 * aggregated into the higher level vdevs in vdev_get_stats().
+		 * Successful TRIM zios include aggregate statistics for all
+		 * discards which resulted from the single TRIM zio.
 		 */
-		if (vd->vdev_ops->vdev_op_leaf &&
-		    (zio->io_priority < ZIO_PRIORITY_NUM_QUEUEABLE)) {
-
+		if (!ZIO_IS_TRIM(zio)) {
 			vs->vs_ops[type]++;
 			vs->vs_bytes[type] += psize;
 
@@ -3080,6 +3120,24 @@
 				vsx->vsx_total_histo[type]
 				    [L_HISTO(zio->io_delta)]++;
 			}
+		} else if (zio->io_dfl_stats != NULL) {
+			vdev_stat_trim_t *vsd = zio->io_dfl_stats;
+
+			vs->vs_ops[type] += vsd->vsd_ops;
+			vs->vs_bytes[type] += vsd->vsd_bytes;
+
+			for (int i = 0; i < VDEV_RQ_HISTO_BUCKETS; i++)
+				vsx->vsx_ind_histo[zio->io_priority][i] +=
+				    vsd->vsd_ind_histo[i];
+
+			for (int i = 0; i < VDEV_L_HISTO_BUCKETS; i++) {
+				vsx->vsx_queue_histo[zio->io_priority][i] +=
+				    vsd->vsd_queue_histo[i];
+				vsx->vsx_disk_histo[type][i] +=
+				    vsd->vsd_disk_histo[i];
+				vsx->vsx_total_histo[type][i] +=
+				    vsd->vsd_total_histo[i];
+			}
 		}
 
 		mutex_exit(&vd->vdev_stat_lock);
@@ -3161,6 +3219,33 @@
 }
 
 /*
+ * Update the aggregate statistics for a TRIM zio.
+ */
+void
+vdev_trim_stat_update(zio_t *zio, uint64_t psize, vdev_trim_stat_flags_t flags)
+{
+	vdev_stat_trim_t *vsd = zio->io_dfl_stats;
+	hrtime_t now = gethrtime();
+	hrtime_t io_delta = io_delta = now - zio->io_timestamp;
+	hrtime_t io_delay = now - zio->io_delay;
+
+	if (flags & TRIM_STAT_OP) {
+		vsd->vsd_ops++;
+		vsd->vsd_bytes += psize;
+	}
+
+	if (flags & TRIM_STAT_RQ_HISTO) {
+		vsd->vsd_ind_histo[RQ_HISTO(psize)]++;
+	}
+
+	if (flags & TRIM_STAT_L_HISTO) {
+		vsd->vsd_queue_histo[L_HISTO(io_delta - io_delay)]++;
+		vsd->vsd_disk_histo[L_HISTO(io_delay)]++;
+		vsd->vsd_total_histo[L_HISTO(io_delta)]++;
+	}
+}
+
+/*
  * Update the in-core space usage stats for this vdev, its metaslab class,
  * and the root vdev.
  */
@@ -3700,6 +3785,201 @@
 	}
 }
 
+/*
+ * Implements the per-vdev portion of manual TRIM. The function passes over
+ * all metaslabs on this vdev and performs a metaslab_trim_all on them. It's
+ * also responsible for rate-control if spa_man_trim_rate is non-zero.
+ *
+ * If fulltrim is set, metaslabs without spacemaps are also trimmed.
+ */
+static void
+vdev_man_trim_impl(vdev_trim_info_t *vti, boolean_t fulltrim)
+{
+	clock_t t = ddi_get_lbolt();
+	spa_t *spa = vti->vti_vdev->vdev_spa;
+	vdev_t *vd = vti->vti_vdev;
+	uint64_t i, cursor;
+	boolean_t was_loaded = B_FALSE;
+
+	vd->vdev_man_trimming = B_TRUE;
+	vd->vdev_trim_prog = 0;
+
+	spa_config_enter(spa, SCL_STATE_ALL, FTAG, RW_READER);
+	ASSERT(vd->vdev_ms[0] != NULL);
+	cursor = vd->vdev_ms[0]->ms_start;
+	i = 0;
+	while (i < vti->vti_vdev->vdev_ms_count && !spa->spa_man_trim_stop) {
+		uint64_t delta;
+		metaslab_t *msp = vd->vdev_ms[i];
+		zio_t *trim_io;
+
+		if (msp->ms_sm == NULL && !fulltrim) {
+			i++;
+			continue;
+		}
+
+		trim_io = metaslab_trim_all(msp, &cursor, &delta, &was_loaded);
+		spa_config_exit(spa, SCL_STATE_ALL, FTAG);
+
+		if (trim_io != NULL) {
+			ASSERT3U(cursor, >=, vd->vdev_ms[0]->ms_start);
+			vd->vdev_trim_prog = cursor - vd->vdev_ms[0]->ms_start;
+			(void) zio_wait(trim_io);
+		} else {
+			/*
+			 * If there was nothing more left to trim, that means
+			 * this metaslab is either done trimming, or we
+			 * couldn't load it, move to the next one.
+			 */
+			i++;
+			if (i < vti->vti_vdev->vdev_ms_count)
+				ASSERT3U(vd->vdev_ms[i]->ms_start, ==, cursor);
+		}
+
+		/* delay loop to handle fixed-rate trimming */
+		for (;;) {
+			uint64_t rate = spa->spa_man_trim_rate;
+			uint64_t sleep_delay;
+
+			if (rate == 0) {
+				/* No delay, just update 't' and move on. */
+				t = ddi_get_lbolt();
+				break;
+			}
+
+			sleep_delay = (delta * hz) / rate;
+			mutex_enter(&spa->spa_man_trim_lock);
+			(void) cv_timedwait(&spa->spa_man_trim_update_cv,
+			    &spa->spa_man_trim_lock, t);
+			mutex_exit(&spa->spa_man_trim_lock);
+
+			/* If interrupted, don't try to relock, get out */
+			if (spa->spa_man_trim_stop)
+				goto out;
+
+			/* Timeout passed, move on to the next metaslab. */
+			if (ddi_get_lbolt() >= t + sleep_delay) {
+				t += sleep_delay;
+				break;
+			}
+		}
+		spa_config_enter(spa, SCL_STATE_ALL, FTAG, RW_READER);
+	}
+	spa_config_exit(spa, SCL_STATE_ALL, FTAG);
+out:
+	/*
+	 * Ensure we're marked as "completed" even if we've had to stop
+	 * before processing all metaslabs.
+	 */
+	mutex_enter(&vd->vdev_stat_lock);
+	vd->vdev_trim_prog = vd->vdev_stat.vs_space;
+	mutex_exit(&vd->vdev_stat_lock);
+	vd->vdev_man_trimming = B_FALSE;
+
+	ASSERT(vti->vti_done_cb != NULL);
+	vti->vti_done_cb(vti->vti_done_arg);
+
+	kmem_free(vti, sizeof (*vti));
+}
+
+void
+vdev_man_trim(vdev_trim_info_t *vti)
+{
+	vdev_man_trim_impl(vti, B_FALSE);
+}
+
+void
+vdev_man_trim_full(vdev_trim_info_t *vti)
+{
+	vdev_man_trim_impl(vti, B_TRUE);
+}
+
+/*
+ * Runs through all metaslabs on the vdev and does their autotrim processing.
+ */
+void
+vdev_auto_trim(vdev_trim_info_t *vti)
+{
+	vdev_t *vd = vti->vti_vdev;
+	spa_t *spa = vd->vdev_spa;
+	uint64_t txg = vti->vti_txg;
+	uint64_t mlim = 0, mused = 0;
+	boolean_t limited;
+
+	ASSERT3P(vd->vdev_top, ==, vd);
+
+	if (vd->vdev_man_trimming)
+		goto out;
+
+	spa_config_enter(spa, SCL_STATE_ALL, FTAG, RW_READER);
+	for (uint64_t i = 0; i < vd->vdev_ms_count; i++)
+		mused += metaslab_trim_mem_used(vd->vdev_ms[i]);
+	mlim = (physmem * PAGESIZE) / (zfs_trim_mem_lim_fact *
+	    spa->spa_root_vdev->vdev_children);
+	limited = mused > mlim;
+	DTRACE_PROBE3(autotrim__mem__lim, vdev_t *, vd, uint64_t, mused,
+	    uint64_t, mlim);
+	for (uint64_t i = 0; i < vd->vdev_ms_count; i++)
+		metaslab_auto_trim(vd->vdev_ms[i], txg, !limited);
+	spa_config_exit(spa, SCL_STATE_ALL, FTAG);
+
+out:
+	ASSERT(vti->vti_done_cb != NULL);
+	vti->vti_done_cb(vti->vti_done_arg);
+
+	kmem_free(vti, sizeof (*vti));
+}
+
+static void
+trim_stop_set(vdev_t *vd, boolean_t flag)
+{
+	mutex_enter(&vd->vdev_trim_zios_lock);
+	vd->vdev_trim_zios_stop = flag;
+	mutex_exit(&vd->vdev_trim_zios_lock);
+
+	for (uint64_t i = 0; i < vd->vdev_children; i++)
+		trim_stop_set(vd->vdev_child[i], flag);
+}
+
+static void
+trim_stop_wait(vdev_t *vd)
+{
+	mutex_enter(&vd->vdev_trim_zios_lock);
+	while (vd->vdev_trim_zios)
+		cv_wait(&vd->vdev_trim_zios_cv, &vd->vdev_trim_zios_lock);
+	mutex_exit(&vd->vdev_trim_zios_lock);
+
+	for (uint64_t i = 0; i < vd->vdev_children; i++)
+		trim_stop_wait(vd->vdev_child[i]);
+}
+
+/*
+ * This function stops all asynchronous trim I/O going to a vdev and all
+ * its children. Because trim zios occur outside of the normal transactional
+ * machinery, we can't rely on the DMU hooks to stop I/O to devices being
+ * removed or reconfigured. Therefore, all pool management tasks which
+ * change the vdev configuration need to stop trim I/Os explicitly.
+ * After this function returns, it is guaranteed that no trim zios will be
+ * executing on the vdev or any of its children until either of the
+ * trim locks is released.
+ */
+void
+vdev_trim_stop_wait(vdev_t *vd)
+{
+	ASSERT(MUTEX_HELD(&vd->vdev_spa->spa_man_trim_lock));
+	ASSERT(MUTEX_HELD(&vd->vdev_spa->spa_auto_trim_lock));
+	/*
+	 * First we mark all devices as requesting a trim stop. This starts
+	 * the vdev queue drain (via zio_trim_should_bypass) quickly, then
+	 * we actually wait for all trim zios to get destroyed and then we
+	 * unmark the stop condition so trim zios can configure once the
+	 * pool management operation is done.
+	 */
+	trim_stop_set(vd, B_TRUE);
+	trim_stop_wait(vd);
+	trim_stop_set(vd, B_FALSE);
+}
+
 #if defined(_KERNEL) && defined(HAVE_SPL)
 EXPORT_SYMBOL(vdev_fault);
 EXPORT_SYMBOL(vdev_degrade);
@@ -3711,5 +3991,9 @@
 MODULE_PARM_DESC(metaslabs_per_vdev,
 	"Divide added vdev into approximately (but no more than) this number "
 	"of metaslabs");
+
+module_param(zfs_trim_mem_lim_fact, int, 0644);
+MODULE_PARM_DESC(metaslabs_per_vdev, "Maximum percentage of physical memory "
+	"to be used for storing trim extents");
 /* END CSTYLED */
 #endif
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_disk.c zfs-kmod-9999/module/zfs/vdev_disk.c
--- zfs-kmod-9999.orig/module/zfs/vdev_disk.c	2017-05-06 11:03:37.161219011 +0200
+++ zfs-kmod-9999/module/zfs/vdev_disk.c	2017-05-06 11:04:34.212030759 +0200
@@ -24,6 +24,7 @@
  * Rewritten for Linux by Brian Behlendorf <behlendorf1@llnl.gov>.
  * LLNL-CODE-403049.
  * Copyright (c) 2012, 2015 by Delphix. All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc.  All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -34,6 +35,7 @@
 #include <sys/fs/zfs.h>
 #include <sys/zio.h>
 #include <sys/sunldi.h>
+#include <sys/dkioc_free_util.h>
 
 char *zfs_vdev_scheduler = VDEV_SCHEDULER;
 static void *zfs_vdev_holder = VDEV_HOLDER;
@@ -319,6 +321,9 @@
 	/* Clear the nowritecache bit, causes vdev_reopen() to try again. */
 	v->vdev_nowritecache = B_FALSE;
 
+	/* Set TRIM flag based on support reported by the underlying device. */
+	v->vdev_notrim = !blk_queue_discard(bdev_get_queue(vd->vd_bdev));
+
 	/* Inform the ZIO pipeline that we are non-rotational */
 	v->vdev_nonrot = blk_queue_nonrot(bdev_get_queue(vd->vd_bdev));
 
@@ -361,14 +366,13 @@
 
 	dr = kmem_zalloc(sizeof (dio_request_t) +
 	    sizeof (struct bio *) * bio_count, KM_SLEEP);
-	if (dr) {
-		atomic_set(&dr->dr_ref, 0);
-		dr->dr_bio_count = bio_count;
-		dr->dr_error = 0;
 
-		for (i = 0; i < dr->dr_bio_count; i++)
-			dr->dr_bio[i] = NULL;
-	}
+	atomic_set(&dr->dr_ref, 0);
+	dr->dr_bio_count = bio_count;
+	dr->dr_error = 0;
+
+	for (i = 0; i < dr->dr_bio_count; i++)
+		dr->dr_bio[i] = NULL;
 
 	return (dr);
 }
@@ -420,6 +424,23 @@
 	return (rc);
 }
 
+#if defined(HAVE_BLK_QUEUE_HAVE_BLK_PLUG)
+static void
+vdev_disk_dio_blk_start_plug(dio_request_t *dr, struct blk_plug *plug)
+{
+	blk_start_plug(plug);
+}
+
+static void
+vdev_disk_dio_blk_finish_plug(dio_request_t *dr, struct blk_plug *plug)
+{
+	blk_finish_plug(plug);
+}
+#else
+#define	vdev_disk_dio_blk_start_plug(dr, plug)	((void)0)
+#define	vdev_disk_dio_blk_finish_plug(dr, plug)	((void)0)
+#endif /* HAVE_BLK_QUEUE_HAVE_BLK_PLUG */
+
 BIO_END_IO_PROTO(vdev_disk_physio_completion, bio, error)
 {
 	dio_request_t *dr = bio->bi_private;
@@ -522,7 +543,7 @@
 	uint64_t abd_offset;
 	uint64_t bio_offset;
 	int bio_size, bio_count = 16;
-	int i = 0, error = 0;
+	int i = 0, error = 0, should_plug = 0;
 #if defined(HAVE_BLK_QUEUE_HAVE_BLK_PLUG)
 	struct blk_plug plug;
 #endif
@@ -532,14 +553,11 @@
 
 retry:
 	dr = vdev_disk_dio_alloc(bio_count);
-	if (dr == NULL)
-		return (ENOMEM);
+	dr->dr_zio = zio;
 
 	if (zio && !(zio->io_flags & (ZIO_FLAG_IO_RETRY | ZIO_FLAG_TRYHARD)))
 		bio_set_flags_failfast(bdev, &flags);
 
-	dr->dr_zio = zio;
-
 	/*
 	 * When the IO size exceeds the maximum bio size for the request
 	 * queue we are forced to break the IO in multiple bio's and wait
@@ -557,6 +575,10 @@
 		if (bio_size <= 0)
 			break;
 
+		/* Plug the device when submitting multiple bio */
+		if (!should_plug && i >= 1)
+			should_plug = 1;
+
 		/*
 		 * By default only 'bio_count' bio's per dio are allowed.
 		 * However, if we find ourselves in a situation where more
@@ -598,20 +620,16 @@
 	/* Extra reference to protect dio_request during vdev_submit_bio */
 	vdev_disk_dio_get(dr);
 
-#if defined(HAVE_BLK_QUEUE_HAVE_BLK_PLUG)
-	if (dr->dr_bio_count > 1)
-		blk_start_plug(&plug);
-#endif
+	if (should_plug)
+		vdev_disk_dio_blk_start_plug(dr, &plug);
 
 	/* Submit all bio's associated with this dio */
 	for (i = 0; i < dr->dr_bio_count; i++)
 		if (dr->dr_bio[i])
 			vdev_submit_bio(dr->dr_bio[i]);
 
-#if defined(HAVE_BLK_QUEUE_HAVE_BLK_PLUG)
-	if (dr->dr_bio_count > 1)
-		blk_finish_plug(&plug);
-#endif
+	if (should_plug)
+		vdev_disk_dio_blk_finish_plug(dr, &plug);
 
 	(void) vdev_disk_dio_put(dr);
 
@@ -661,6 +679,148 @@
 	return (0);
 }
 
+static int
+vdev_disk_io_discard_sync(struct block_device *bdev, zio_t *zio)
+{
+	dkioc_free_list_t *dfl = zio->io_dfl;
+
+	zio->io_dfl_stats = kmem_zalloc(sizeof (vdev_stat_trim_t), KM_SLEEP);
+
+	for (int i = 0; i < dfl->dfl_num_exts; i++) {
+		int error;
+
+		if (dfl->dfl_exts[i].dfle_length == 0)
+			continue;
+
+		error = -blkdev_issue_discard(bdev,
+		    (dfl->dfl_exts[i].dfle_start + dfl->dfl_offset) >> 9,
+		    dfl->dfl_exts[i].dfle_length >> 9, GFP_NOFS, 0);
+		if (error != 0) {
+			return (SET_ERROR(error));
+		} else {
+			vdev_trim_stat_update(zio,
+			    dfl->dfl_exts[i].dfle_length, TRIM_STAT_ALL);
+		}
+	}
+
+	return (0);
+}
+
+BIO_END_IO_PROTO(vdev_disk_io_discard_completion, bio, error)
+{
+	dio_request_t *dr = bio->bi_private;
+	zio_t *zio = dr->dr_zio;
+
+	if (dr->dr_error == 0) {
+#ifdef HAVE_1ARG_BIO_END_IO_T
+		dr->dr_error = -(bio->bi_error);
+#else
+		dr->dr_error = -(error);
+#endif
+	}
+
+	/*
+	 * Only the latency is updated at completion.  The ops and request
+	 * size must be update when submitted since the size is no longer
+	 * available as part of the bio.
+	 */
+	vdev_trim_stat_update(zio, 0, TRIM_STAT_L_HISTO);
+
+	/* Drop reference acquired by vdev_disk_io_discard() */
+	(void) vdev_disk_dio_put(dr);
+}
+
+/*
+ * zio->io_dfl contains a dkioc_free_list_t specifying which offsets are to
+ * be freed.  Individual bio requests are constructed for each discard and
+ * submitted to the block layer to be handled asynchronously.  Any range
+ * with a length of zero or a length larger than UINT_MAX are ignored.
+ */
+static int
+vdev_disk_io_discard(struct block_device *bdev, zio_t *zio)
+{
+	dio_request_t *dr;
+	dkioc_free_list_t *dfl = zio->io_dfl;
+	unsigned int max_discard_sectors;
+	unsigned int alignment, granularity;
+	struct request_queue *q;
+#if defined(HAVE_BLK_QUEUE_HAVE_BLK_PLUG)
+	struct blk_plug plug;
+#endif
+
+	q = bdev_get_queue(bdev);
+	if (!q)
+		return (SET_ERROR(ENXIO));
+
+	if (!blk_queue_discard(q))
+		return (SET_ERROR(ENOTSUP));
+
+	zio->io_dfl_stats = kmem_zalloc(sizeof (vdev_stat_trim_t), KM_SLEEP);
+	dr = vdev_disk_dio_alloc(0);
+	dr->dr_zio = zio;
+
+	granularity = MAX(q->limits.discard_granularity >> 9, 1U);
+	alignment = (bdev_discard_alignment(bdev) >> 9) % granularity;
+
+	max_discard_sectors = MIN(q->limits.max_discard_sectors, UINT_MAX >> 9);
+	max_discard_sectors -= max_discard_sectors % granularity;
+
+	/* Extra reference to protect dio_request during vdev_submit_bio */
+	vdev_disk_dio_get(dr);
+	vdev_disk_dio_blk_start_plug(dr, &plug);
+
+	for (int i = 0; i < dfl->dfl_num_exts; i++) {
+		uint64_t nr_sectors = dfl->dfl_exts[i].dfle_length >> 9;
+		uint64_t sector = (dfl->dfl_exts[i].dfle_start +
+		    dfl->dfl_offset) >> 9;
+		struct bio *bio;
+		unsigned int request_sectors;
+		sector_t end_sector;
+
+		while (nr_sectors > 0) {
+			bio = bio_alloc(GFP_NOIO, 1);
+			if (unlikely(bio == NULL))
+				break;
+
+			request_sectors = min_t(sector_t, nr_sectors,
+			    max_discard_sectors);
+
+			/* When splitting requests align the end of each. */
+			end_sector = sector + request_sectors;
+			if (request_sectors < nr_sectors &&
+			    (end_sector % granularity) != alignment) {
+				end_sector = ((end_sector - alignment) /
+				    granularity) * granularity + alignment;
+				request_sectors = end_sector - sector;
+			}
+
+			bio->bi_bdev = bdev;
+			bio->bi_end_io = vdev_disk_io_discard_completion;
+			bio->bi_private = dr;
+			bio_set_discard(bio);
+			BIO_BI_SECTOR(bio) = sector;
+			BIO_BI_SIZE(bio) = request_sectors << 9;
+
+			nr_sectors -= request_sectors;
+			sector = end_sector;
+
+			vdev_trim_stat_update(zio, BIO_BI_SIZE(bio),
+			    TRIM_STAT_OP | TRIM_STAT_RQ_HISTO);
+
+			/* Matching put in vdev_disk_discard_completion */
+			vdev_disk_dio_get(dr);
+			vdev_submit_bio(bio);
+
+			cond_resched();
+		}
+	}
+
+	vdev_disk_dio_blk_finish_plug(dr, &plug);
+	(void) vdev_disk_dio_put(dr);
+
+	return (0);
+}
+
 static void
 vdev_disk_io_start(zio_t *zio)
 {
@@ -696,6 +856,37 @@
 
 			break;
 
+		case DKIOCFREE:
+
+			if (!zfs_trim)
+				break;
+
+			/*
+			 * We perform device support checks here instead of
+			 * in zio_trim_*(), as zio_trim_*() might be invoked
+			 * on a top-level vdev, whereas vdev_disk_io_start
+			 * is guaranteed to be operating a leaf disk vdev.
+			 */
+			if (v->vdev_notrim &&
+			    spa_get_force_trim(v->vdev_spa) !=
+			    SPA_FORCE_TRIM_ON) {
+				zio->io_error = SET_ERROR(ENOTSUP);
+				break;
+			}
+
+			if (zfs_trim_sync) {
+				error = vdev_disk_io_discard_sync(vd->vd_bdev,
+				    zio);
+			} else {
+				error = vdev_disk_io_discard(vd->vd_bdev, zio);
+				if (error == 0)
+					return;
+			}
+
+			zio->io_error = error;
+
+			break;
+
 		default:
 			zio->io_error = SET_ERROR(ENOTSUP);
 		}
@@ -790,16 +981,17 @@
 }
 
 vdev_ops_t vdev_disk_ops = {
-	vdev_disk_open,
-	vdev_disk_close,
-	vdev_default_asize,
-	vdev_disk_io_start,
-	vdev_disk_io_done,
-	NULL,
-	vdev_disk_hold,
-	vdev_disk_rele,
-	VDEV_TYPE_DISK,		/* name of this vdev type */
-	B_TRUE			/* leaf vdev */
+	.vdev_op_open =		vdev_disk_open,
+	.vdev_op_close =	vdev_disk_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_disk_io_start,
+	.vdev_op_io_done =	vdev_disk_io_done,
+	.vdev_op_state_change =	NULL,
+	.vdev_op_hold =		vdev_disk_hold,
+	.vdev_op_rele =		vdev_disk_rele,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_DISK,	/* name of this vdev type */
+	.vdev_op_leaf =		B_TRUE		/* leaf vdev */
 };
 
 module_param(zfs_vdev_scheduler, charp, 0644);
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_file.c zfs-kmod-9999/module/zfs/vdev_file.c
--- zfs-kmod-9999.orig/module/zfs/vdev_file.c	2017-05-06 11:03:37.162219008 +0200
+++ zfs-kmod-9999/module/zfs/vdev_file.c	2017-05-06 11:04:34.210030766 +0200
@@ -21,6 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2015 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -32,6 +33,9 @@
 #include <sys/fs/zfs.h>
 #include <sys/fm/fs/zfs.h>
 #include <sys/abd.h>
+#include <sys/fcntl.h>
+#include <sys/vnode.h>
+#include <sys/dkioc_free_util.h>
 
 /*
  * Virtual device vector for files.
@@ -223,6 +227,44 @@
 			zio->io_error = VOP_FSYNC(vf->vf_vnode, FSYNC | FDSYNC,
 			    kcred, NULL);
 			break;
+
+		case DKIOCFREE:
+		{
+			const dkioc_free_list_t *dfl = zio->io_dfl;
+
+			ASSERT(dfl != NULL);
+			if (!zfs_trim)
+				break;
+
+			zio->io_dfl_stats = kmem_zalloc(
+			    sizeof (vdev_stat_trim_t), KM_SLEEP);
+
+			for (int i = 0; i < dfl->dfl_num_exts; i++) {
+				struct flock flck;
+				int error;
+
+				if (dfl->dfl_exts[i].dfle_length == 0)
+					continue;
+
+				bzero(&flck, sizeof (flck));
+				flck.l_type = F_FREESP;
+				flck.l_start = dfl->dfl_exts[i].dfle_start +
+				    dfl->dfl_offset;
+				flck.l_len = dfl->dfl_exts[i].dfle_length;
+				flck.l_whence = 0;
+
+				error = VOP_SPACE(vf->vf_vnode,
+				    F_FREESP, &flck, 0, 0, kcred, NULL);
+				if (error != 0) {
+					zio->io_error = SET_ERROR(error);
+					break;
+				} else {
+					vdev_trim_stat_update(zio, flck.l_len,
+					    TRIM_STAT_ALL);
+				}
+			}
+			break;
+		}
 		default:
 			zio->io_error = SET_ERROR(ENOTSUP);
 		}
@@ -244,16 +286,17 @@
 }
 
 vdev_ops_t vdev_file_ops = {
-	vdev_file_open,
-	vdev_file_close,
-	vdev_default_asize,
-	vdev_file_io_start,
-	vdev_file_io_done,
-	NULL,
-	vdev_file_hold,
-	vdev_file_rele,
-	VDEV_TYPE_FILE,		/* name of this vdev type */
-	B_TRUE			/* leaf vdev */
+	.vdev_op_open =		vdev_file_open,
+	.vdev_op_close =	vdev_file_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_file_io_start,
+	.vdev_op_io_done =	vdev_file_io_done,
+	.vdev_op_state_change =	NULL,
+	.vdev_op_hold =		vdev_file_hold,
+	.vdev_op_rele =		vdev_file_rele,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_FILE,	/* name of this vdev type */
+	.vdev_op_leaf =		B_TRUE		/* leaf vdev */
 };
 
 void
@@ -277,16 +320,17 @@
 #ifndef _KERNEL
 
 vdev_ops_t vdev_disk_ops = {
-	vdev_file_open,
-	vdev_file_close,
-	vdev_default_asize,
-	vdev_file_io_start,
-	vdev_file_io_done,
-	NULL,
-	vdev_file_hold,
-	vdev_file_rele,
-	VDEV_TYPE_DISK,		/* name of this vdev type */
-	B_TRUE			/* leaf vdev */
+	.vdev_op_open =		vdev_file_open,
+	.vdev_op_close =	vdev_file_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_file_io_start,
+	.vdev_op_io_done =	vdev_file_io_done,
+	.vdev_op_state_change =	NULL,
+	.vdev_op_hold =		vdev_file_hold,
+	.vdev_op_rele =		vdev_file_rele,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_DISK,	/* name of this vdev type */
+	.vdev_op_leaf =		B_TRUE		/* leaf vdev */
 };
 
 #endif
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_label.c zfs-kmod-9999/module/zfs/vdev_label.c
--- zfs-kmod-9999.orig/module/zfs/vdev_label.c	2017-05-06 11:03:37.162219008 +0200
+++ zfs-kmod-9999/module/zfs/vdev_label.c	2017-05-06 11:04:34.211030763 +0200
@@ -22,6 +22,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2012, 2015 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 /*
@@ -250,6 +251,12 @@
 	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_SCRUB_ACTIVE_QUEUE,
 	    vsx->vsx_active_queue[ZIO_PRIORITY_SCRUB]);
 
+	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_AUTO_TRIM_ACTIVE_QUEUE,
+	    vsx->vsx_active_queue[ZIO_PRIORITY_AUTO_TRIM]);
+
+	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_MAN_TRIM_ACTIVE_QUEUE,
+	    vsx->vsx_active_queue[ZIO_PRIORITY_MAN_TRIM]);
+
 	/* ZIOs pending */
 	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_SYNC_R_PEND_QUEUE,
 	    vsx->vsx_pend_queue[ZIO_PRIORITY_SYNC_READ]);
@@ -266,6 +273,12 @@
 	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_SCRUB_PEND_QUEUE,
 	    vsx->vsx_pend_queue[ZIO_PRIORITY_SCRUB]);
 
+	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_AUTO_TRIM_PEND_QUEUE,
+	    vsx->vsx_pend_queue[ZIO_PRIORITY_AUTO_TRIM]);
+
+	fnvlist_add_uint64(nvx, ZPOOL_CONFIG_VDEV_MAN_TRIM_PEND_QUEUE,
+	    vsx->vsx_pend_queue[ZIO_PRIORITY_MAN_TRIM]);
+
 	/* Histograms */
 	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_TOT_R_LAT_HISTO,
 	    vsx->vsx_total_histo[ZIO_TYPE_READ],
@@ -303,6 +316,14 @@
 	    vsx->vsx_queue_histo[ZIO_PRIORITY_SCRUB],
 	    ARRAY_SIZE(vsx->vsx_queue_histo[ZIO_PRIORITY_SCRUB]));
 
+	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_AUTO_TRIM_LAT_HISTO,
+	    vsx->vsx_queue_histo[ZIO_PRIORITY_AUTO_TRIM],
+	    ARRAY_SIZE(vsx->vsx_queue_histo[ZIO_PRIORITY_AUTO_TRIM]));
+
+	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_MAN_TRIM_LAT_HISTO,
+	    vsx->vsx_queue_histo[ZIO_PRIORITY_MAN_TRIM],
+	    ARRAY_SIZE(vsx->vsx_queue_histo[ZIO_PRIORITY_MAN_TRIM]));
+
 	/* Request sizes */
 	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_SYNC_IND_R_HISTO,
 	    vsx->vsx_ind_histo[ZIO_PRIORITY_SYNC_READ],
@@ -324,6 +345,14 @@
 	    vsx->vsx_ind_histo[ZIO_PRIORITY_SCRUB],
 	    ARRAY_SIZE(vsx->vsx_ind_histo[ZIO_PRIORITY_SCRUB]));
 
+	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_IND_AUTO_TRIM_HISTO,
+	    vsx->vsx_ind_histo[ZIO_PRIORITY_AUTO_TRIM],
+	    ARRAY_SIZE(vsx->vsx_ind_histo[ZIO_PRIORITY_AUTO_TRIM]));
+
+	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_IND_MAN_TRIM_HISTO,
+	    vsx->vsx_ind_histo[ZIO_PRIORITY_MAN_TRIM],
+	    ARRAY_SIZE(vsx->vsx_ind_histo[ZIO_PRIORITY_MAN_TRIM]));
+
 	fnvlist_add_uint64_array(nvx, ZPOOL_CONFIG_VDEV_SYNC_AGG_R_HISTO,
 	    vsx->vsx_agg_histo[ZIO_PRIORITY_SYNC_READ],
 	    ARRAY_SIZE(vsx->vsx_agg_histo[ZIO_PRIORITY_SYNC_READ]));
@@ -536,6 +565,12 @@
 			fnvlist_add_uint64(nv, ZPOOL_CONFIG_ORIG_GUID,
 			    vd->vdev_orig_guid);
 		}
+
+		/* grab per-leaf-vdev trim stats */
+		if (getstats) {
+			fnvlist_add_uint64(nv, ZPOOL_CONFIG_TRIM_PROG,
+			    vd->vdev_trim_prog);
+		}
 	}
 
 	return (nv);
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_mirror.c zfs-kmod-9999/module/zfs/vdev_mirror.c
--- zfs-kmod-9999.orig/module/zfs/vdev_mirror.c	2017-05-06 11:03:37.163219005 +0200
+++ zfs-kmod-9999/module/zfs/vdev_mirror.c	2017-05-06 11:04:34.191030829 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2012, 2015 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -492,6 +493,9 @@
 	int good_copies = 0;
 	int unexpected_errors = 0;
 
+	if (ZIO_IS_TRIM(zio))
+		return;
+
 	for (c = 0; c < mm->mm_children; c++) {
 		mc = &mm->mm_child[c];
 
@@ -607,42 +611,45 @@
 }
 
 vdev_ops_t vdev_mirror_ops = {
-	vdev_mirror_open,
-	vdev_mirror_close,
-	vdev_default_asize,
-	vdev_mirror_io_start,
-	vdev_mirror_io_done,
-	vdev_mirror_state_change,
-	NULL,
-	NULL,
-	VDEV_TYPE_MIRROR,	/* name of this vdev type */
-	B_FALSE			/* not a leaf vdev */
+	.vdev_op_open =		vdev_mirror_open,
+	.vdev_op_close =	vdev_mirror_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_mirror_io_start,
+	.vdev_op_io_done =	vdev_mirror_io_done,
+	.vdev_op_state_change =	vdev_mirror_state_change,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_MIRROR, /* name of this vdev type */
+	.vdev_op_leaf =		B_FALSE		/* not a leaf vdev */
 };
 
 vdev_ops_t vdev_replacing_ops = {
-	vdev_mirror_open,
-	vdev_mirror_close,
-	vdev_default_asize,
-	vdev_mirror_io_start,
-	vdev_mirror_io_done,
-	vdev_mirror_state_change,
-	NULL,
-	NULL,
-	VDEV_TYPE_REPLACING,	/* name of this vdev type */
-	B_FALSE			/* not a leaf vdev */
+	.vdev_op_open =		vdev_mirror_open,
+	.vdev_op_close =	vdev_mirror_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_mirror_io_start,
+	.vdev_op_io_done =	vdev_mirror_io_done,
+	.vdev_op_state_change =	vdev_mirror_state_change,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_REPLACING, /* name of this vd type */
+	.vdev_op_leaf =		B_FALSE		/* not a leaf vdev */
 };
 
 vdev_ops_t vdev_spare_ops = {
-	vdev_mirror_open,
-	vdev_mirror_close,
-	vdev_default_asize,
-	vdev_mirror_io_start,
-	vdev_mirror_io_done,
-	vdev_mirror_state_change,
-	NULL,
-	NULL,
-	VDEV_TYPE_SPARE,	/* name of this vdev type */
-	B_FALSE			/* not a leaf vdev */
+	.vdev_op_open =		vdev_mirror_open,
+	.vdev_op_close =	vdev_mirror_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_mirror_io_start,
+	.vdev_op_io_done =	vdev_mirror_io_done,
+	.vdev_op_state_change =	vdev_mirror_state_change,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_SPARE, /* name of this vdev type */
+	.vdev_op_leaf =		B_FALSE		/* not a leaf vdev */
 };
 
 #if defined(_KERNEL) && defined(HAVE_SPL)
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_missing.c zfs-kmod-9999/module/zfs/vdev_missing.c
--- zfs-kmod-9999.orig/module/zfs/vdev_missing.c	2017-05-06 11:03:37.163219005 +0200
+++ zfs-kmod-9999/module/zfs/vdev_missing.c	2017-05-06 11:04:34.191030829 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2012, 2014 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 /*
@@ -80,27 +81,29 @@
 }
 
 vdev_ops_t vdev_missing_ops = {
-	vdev_missing_open,
-	vdev_missing_close,
-	vdev_default_asize,
-	vdev_missing_io_start,
-	vdev_missing_io_done,
-	NULL,
-	NULL,
-	NULL,
-	VDEV_TYPE_MISSING,	/* name of this vdev type */
-	B_TRUE			/* leaf vdev */
+	.vdev_op_open =		vdev_missing_open,
+	.vdev_op_close =	vdev_missing_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_missing_io_start,
+	.vdev_op_io_done =	vdev_missing_io_done,
+	.vdev_op_state_change =	NULL,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_MISSING, /* name of this vdev type */
+	.vdev_op_leaf =		B_TRUE		/* leaf vdev */
 };
 
 vdev_ops_t vdev_hole_ops = {
-	vdev_missing_open,
-	vdev_missing_close,
-	vdev_default_asize,
-	vdev_missing_io_start,
-	vdev_missing_io_done,
-	NULL,
-	NULL,
-	NULL,
-	VDEV_TYPE_HOLE,		/* name of this vdev type */
-	B_TRUE			/* leaf vdev */
+	.vdev_op_open =		vdev_missing_open,
+	.vdev_op_close =	vdev_missing_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	vdev_missing_io_start,
+	.vdev_op_io_done =	vdev_missing_io_done,
+	.vdev_op_state_change =	NULL,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		NULL,
+	.vdev_op_type =		VDEV_TYPE_HOLE,	/* name of this vdev type */
+	.vdev_op_leaf =		B_TRUE		/* leaf vdev */
 };
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_queue.c zfs-kmod-9999/module/zfs/vdev_queue.c
--- zfs-kmod-9999.orig/module/zfs/vdev_queue.c	2017-05-06 11:03:37.164219001 +0200
+++ zfs-kmod-9999/module/zfs/vdev_queue.c	2017-05-06 11:04:34.192030825 +0200
@@ -152,6 +152,8 @@
 uint32_t zfs_vdev_async_write_max_active = 10;
 uint32_t zfs_vdev_scrub_min_active = 1;
 uint32_t zfs_vdev_scrub_max_active = 2;
+uint32_t zfs_vdev_trim_min_active = 1;
+uint32_t zfs_vdev_trim_max_active = 10;
 
 /*
  * When the pool has less than zfs_vdev_async_write_active_min_dirty_percent
@@ -213,11 +215,14 @@
 static inline avl_tree_t *
 vdev_queue_type_tree(vdev_queue_t *vq, zio_type_t t)
 {
-	ASSERT(t == ZIO_TYPE_READ || t == ZIO_TYPE_WRITE);
+	ASSERT(t == ZIO_TYPE_READ || t == ZIO_TYPE_WRITE ||
+	    t == ZIO_TYPE_IOCTL);
 	if (t == ZIO_TYPE_READ)
 		return (&vq->vq_read_offset_tree);
-	else
+	else if (t == ZIO_TYPE_WRITE)
 		return (&vq->vq_write_offset_tree);
+	else
+		return (NULL);
 }
 
 int
@@ -248,6 +253,9 @@
 		return (zfs_vdev_async_write_min_active);
 	case ZIO_PRIORITY_SCRUB:
 		return (zfs_vdev_scrub_min_active);
+	case ZIO_PRIORITY_AUTO_TRIM:
+	case ZIO_PRIORITY_MAN_TRIM:
+		return (zfs_vdev_trim_min_active);
 	default:
 		panic("invalid priority %u", p);
 		return (0);
@@ -316,6 +324,9 @@
 		return (vdev_queue_max_async_writes(spa));
 	case ZIO_PRIORITY_SCRUB:
 		return (zfs_vdev_scrub_max_active);
+	case ZIO_PRIORITY_AUTO_TRIM:
+	case ZIO_PRIORITY_MAN_TRIM:
+		return (zfs_vdev_trim_max_active);
 	default:
 		panic("invalid priority %u", p);
 		return (0);
@@ -384,8 +395,12 @@
 		 * The synchronous i/o queues are dispatched in FIFO rather
 		 * than LBA order. This provides more consistent latency for
 		 * these i/os.
+		 * The same is true of the TRIM queue, where LBA ordering
+		 * doesn't help.
 		 */
-		if (p == ZIO_PRIORITY_SYNC_READ || p == ZIO_PRIORITY_SYNC_WRITE)
+		if (p == ZIO_PRIORITY_SYNC_READ ||
+		    p == ZIO_PRIORITY_SYNC_WRITE ||
+		    p == ZIO_PRIORITY_AUTO_TRIM || p == ZIO_PRIORITY_MAN_TRIM)
 			compfn = vdev_queue_timestamp_compare;
 		else
 			compfn = vdev_queue_offset_compare;
@@ -415,11 +430,14 @@
 vdev_queue_io_add(vdev_queue_t *vq, zio_t *zio)
 {
 	spa_t *spa = zio->io_spa;
+	avl_tree_t *qtt;
 	spa_stats_history_t *ssh = &spa->spa_stats.io_history;
 
 	ASSERT3U(zio->io_priority, <, ZIO_PRIORITY_NUM_QUEUEABLE);
 	avl_add(vdev_queue_class_tree(vq, zio->io_priority), zio);
-	avl_add(vdev_queue_type_tree(vq, zio->io_type), zio);
+	qtt = vdev_queue_type_tree(vq, zio->io_type);
+	if (qtt != NULL)
+		avl_add(qtt, zio);
 
 	if (ssh->kstat != NULL) {
 		mutex_enter(&ssh->lock);
@@ -432,11 +450,14 @@
 vdev_queue_io_remove(vdev_queue_t *vq, zio_t *zio)
 {
 	spa_t *spa = zio->io_spa;
+	avl_tree_t *qtt;
 	spa_stats_history_t *ssh = &spa->spa_stats.io_history;
 
 	ASSERT3U(zio->io_priority, <, ZIO_PRIORITY_NUM_QUEUEABLE);
 	avl_remove(vdev_queue_class_tree(vq, zio->io_priority), zio);
-	avl_remove(vdev_queue_type_tree(vq, zio->io_type), zio);
+	qtt = vdev_queue_type_tree(vq, zio->io_type);
+	if (qtt != NULL)
+		avl_remove(qtt, zio);
 
 	if (ssh->kstat != NULL) {
 		mutex_enter(&ssh->lock);
@@ -692,7 +713,7 @@
 	 * For LBA-ordered queues (async / scrub), issue the i/o which follows
 	 * the most recently issued i/o in LBA (offset) order.
 	 *
-	 * For FIFO queues (sync), issue the i/o with the lowest timestamp.
+	 * For FIFO queues (sync/trim), issue the i/o with the lowest timestamp.
 	 */
 	tree = vdev_queue_class_tree(vq, p);
 	vq->vq_io_search.io_timestamp = 0;
@@ -725,7 +746,10 @@
 	}
 
 	vdev_queue_pending_add(vq, zio);
-	vq->vq_last_offset = zio->io_offset;
+	/* trim I/Os have no single meaningful offset */
+	if (zio->io_priority != ZIO_PRIORITY_AUTO_TRIM ||
+	    zio->io_priority != ZIO_PRIORITY_MAN_TRIM)
+		vq->vq_last_offset = zio->io_offset;
 
 	return (zio);
 }
@@ -748,11 +772,12 @@
 		    zio->io_priority != ZIO_PRIORITY_ASYNC_READ &&
 		    zio->io_priority != ZIO_PRIORITY_SCRUB)
 			zio->io_priority = ZIO_PRIORITY_ASYNC_READ;
-	} else {
-		ASSERT(zio->io_type == ZIO_TYPE_WRITE);
+	} else if (zio->io_type == ZIO_TYPE_WRITE) {
 		if (zio->io_priority != ZIO_PRIORITY_SYNC_WRITE &&
 		    zio->io_priority != ZIO_PRIORITY_ASYNC_WRITE)
 			zio->io_priority = ZIO_PRIORITY_ASYNC_WRITE;
+	} else {
+		ASSERT(ZIO_IS_TRIM(zio));
 	}
 
 	zio->io_flags |= ZIO_FLAG_DONT_CACHE | ZIO_FLAG_DONT_QUEUE;
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_raidz.c zfs-kmod-9999/module/zfs/vdev_raidz.c
--- zfs-kmod-9999.orig/module/zfs/vdev_raidz.c	2017-05-06 11:03:37.165218998 +0200
+++ zfs-kmod-9999/module/zfs/vdev_raidz.c	2017-05-06 11:04:34.192030825 +0200
@@ -23,6 +23,7 @@
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2012, 2014 by Delphix. All rights reserved.
  * Copyright (c) 2016 Gvozden Nekovi. All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -35,6 +36,7 @@
 #include <sys/fm/fs/zfs.h>
 #include <sys/vdev_raidz.h>
 #include <sys/vdev_raidz_impl.h>
+#include <sys/dkioc_free_util.h>
 
 /*
  * Virtual device vector for RAID-Z.
@@ -135,6 +137,10 @@
 {
 	int c;
 
+	/* raidz_map_t without abd allocation from vdev_raidz_trim() */
+	if (rm->rm_col[0].rc_abd == NULL)
+		goto out;
+
 	for (c = 0; c < rm->rm_firstdatacol; c++) {
 		abd_free(rm->rm_col[c].rc_abd);
 
@@ -148,6 +154,7 @@
 	if (rm->rm_abd_copy != NULL)
 		abd_free(rm->rm_abd_copy);
 
+out:
 	kmem_free(rm, offsetof(raidz_map_t, rm_col[rm->rm_scols]));
 }
 
@@ -426,18 +433,21 @@
 	ASSERT3U(rm->rm_asize - asize, ==, rm->rm_nskip << unit_shift);
 	ASSERT3U(rm->rm_nskip, <=, nparity);
 
-	for (c = 0; c < rm->rm_firstdatacol; c++)
-		rm->rm_col[c].rc_abd =
-		    abd_alloc_linear(rm->rm_col[c].rc_size, B_FALSE);
-
-	rm->rm_col[c].rc_abd = abd_get_offset_size(zio->io_abd, 0,
-	    rm->rm_col[c].rc_size);
-	off = rm->rm_col[c].rc_size;
+	if (zio->io_abd != NULL) {
+		for (c = 0; c < rm->rm_firstdatacol; c++)
+			rm->rm_col[c].rc_abd =
+			    abd_alloc_linear(rm->rm_col[c].rc_size, B_FALSE);
 
-	for (c = c + 1; c < acols; c++) {
-		rm->rm_col[c].rc_abd = abd_get_offset_size(zio->io_abd, off,
+		rm->rm_col[c].rc_abd = abd_get_offset_size(zio->io_abd, 0,
 		    rm->rm_col[c].rc_size);
-		off += rm->rm_col[c].rc_size;
+		off = rm->rm_col[c].rc_size;
+
+		for (c = c + 1; c < acols; c++) {
+			rm->rm_col[c].rc_abd =
+			    abd_get_offset_size(zio->io_abd, off,
+			    rm->rm_col[c].rc_size);
+			off += rm->rm_col[c].rc_size;
+		}
 	}
 
 	/*
@@ -1631,6 +1641,38 @@
 	return (asize);
 }
 
+/*
+ * Converts an allocated size on a raidz vdev back to a logical block
+ * size. This is used in trimming to figure out the appropriate logical
+ * size to pass to vdev_raidz_map_alloc when splitting up extents of free
+ * space obtained from metaslabs. However, a range of free space on a
+ * raidz vdev might have originally consisted of multiple blocks and
+ * those, taken together with their skip blocks, might not always align
+ * neatly to a new vdev_raidz_map_alloc covering the entire unified
+ * range. So to ensure that the newly allocated raidz map *always* fits
+ * within the asize passed to this function and never exceeds it (since
+ * that might trim allocated data past it), we round it down to the
+ * nearest suitable multiple of the vdev ashift (hence the "_floor" in
+ * this function's name).
+ */
+static uint64_t
+vdev_raidz_psize_floor(vdev_t *vd, uint64_t asize)
+{
+	uint64_t psize;
+	uint64_t ashift = vd->vdev_top->vdev_ashift;
+	uint64_t cols = vd->vdev_children;
+	uint64_t nparity = vd->vdev_nparity;
+
+	psize = (asize - (nparity << ashift));
+	psize /= cols;
+	psize *= cols - nparity;
+	psize += (1 << ashift) - 1;
+
+	psize = P2ALIGN(psize, 1 << ashift);
+
+	return (psize);
+}
+
 static void
 vdev_raidz_child_done(zio_t *zio)
 {
@@ -2041,6 +2083,9 @@
 	int tgts[VDEV_RAIDZ_MAXPARITY];
 	int code;
 
+	if (ZIO_IS_TRIM(zio))
+		return;
+
 	ASSERT(zio->io_bp != NULL);  /* XXX need to add code to enforce this */
 
 	ASSERT(rm->rm_missingparity <= rm->rm_firstdatacol);
@@ -2299,15 +2344,109 @@
 		vdev_set_state(vd, B_FALSE, VDEV_STATE_HEALTHY, VDEV_AUX_NONE);
 }
 
+static inline void
+vdev_raidz_trim_append_rc(dkioc_free_list_t *dfl, uint64_t *num_extsp,
+    const raidz_col_t *rc)
+{
+	uint64_t num_exts = *num_extsp;
+	ASSERT(rc->rc_size != 0);
+
+	if (dfl->dfl_num_exts > 0 &&
+	    dfl->dfl_exts[num_exts - 1].dfle_start +
+	    dfl->dfl_exts[num_exts - 1].dfle_length == rc->rc_offset) {
+		dfl->dfl_exts[num_exts - 1].dfle_length += rc->rc_size;
+	} else {
+		dfl->dfl_exts[num_exts].dfle_start = rc->rc_offset;
+		dfl->dfl_exts[num_exts].dfle_length = rc->rc_size;
+		(*num_extsp)++;
+	}
+}
+
+/*
+ * Processes a trim for a raidz vdev. Because trims deal with physical
+ * addresses, we can't simply pass through our logical vdev addresses to
+ * the underlying devices. Instead, we compute a raidz map based on the
+ * logical extent addresses provided to us and construct new extent
+ * lists that then go to each component vdev.
+ */
+static void
+vdev_raidz_trim(vdev_t *vd, zio_t *pio, dkioc_free_list_t *dfl,
+    boolean_t auto_trim)
+{
+	dkioc_free_list_t **sub_dfls;
+	uint64_t *sub_dfls_num_exts;
+	zio_t *zio;
+
+	sub_dfls = kmem_zalloc(sizeof (*sub_dfls) * vd->vdev_children,
+	    KM_SLEEP);
+	sub_dfls_num_exts = kmem_zalloc(sizeof (uint64_t) * vd->vdev_children,
+	    KM_SLEEP);
+	zio = kmem_zalloc(sizeof (*zio), KM_SLEEP);
+	for (int i = 0; i < vd->vdev_children; i++) {
+		/*
+		 * We might over-allocate here, because the sub-lists can never
+		 * be longer than the parent list, but they can be shorter.
+		 * The underlying driver will discard zero-length extents.
+		 */
+		sub_dfls[i] = dfl_alloc(dfl->dfl_num_exts, KM_SLEEP);
+		sub_dfls[i]->dfl_num_exts = dfl->dfl_num_exts;
+		sub_dfls[i]->dfl_flags = dfl->dfl_flags;
+		sub_dfls[i]->dfl_offset = dfl->dfl_offset;
+		/* don't copy the check func, because it isn't raidz-aware */
+	}
+
+	/*
+	 * Process all extents and redistribute them to the component vdevs
+	 * according to a computed raidz map geometry.
+	 */
+	for (int i = 0; i < dfl->dfl_num_exts; i++) {
+		uint64_t start = dfl->dfl_exts[i].dfle_start;
+		uint64_t length = dfl->dfl_exts[i].dfle_length;
+		uint64_t j;
+		raidz_map_t *rm;
+
+		zio->io_offset = start;
+		zio->io_size = vdev_raidz_psize_floor(vd, length);
+		zio->io_abd = NULL;
+
+		rm = vdev_raidz_map_alloc(zio, vd->vdev_top->vdev_ashift,
+		    vd->vdev_children, vd->vdev_nparity);
+
+		for (j = 0; j < rm->rm_cols; j++) {
+			uint64_t devidx = rm->rm_col[j].rc_devidx;
+			vdev_raidz_trim_append_rc(sub_dfls[devidx],
+			    &sub_dfls_num_exts[devidx], &rm->rm_col[j]);
+		}
+		vdev_raidz_map_free(rm);
+	}
+
+	/*
+	 * Issue the component ioctls as children of the parent zio.
+	 */
+	for (int i = 0; i < vd->vdev_children; i++) {
+		if (sub_dfls_num_exts[i] != 0) {
+			vdev_t *child = vd->vdev_child[i];
+			zio_nowait(zio_trim_dfl(pio, child->vdev_spa, child,
+			    sub_dfls[i], B_TRUE, auto_trim, NULL, NULL));
+		} else {
+			dfl_free(sub_dfls[i]);
+		}
+	}
+	kmem_free(sub_dfls, sizeof (*sub_dfls) * vd->vdev_children);
+	kmem_free(sub_dfls_num_exts, sizeof (uint64_t) * vd->vdev_children);
+	kmem_free(zio, sizeof (*zio));
+}
+
 vdev_ops_t vdev_raidz_ops = {
-	vdev_raidz_open,
-	vdev_raidz_close,
-	vdev_raidz_asize,
-	vdev_raidz_io_start,
-	vdev_raidz_io_done,
-	vdev_raidz_state_change,
-	NULL,
-	NULL,
-	VDEV_TYPE_RAIDZ,	/* name of this vdev type */
-	B_FALSE			/* not a leaf vdev */
+	.vdev_op_open =		vdev_raidz_open,
+	.vdev_op_close =	vdev_raidz_close,
+	.vdev_op_asize =	vdev_raidz_asize,
+	.vdev_op_io_start =	vdev_raidz_io_start,
+	.vdev_op_io_done =	vdev_raidz_io_done,
+	.vdev_op_state_change =	vdev_raidz_state_change,
+	.vdev_op_hold =		NULL,
+	.vdev_op_rele =		NULL,
+	.vdev_op_trim =		vdev_raidz_trim,
+	.vdev_op_type =		VDEV_TYPE_RAIDZ, /* name of this vdev type */
+	.vdev_op_leaf =		B_FALSE		/* not a leaf vdev */
 };
diff -Nuar zfs-kmod-9999.orig/module/zfs/vdev_root.c zfs-kmod-9999/module/zfs/vdev_root.c
--- zfs-kmod-9999.orig/module/zfs/vdev_root.c	2017-05-06 11:03:37.172218975 +0200
+++ zfs-kmod-9999/module/zfs/vdev_root.c	2017-05-06 11:04:34.192030825 +0200
@@ -25,6 +25,7 @@
 
 /*
  * Copyright (c) 2013 by Delphix. All rights reserved.
+ * Copyright 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/zfs_context.h>
@@ -112,14 +113,15 @@
 }
 
 vdev_ops_t vdev_root_ops = {
-	vdev_root_open,
-	vdev_root_close,
-	vdev_default_asize,
-	NULL,			/* io_start - not applicable to the root */
-	NULL,			/* io_done - not applicable to the root */
-	vdev_root_state_change,
-	NULL,
-	NULL,
-	VDEV_TYPE_ROOT,		/* name of this vdev type */
-	B_FALSE			/* not a leaf vdev */
+	.vdev_op_open =		vdev_root_open,
+	.vdev_op_close =	vdev_root_close,
+	.vdev_op_asize =	vdev_default_asize,
+	.vdev_op_io_start =	NULL,		/* not applicable to the root */
+	.vdev_op_io_done =	NULL,		/* not applicable to the root */
+	.vdev_op_state_change =	vdev_root_state_change,
+	.vdev_op_hold =		NULL,		/* not applicable to the root */
+	.vdev_op_rele =		NULL,		/* not applicable to the root */
+	.vdev_op_trim =		NULL,		/* not applicable to the root */
+	.vdev_op_type =		VDEV_TYPE_ROOT,	/* name of this vdev type */
+	.vdev_op_leaf =		B_FALSE		/* not a leaf vdev */
 };
diff -Nuar zfs-kmod-9999.orig/module/zfs/zfs_ioctl.c zfs-kmod-9999/module/zfs/zfs_ioctl.c
--- zfs-kmod-9999.orig/module/zfs/zfs_ioctl.c	2017-05-06 11:03:37.185218932 +0200
+++ zfs-kmod-9999/module/zfs/zfs_ioctl.c	2017-05-06 11:04:34.202030792 +0200
@@ -1690,6 +1690,36 @@
 	return (error);
 }
 
+/*
+ * inputs:
+ * zc_name              name of the pool
+ * zc_cookie            trim_cmd_info_t
+ */
+static int
+zfs_ioc_pool_trim(zfs_cmd_t *zc)
+{
+	spa_t *spa;
+	int error;
+	trim_cmd_info_t	tci;
+
+	if (ddi_copyin((void *)(uintptr_t)zc->zc_cookie, &tci,
+	    sizeof (tci), 0) == -1)
+		return (EFAULT);
+
+	if ((error = spa_open(zc->zc_name, &spa, FTAG)) != 0)
+		return (error);
+
+	if (tci.tci_start) {
+		spa_man_trim(spa, tci.tci_rate, tci.tci_fulltrim);
+	} else {
+		spa_man_trim_stop(spa);
+	}
+
+	spa_close(spa, FTAG);
+
+	return (error);
+}
+
 static int
 zfs_ioc_pool_freeze(zfs_cmd_t *zc)
 {
@@ -5868,6 +5898,8 @@
 	    zfs_secpolicy_config, B_TRUE, POOL_CHECK_NONE);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_SCAN,
 	    zfs_ioc_pool_scan);
+	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_TRIM,
+	    zfs_ioc_pool_trim);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_POOL_UPGRADE,
 	    zfs_ioc_pool_upgrade);
 	zfs_ioctl_register_pool_modify(ZFS_IOC_VDEV_ADD,
diff -Nuar zfs-kmod-9999.orig/module/zfs/zio.c zfs-kmod-9999/module/zfs/zio.c
--- zfs-kmod-9999.orig/module/zfs/zio.c	2017-05-06 11:03:37.193218906 +0200
+++ zfs-kmod-9999/module/zfs/zio.c	2017-05-06 11:04:34.213030756 +0200
@@ -21,7 +21,7 @@
 /*
  * Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2011, 2017 by Delphix. All rights reserved.
- * Copyright (c) 2011 Nexenta Systems, Inc. All rights reserved.
+ * Copyright (c) 2017 Nexenta Systems, Inc. All rights reserved.
  */
 
 #include <sys/sysmacros.h>
@@ -43,6 +43,8 @@
 #include <sys/time.h>
 #include <sys/trace_zio.h>
 #include <sys/abd.h>
+#include <sys/dkioc_free_util.h>
+#include <sys/metaslab_impl.h>
 
 /*
  * ==========================================================================
@@ -115,6 +117,15 @@
 
 static void zio_taskq_dispatch(zio_t *, zio_taskq_type_t, boolean_t);
 
+/*
+ * Tunable to allow for debugging SCSI UNMAP/SATA TRIM calls. Disabling
+ * it will prevent ZFS from attempting to issue DKIOCFREE ioctls to the
+ * underlying storage.
+ */
+int zfs_trim = B_TRUE;
+int zfs_trim_min_ext_sz = 128 << 10;	/* 128k */
+int zfs_trim_sync = B_TRUE;
+
 void
 zio_init(void)
 {
@@ -680,11 +691,27 @@
 static void
 zio_destroy(zio_t *zio)
 {
+	if (ZIO_IS_TRIM(zio)) {
+		vdev_t *vd = zio->io_vd;
+		ASSERT(vd != NULL);
+		ASSERT(!MUTEX_HELD(&vd->vdev_trim_zios_lock));
+		mutex_enter(&vd->vdev_trim_zios_lock);
+		ASSERT(vd->vdev_trim_zios != 0);
+		vd->vdev_trim_zios--;
+		cv_broadcast(&vd->vdev_trim_zios_cv);
+		mutex_exit(&vd->vdev_trim_zios_lock);
+	}
 	metaslab_trace_fini(&zio->io_alloc_list);
 	list_destroy(&zio->io_parent_list);
 	list_destroy(&zio->io_child_list);
 	mutex_destroy(&zio->io_lock);
 	cv_destroy(&zio->io_cv);
+	if (zio->io_dfl_stats != NULL)
+		kmem_free(zio->io_dfl_stats, sizeof (vdev_stat_trim_t));
+	if (zio->io_dfl != NULL && zio->io_dfl_free_on_destroy)
+		dfl_free(zio->io_dfl);
+	else
+		ASSERT0(zio->io_dfl_free_on_destroy);
 	kmem_cache_free(zio_cache, zio);
 }
 
@@ -1005,6 +1032,180 @@
 	return (zio);
 }
 
+/*
+ * Performs the same function as zio_trim_tree, but takes a dkioc_free_list_t
+ * instead of a range tree of extents. The `dfl' argument is stored in the
+ * zio and shouldn't be altered by the caller after calling zio_trim_dfl.
+ * If `dfl_free_on_destroy' is true, the zio will destroy and free the list
+ * using dfl_free after the zio is done executing.
+ */
+zio_t *
+zio_trim_dfl(zio_t *pio, spa_t *spa, vdev_t *vd, dkioc_free_list_t *dfl,
+    boolean_t dfl_free_on_destroy, boolean_t auto_trim,
+    zio_done_func_t *done, void *private)
+{
+	zio_t *zio;
+	int c;
+
+	ASSERT(dfl->dfl_num_exts != 0);
+
+	if (!vdev_writeable(vd)) {
+		/* Skip unavailable vdevs, just create a dummy zio. */
+		zio = zio_null(pio, spa, vd, done, private, 0);
+		zio->io_dfl = dfl;
+		zio->io_dfl_free_on_destroy = dfl_free_on_destroy;
+	} else if (vd->vdev_ops->vdev_op_leaf) {
+		/*
+		 * A trim zio is a special ioctl zio that can enter the vdev
+		 * queue. We don't want to be sorted in the queue by offset,
+		 * but sometimes the queue requires that, so we fake an
+		 * offset value. We simply use the offset of the first extent
+		 * and the minimum allocation unit on the vdev to keep the
+		 * queue's algorithms working more-or-less as they should.
+		 */
+		uint64_t off = dfl->dfl_exts[0].dfle_start;
+		uint64_t size = 1 << vd->vdev_top->vdev_ashift;
+
+		zio = zio_create(pio, spa, 0, NULL, NULL,
+		    size, size, done, private, ZIO_TYPE_IOCTL,
+		    auto_trim ? ZIO_PRIORITY_AUTO_TRIM : ZIO_PRIORITY_MAN_TRIM,
+		    ZIO_FLAG_CANFAIL | ZIO_FLAG_DONT_RETRY |
+		    ZIO_FLAG_DONT_PROPAGATE | ZIO_FLAG_DONT_AGGREGATE, vd, off,
+		    NULL, ZIO_STAGE_OPEN, ZIO_TRIM_PIPELINE);
+		zio->io_cmd = DKIOCFREE;
+		zio->io_dfl = dfl;
+		zio->io_dfl_free_on_destroy = dfl_free_on_destroy;
+
+		mutex_enter(&vd->vdev_trim_zios_lock);
+		vd->vdev_trim_zios++;
+		mutex_exit(&vd->vdev_trim_zios_lock);
+	} else {
+		/*
+		 * Trims to non-leaf vdevs have two possible paths. For vdevs
+		 * that do not provide a specific trim fanout handler, we
+		 * simply duplicate the trim to each child. vdevs which do
+		 * have a trim fanout handler are responsible for doing the
+		 * fanout themselves.
+		 */
+		zio = zio_null(pio, spa, vd, done, private, 0);
+		zio->io_dfl = dfl;
+		zio->io_dfl_free_on_destroy = dfl_free_on_destroy;
+
+		if (vd->vdev_ops->vdev_op_trim != NULL) {
+			vd->vdev_ops->vdev_op_trim(vd, zio, dfl, auto_trim);
+		} else {
+			for (c = 0; c < vd->vdev_children; c++) {
+				zio_nowait(zio_trim_dfl(zio, spa,
+				    vd->vdev_child[c], dfl, B_FALSE, auto_trim,
+				    NULL, NULL));
+			}
+		}
+	}
+
+	return (zio);
+}
+
+/*
+ * This check is used by zio_trim_tree to set in dfl_ck_func to help debugging
+ * extent trimming. If the SCSI driver (sd) was compiled with the DEBUG flag
+ * set, dfl_ck_func is called for every extent to verify that it is indeed
+ * ok to be trimmed. This function compares the extent address with the tree
+ * of free blocks (ms_tree) in the metaslab which this trim was originally
+ * part of.
+ */
+static void
+zio_trim_check(uint64_t start, uint64_t len, void *msp)
+{
+	metaslab_t *ms = msp;
+	boolean_t held = MUTEX_HELD(&ms->ms_lock);
+	if (!held)
+		mutex_enter(&ms->ms_lock);
+	ASSERT(ms->ms_trimming_ts != NULL);
+	if (ms->ms_loaded)
+		ASSERT(range_tree_contains(ms->ms_trimming_ts->ts_tree,
+		    start - VDEV_LABEL_START_SIZE, len));
+	if (!held)
+		mutex_exit(&ms->ms_lock);
+}
+
+/*
+ * Takes a bunch of freed extents and tells the underlying vdevs that the
+ * space associated with these extents can be released.
+ * This is used by flash storage to pre-erase blocks for rapid reuse later
+ * and thin-provisioned block storage to reclaim unused blocks.
+ * This function is actually a front-end to zio_trim_dfl. It simply converts
+ * the provided range_tree's contents into a dkioc_free_list_t and calls
+ * zio_trim_dfl with it. The `tree' argument is not used after this function
+ * returns and can be discarded by the caller.
+ */
+zio_t *
+zio_trim_tree(zio_t *pio, spa_t *spa, vdev_t *vd, struct range_tree *tree,
+    boolean_t auto_trim, zio_done_func_t *done, void *private,
+    int dkiocfree_flags, metaslab_t *msp)
+{
+	dkioc_free_list_t *dfl = NULL;
+	range_seg_t *rs;
+	uint64_t rs_idx;
+	uint64_t num_exts;
+	uint64_t bytes_issued = 0, bytes_skipped = 0, exts_skipped = 0;
+
+	ASSERT(range_tree_space(tree) != 0);
+
+	num_exts = avl_numnodes(&tree->rt_root);
+	dfl = dfl_alloc(num_exts, KM_SLEEP);
+	dfl->dfl_flags = dkiocfree_flags;
+	dfl->dfl_num_exts = num_exts;
+	dfl->dfl_offset = VDEV_LABEL_START_SIZE;
+	if (msp) {
+		dfl->dfl_ck_func = zio_trim_check;
+		dfl->dfl_ck_arg = msp;
+	}
+
+	for (rs = avl_first(&tree->rt_root), rs_idx = 0; rs != NULL;
+	    rs = AVL_NEXT(&tree->rt_root, rs)) {
+		uint64_t len = rs->rs_end - rs->rs_start;
+
+		/* Skip extents that are too short to bother with. */
+		if (len < zfs_trim_min_ext_sz) {
+			bytes_skipped += len;
+			exts_skipped++;
+			continue;
+		}
+
+		dfl->dfl_exts[rs_idx].dfle_start = rs->rs_start;
+		dfl->dfl_exts[rs_idx].dfle_length = len;
+
+		/* check we're a multiple of the vdev ashift */
+		ASSERT0(dfl->dfl_exts[rs_idx].dfle_start &
+		    ((1 << vd->vdev_ashift) - 1));
+		ASSERT0(dfl->dfl_exts[rs_idx].dfle_length &
+		    ((1 << vd->vdev_ashift) - 1));
+
+		rs_idx++;
+		bytes_issued += len;
+	}
+
+	spa_trimstats_update(spa, rs_idx, bytes_issued, exts_skipped,
+	    bytes_skipped);
+
+	/* the zfs_trim_min_ext_sz filter may have shortened the list */
+	if (dfl->dfl_num_exts != rs_idx) {
+		if (rs_idx == 0) {
+			/* Removing short extents has removed all extents. */
+			dfl_free(dfl);
+			return (zio_null(pio, spa, vd, done, private, 0));
+		}
+		dkioc_free_list_t *dfl2 = dfl_alloc(rs_idx, KM_SLEEP);
+		bcopy(dfl, dfl2, DFL_SZ(rs_idx));
+		dfl2->dfl_num_exts = rs_idx;
+		dfl_free(dfl);
+		dfl = dfl2;
+	}
+
+	return (zio_trim_dfl(pio, spa, vd, dfl, B_TRUE, auto_trim, done,
+	    private));
+}
+
 zio_t *
 zio_read_phys(zio_t *pio, vdev_t *vd, uint64_t offset, uint64_t size,
     abd_t *data, int checksum, zio_done_func_t *done, void *private,
@@ -3154,6 +3355,30 @@
  * ==========================================================================
  */
 
+/*
+ * Late pipeline bypass for trim zios. Because our zio trim queues can be
+ * pretty long and we might want to quickly terminate trims for performance
+ * reasons, we check the following conditions:
+ * 1) If a manual trim was initiated with the queue full of auto trim zios,
+ *	we want to skip doing the auto trims, because they hold up the manual
+ *	trim unnecessarily. Manual trim processes all empty space anyway.
+ * 2) If the autotrim property of the pool is flipped to off, usually due to
+ *	performance reasons, we want to stop trying to do autotrims/
+ * 3) If a manual trim shutdown was requested, immediately terminate them.
+ * 4) If a pool vdev reconfiguration is imminent, we must discard all queued
+ *	up trims to let it proceed as quickly as possible.
+ */
+static inline boolean_t
+zio_trim_should_bypass(const zio_t *zio)
+{
+	ASSERT(ZIO_IS_TRIM(zio));
+	return ((zio->io_priority == ZIO_PRIORITY_AUTO_TRIM &&
+	    (zio->io_vd->vdev_top->vdev_man_trimming ||
+	    zio->io_spa->spa_auto_trim != SPA_AUTO_TRIM_ON)) ||
+	    (zio->io_priority == ZIO_PRIORITY_MAN_TRIM &&
+	    zio->io_spa->spa_man_trim_stop) ||
+	    zio->io_vd->vdev_trim_zios_stop);
+}
 
 /*
  * Issue an I/O to the underlying vdev. Typically the issue pipeline
@@ -3266,7 +3491,8 @@
 	}
 
 	if (vd->vdev_ops->vdev_op_leaf &&
-	    (zio->io_type == ZIO_TYPE_READ || zio->io_type == ZIO_TYPE_WRITE)) {
+	    (zio->io_type == ZIO_TYPE_READ || zio->io_type == ZIO_TYPE_WRITE ||
+	    ZIO_IS_TRIM(zio))) {
 
 		if (zio->io_type == ZIO_TYPE_READ && vdev_cache_read(zio))
 			return (ZIO_PIPELINE_CONTINUE);
@@ -3281,6 +3507,9 @@
 		}
 	}
 
+	if (ZIO_IS_TRIM(zio) && zio_trim_should_bypass(zio))
+		return (ZIO_PIPELINE_CONTINUE);
+
 	zio->io_delay = gethrtime();
 	vd->vdev_ops->vdev_op_io_start(zio);
 	return (ZIO_PIPELINE_STOP);
@@ -3296,7 +3525,8 @@
 	if (zio_wait_for_children(zio, ZIO_CHILD_VDEV, ZIO_WAIT_DONE))
 		return (ZIO_PIPELINE_STOP);
 
-	ASSERT(zio->io_type == ZIO_TYPE_READ || zio->io_type == ZIO_TYPE_WRITE);
+	ASSERT(zio->io_type == ZIO_TYPE_READ ||
+	    zio->io_type == ZIO_TYPE_WRITE || ZIO_IS_TRIM(zio));
 
 	if (zio->io_delay)
 		zio->io_delay = gethrtime() - zio->io_delay;
@@ -3315,7 +3545,7 @@
 		if (zio_injection_enabled && zio->io_error == 0)
 			zio->io_error = zio_handle_label_injection(zio, EIO);
 
-		if (zio->io_error) {
+		if (zio->io_error && !ZIO_IS_TRIM(zio)) {
 			if (!vdev_accessible(vd, zio)) {
 				zio->io_error = SET_ERROR(ENXIO);
 			} else {
@@ -3414,14 +3644,17 @@
 	}
 
 	/*
-	 * If a cache flush returns ENOTSUP or ENOTTY, we know that no future
-	 * attempts will ever succeed. In this case we set a persistent bit so
-	 * that we don't bother with it in the future.
+	 * If a cache flush or discard returns ENOTSUP or ENOTTY, we know that
+	 * no future attempts will ever succeed. In this case we set a
+	 * persistent bit so that we don't bother with it in the future.
 	 */
 	if ((zio->io_error == ENOTSUP || zio->io_error == ENOTTY) &&
-	    zio->io_type == ZIO_TYPE_IOCTL &&
-	    zio->io_cmd == DKIOCFLUSHWRITECACHE && vd != NULL)
-		vd->vdev_nowritecache = B_TRUE;
+	    zio->io_type == ZIO_TYPE_IOCTL && vd != NULL) {
+		if (zio->io_cmd == DKIOCFLUSHWRITECACHE)
+			vd->vdev_nowritecache = B_TRUE;
+		if (zio->io_cmd == DKIOCFREE)
+			vd->vdev_notrim = B_TRUE;
+	}
 
 	if (zio->io_error)
 		zio->io_pipeline = ZIO_INTERLOCK_PIPELINE;
@@ -4219,4 +4452,13 @@
 module_param(zio_dva_throttle_enabled, int, 0644);
 MODULE_PARM_DESC(zio_dva_throttle_enabled,
 	"Throttle block allocations in the ZIO pipeline");
+
+module_param(zfs_trim, int, 0644);
+MODULE_PARM_DESC(zfs_trim, "Enable TRIM");
+
+module_param(zfs_trim_min_ext_sz, int, 0644);
+MODULE_PARM_DESC(zfs_trim_min_ext_sz, "Minimum size to TRIM");
+
+module_param(zfs_trim_sync, int, 0644);
+MODULE_PARM_DESC(zfs_trim_sync, "Issue TRIM commands synchronously");
 #endif
diff -Nuar zfs-kmod-9999.orig/module/zfs/zvol.c zfs-kmod-9999/module/zfs/zvol.c
--- zfs-kmod-9999.orig/module/zfs/zvol.c	2017-05-06 11:03:37.198218889 +0200
+++ zfs-kmod-9999/module/zfs/zvol.c	2017-05-06 11:04:34.194030819 +0200
@@ -34,7 +34,7 @@
  * Volumes are persistent through reboot and module load.  No user command
  * needs to be run before opening and using a device.
  *
- * Copyright 2014 Nexenta Systems, Inc.  All rights reserved.
+ * Copyright 2016 Nexenta Systems, Inc.  All rights reserved.
  * Copyright (c) 2016 Actifio, Inc. All rights reserved.
  */
 
diff -Nuar zfs-kmod-9999.orig/tests/runfiles/linux.run zfs-kmod-9999/tests/runfiles/linux.run
--- zfs-kmod-9999.orig/tests/runfiles/linux.run	2017-05-06 11:03:37.213218840 +0200
+++ zfs-kmod-9999/tests/runfiles/linux.run	2017-05-06 11:04:34.194030819 +0200
@@ -626,6 +626,9 @@
 [tests/functional/tmpfile]
 tests = ['tmpfile_001_pos', 'tmpfile_002_pos', 'tmpfile_003_pos']
 
+[tests/functional/trim]
+tests = ['autotrim_001_pos', 'manualtrim_001_pos']
+
 [tests/functional/truncate]
 tests = ['truncate_001_pos', 'truncate_002_pos']
 
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/cli_root/zpool_get/zpool_get.cfg zfs-kmod-9999/tests/zfs-tests/tests/functional/cli_root/zpool_get/zpool_get.cfg
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/cli_root/zpool_get/zpool_get.cfg	2017-05-06 11:03:37.359218358 +0200
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/cli_root/zpool_get/zpool_get.cfg	2017-05-06 11:04:34.194030819 +0200
@@ -33,7 +33,8 @@
 typeset -a properties=("size" "capacity" "altroot" "health" "guid" "version"
     "bootfs" "delegation" "autoreplace" "cachefile" "dedupditto" "dedupratio"
     "free" "allocated" "readonly" "comment" "expandsize" "freeing" "failmode"
-    "listsnapshots" "autoexpand" "fragmentation" "leaked" "ashift"
+    "listsnapshots" "autoexpand" "fragmentation" "leaked" "ashift" "forcetrim"
+    "autotrim"
     "feature@async_destroy" "feature@empty_bpobj" "feature@lz4_compress"
     "feature@large_blocks" "feature@large_dnode" "feature@filesystem_limits"
     "feature@spacemap_histogram" "feature@enabled_txg" "feature@hole_birth"
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/Makefile.am zfs-kmod-9999/tests/zfs-tests/tests/functional/Makefile.am
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/Makefile.am	2017-05-06 11:03:37.229218787 +0200
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/Makefile.am	2017-05-06 11:04:34.194030819 +0200
@@ -54,6 +54,7 @@
 	sparse \
 	threadsappend \
 	tmpfile \
+	trim \
 	truncate \
 	upgrade \
 	userquota \
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/autotrim_001_pos.ksh zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/autotrim_001_pos.ksh
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/autotrim_001_pos.ksh	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/autotrim_001_pos.ksh	2017-05-06 11:04:34.194030819 +0200
@@ -0,0 +1,114 @@
+#!/bin/ksh -p
+#
+# CDDL HEADER START
+#
+# The contents of this file are subject to the terms of the
+# Common Development and Distribution License (the "License").
+# You may not use this file except in compliance with the License.
+#
+# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+# or http://www.opensolaris.org/os/licensing.
+# See the License for the specific language governing permissions
+# and limitations under the License.
+#
+# When distributing Covered Code, include this CDDL HEADER in each
+# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+# If applicable, add the following below this CDDL HEADER, with the
+# fields enclosed by brackets "[]" replaced with your own identifying
+# information: Portions Copyright [yyyy] [name of copyright owner]
+#
+# CDDL HEADER END
+#
+
+#
+# Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
+# Use is subject to license terms.
+#
+#
+# Copyright (c) 2013, 2014 by Delphix. All rights reserved.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/functional/trim/trim.cfg
+. $STF_SUITE/tests/functional/trim/trim.kshlib
+
+set_tunable zfs_trim_min_ext_sz 4096
+set_tunable zfs_txgs_per_trim 2
+
+function getsizemb
+{
+	typeset rval
+
+	rval=$(du --block-size 1048576 -s "$1" | sed -e 's;[ 	].*;;')
+	echo -n "$rval"
+}
+
+function checkvdevs
+{
+	typeset vd sz
+
+	for vd in $VDEVS; do
+		sz=$(getsizemb $vd)
+		log_note Size of $vd is $sz MB
+		log_must test $sz -le $SHRUNK_SIZE_MB
+	done
+}
+
+function txgs
+{
+	typeset x
+
+	# Run some txgs in order to let autotrim do its work.
+	#
+	for x in 1 2 3; do
+		log_must zfs snapshot $TRIMPOOL@snap
+		log_must zfs destroy  $TRIMPOOL@snap
+		log_must zfs snapshot $TRIMPOOL@snap
+		log_must zfs destroy  $TRIMPOOL@snap
+	done
+}
+
+#
+# Check various pool geometries:  Create the pool, fill it, remove the test file,
+# run some txgs, export the pool and verify that the vdevs shrunk.
+#
+
+#
+# raidz
+#
+for z in 1 2 3; do
+	setupvdevs
+	log_must zpool create -f $TRIMPOOL raidz$z $VDEVS
+	log_must zpool set autotrim=on $TRIMPOOL
+	log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+	log_must rm "/$TRIMPOOL/$TESTFILE"
+	txgs
+	log_must zpool export $TRIMPOOL
+	checkvdevs
+done
+
+#
+# mirror
+#
+setupvdevs
+log_must zpool create -f $TRIMPOOL mirror $MIRROR_VDEVS_1 mirror $MIRROR_VDEVS_2
+log_must zpool set autotrim=on $TRIMPOOL
+log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+log_must rm "/$TRIMPOOL/$TESTFILE"
+txgs
+log_must zpool export $TRIMPOOL
+checkvdevs
+
+#
+# stripe
+#
+setupvdevs
+log_must zpool create -f $TRIMPOOL $STRIPE_VDEVS
+log_must zpool set autotrim=on $TRIMPOOL
+log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+log_must rm "/$TRIMPOOL/$TESTFILE"
+txgs
+log_must zpool export $TRIMPOOL
+checkvdevs
+
+log_pass TRIM successfully shrunk vdevs
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/cleanup.ksh zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/cleanup.ksh
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/cleanup.ksh	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/cleanup.ksh	2017-05-06 11:04:34.194030819 +0200
@@ -0,0 +1,31 @@
+#!/bin/ksh -p
+#
+# CDDL HEADER START
+#
+# The contents of this file are subject to the terms of the
+# Common Development and Distribution License (the "License").
+# You may not use this file except in compliance with the License.
+#
+# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+# or http://www.opensolaris.org/os/licensing.
+# See the License for the specific language governing permissions
+# and limitations under the License.
+#
+# When distributing Covered Code, include this CDDL HEADER in each
+# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+# If applicable, add the following below this CDDL HEADER, with the
+# fields enclosed by brackets "[]" replaced with your own identifying
+# information: Portions Copyright [yyyy] [name of copyright owner]
+#
+# CDDL HEADER END
+#
+
+#
+# Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
+# Use is subject to license terms.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/functional/trim/trim.cfg
+
+rm -f $VDEVS
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/Makefile.am zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/Makefile.am
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/Makefile.am	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/Makefile.am	2017-05-06 11:04:34.194030819 +0200
@@ -0,0 +1,8 @@
+pkgdatadir = $(datadir)/@PACKAGE@/zfs-tests/tests/functional/trim
+dist_pkgdata_SCRIPTS = \
+	setup.ksh \
+	trim.cfg \
+	trim.kshlib \
+	cleanup.ksh \
+	autotrim_001_pos.ksh \
+	manualtrim_001_pos.ksh
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/manualtrim_001_pos.ksh zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/manualtrim_001_pos.ksh
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/manualtrim_001_pos.ksh	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/manualtrim_001_pos.ksh	2017-05-06 11:04:34.195030815 +0200
@@ -0,0 +1,100 @@
+#!/bin/ksh -p
+#
+# CDDL HEADER START
+#
+# The contents of this file are subject to the terms of the
+# Common Development and Distribution License (the "License").
+# You may not use this file except in compliance with the License.
+#
+# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+# or http://www.opensolaris.org/os/licensing.
+# See the License for the specific language governing permissions
+# and limitations under the License.
+#
+# When distributing Covered Code, include this CDDL HEADER in each
+# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+# If applicable, add the following below this CDDL HEADER, with the
+# fields enclosed by brackets "[]" replaced with your own identifying
+# information: Portions Copyright [yyyy] [name of copyright owner]
+#
+# CDDL HEADER END
+#
+
+#
+# Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
+# Use is subject to license terms.
+#
+#
+# Copyright (c) 2013, 2014 by Delphix. All rights reserved.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/functional/trim/trim.cfg
+. $STF_SUITE/tests/functional/trim/trim.kshlib
+
+set_tunable zfs_trim_min_ext_sz 4096
+
+function getsizemb
+{
+	typeset rval
+
+	rval=$(du --block-size 1048576 -s "$1" | sed -e 's;[ 	].*;;')
+	echo -n "$rval"
+}
+
+function checkvdevs
+{
+	typeset vd sz
+
+	for vd in $VDEVS; do
+		sz=$(getsizemb $vd)
+		log_note Size of $vd is $sz MB
+		log_must test $sz -le $SHRUNK_SIZE_MB
+	done
+}
+
+function dotrim
+{
+	log_must rm "/$TRIMPOOL/$TESTFILE"
+	log_must zpool export $TRIMPOOL
+	log_must zpool import -d $VDEVDIR $TRIMPOOL
+	log_must zpool trim $TRIMPOOL
+	sleep 5
+	log_must zpool export $TRIMPOOL
+}
+
+#
+# Check various pool geometries:  Create the pool, fill it, remove the test file,
+# perform a manual trim, export the pool and verify that the vdevs shrunk.
+#
+
+#
+# raidz
+#
+for z in 1 2 3; do
+	setupvdevs
+	log_must zpool create -f $TRIMPOOL raidz$z $VDEVS
+	log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+	dotrim
+	checkvdevs
+done
+
+#
+# mirror
+#
+setupvdevs
+log_must zpool create -f $TRIMPOOL mirror $MIRROR_VDEVS_1 mirror $MIRROR_VDEVS_2
+log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+dotrim
+checkvdevs
+
+#
+# stripe
+#
+setupvdevs
+log_must zpool create -f $TRIMPOOL $STRIPE_VDEVS
+log_must file_write -o create -f "/$TRIMPOOL/$TESTFILE" -b $BLOCKSIZE -c $NUM_WRITES -d R -w
+dotrim
+checkvdevs
+
+log_pass Manual TRIM successfully shrunk vdevs
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/setup.ksh zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/setup.ksh
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/setup.ksh	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/setup.ksh	2017-05-06 11:04:34.195030815 +0200
@@ -0,0 +1,36 @@
+#!/bin/ksh -p
+#
+# CDDL HEADER START
+#
+# The contents of this file are subject to the terms of the
+# Common Development and Distribution License (the "License").
+# You may not use this file except in compliance with the License.
+#
+# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+# or http://www.opensolaris.org/os/licensing.
+# See the License for the specific language governing permissions
+# and limitations under the License.
+#
+# When distributing Covered Code, include this CDDL HEADER in each
+# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+# If applicable, add the following below this CDDL HEADER, with the
+# fields enclosed by brackets "[]" replaced with your own identifying
+# information: Portions Copyright [yyyy] [name of copyright owner]
+#
+# CDDL HEADER END
+#
+
+#
+# Copyright 2009 Sun Microsystems, Inc.  All rights reserved.
+# Use is subject to license terms.
+#
+
+#
+# Copyright (c) 2013 by Delphix. All rights reserved.
+#
+
+. $STF_SUITE/include/libtest.shlib
+. $STF_SUITE/tests/functional/trim/trim.cfg
+. $STF_SUITE/tests/functional/trim/trim.kshlib
+
+log_pass TRIM setup succeeded
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/trim.cfg zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/trim.cfg
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/trim.cfg	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/trim.cfg	2017-05-06 11:04:34.195030815 +0200
@@ -0,0 +1,60 @@
+#
+# CDDL HEADER START
+#
+# The contents of this file are subject to the terms of the
+# Common Development and Distribution License (the "License").
+# You may not use this file except in compliance with the License.
+#
+# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
+# or http://www.opensolaris.org/os/licensing.
+# See the License for the specific language governing permissions
+# and limitations under the License.
+#
+# When distributing Covered Code, include this CDDL HEADER in each
+# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
+# If applicable, add the following below this CDDL HEADER, with the
+# fields enclosed by brackets "[]" replaced with your own identifying
+# information: Portions Copyright [yyyy] [name of copyright owner]
+#
+# CDDL HEADER END
+#
+
+#
+# Copyright 2008 Sun Microsystems, Inc.  All rights reserved.
+# Use is subject to license terms.
+#
+
+#
+# Copyright (c) 2013 by Delphix. All rights reserved.
+#
+
+#
+# Parameters
+#
+TRIMPOOL=trimpool
+VDEVDIR="/tmp"
+VDEVS="/tmp/trim1.dev /tmp/trim2.dev /tmp/trim3.dev /tmp/trim4.dev /tmp/trim5.dev"
+VDEV_SIZE=128m
+TESTFILE=testfile
+SHRUNK_SIZE_MB=20
+
+NUM_WRITES=2048
+BLOCKSIZE=65536
+
+#
+# Computed values and parameters
+#
+function get_mirror_vdevs
+{
+	set -- $VDEVS
+	MIRROR_VDEVS_1="$1 $2"
+	MIRROR_VDEVS_2="$3 $4"
+}
+get_mirror_vdevs
+	
+function get_stripe_vdevs
+{
+	set -- $VDEVS
+	STRIPE_VDEVS="$1 $2 $3 $4"
+}
+get_stripe_vdevs
diff -Nuar zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/trim.kshlib zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/trim.kshlib
--- zfs-kmod-9999.orig/tests/zfs-tests/tests/functional/trim/trim.kshlib	1970-01-01 01:00:00.000000000 +0100
+++ zfs-kmod-9999/tests/zfs-tests/tests/functional/trim/trim.kshlib	2017-05-06 11:04:34.195030815 +0200
@@ -0,0 +1,35 @@
+#
+# This file and its contents are supplied under the terms of the
+# Common Development and Distribution License ("CDDL"), version 1.0.
+# You may only use this file in accordance with the terms of version
+# 1.0 of the CDDL.
+#
+# A full copy of the text of the CDDL should have accompanied this
+# source.  A copy of the CDDL is also available via the Internet at
+# http://www.illumos.org/license/CDDL.
+#
+
+function set_tunable
+{
+	typeset tunable="$1"
+	typeset value="$2"
+	typeset zfs_tunables="/sys/module/zfs/parameters"
+
+	[[ -z "$tunable" ]] && return 1
+	[[ -z "$value" ]] && return 1
+	[[ -f "$zfs_tunables/$tunable" ]] || return 1
+
+	echo -n "$value" > "$zfs_tunables/$tunable"
+	return "$?"
+}
+
+function find_scsi_debug
+{
+	grep -H scsi_debug /sys/block/*/device/model | $AWK -F/ '{print $4}' | tr '\n' ' '
+}
+
+function setupvdevs
+{
+	log_must rm -f $VDEVS
+	log_must truncate -s 192m $VDEVS
+}
